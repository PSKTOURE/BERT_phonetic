{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import AutoTokenizer, DataCollatorForLanguageModeling\n",
    "import epitran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_tokenizer = AutoTokenizer.from_pretrained(\"/home/toure215/BERT_phonetic/tokenizers/IPA/tokenizer_WordPiece_IPA\")\n",
    "phonetic_tokenizer = AutoTokenizer.from_pretrained(\"/home/toure215/BERT_phonetic/tokenizers/IPA/tokenizer_phonetic_WordPiece_IPA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len normal tokenizer vocab:  30522\n",
      "Len phonetic tokenizer vocab:  30522\n"
     ]
    }
   ],
   "source": [
    "print(\"Len normal tokenizer vocab: \", len(normal_tokenizer.get_vocab()))\n",
    "print(\"Len phonetic tokenizer vocab: \", len(phonetic_tokenizer.get_vocab()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "epi = epitran.Epitran(\"eng-Latn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [1, 20585, 8, 950, 1140, 161, 2138, 2635, 116, 9, 31, 298, 23, 3772, 159, 88, 1301, 100, 6221, 9, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'offset_mapping': [(0, 0), (0, 5), (5, 6), (7, 9), (10, 14), (15, 17), (18, 22), (22, 24), (24, 26), (26, 27), (28, 29), (30, 32), (33, 34), (35, 42), (43, 45), (46, 49), (50, 60), (61, 63), (64, 71), (71, 72), (0, 0)]}\n",
      "['[CLS]', 'hello', ',', 'my', 'name', 'is', 'paul', '##ev', '##ec', '.', 'i', 'am', 'a', 'student', 'at', 'the', 'university', 'of', 'toronto', '.', '[SEP]']\n",
      "[None, 0, 1, 2, 3, 4, 5, 5, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, None]\n",
      "[(0, 0), (0, 5), (5, 6), (7, 9), (10, 14), (15, 17), (18, 22), (22, 24), (24, 26), (26, 27), (28, 29), (30, 32), (33, 34), (35, 42), (43, 45), (46, 49), (50, 60), (61, 63), (64, 71), (71, 72), (0, 0)]\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, my name is Paulevec. I am a student at the University of Toronto.\"\n",
    "tokenized_text = normal_tokenizer(text, return_offsets_mapping=True)\n",
    "print(tokenized_text)\n",
    "ids_to_tokens = normal_tokenizer.convert_ids_to_tokens(tokenized_text[\"input_ids\"])\n",
    "print(ids_to_tokens)\n",
    "print(tokenized_text.word_ids())\n",
    "print(tokenized_text.offset_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "nums = [1, 4, 5, 4]\n",
    "idx = nums.index(4)\n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xsampa_tokens(word, phonetic_tokenizer):\n",
    "    phonetic_word = \"\".joint(epi.xsampa_list(word))\n",
    "    tokenized_word = phonetic_tokenizer(phonetic_word, add_special_tokens=False)\n",
    "    ids = tokenized_word[\"input_ids\"]\n",
    "    return ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "import random\n",
    "\n",
    "class CustomDataCollatorForMLM(DataCollatorForLanguageModeling):\n",
    "    def __init__(self, tokenizer, phonetic_tokenizer, word_to_phonetic, mlm_probability=0.15):\n",
    "        super().__init__(tokenizer=tokenizer, mlm_probability=mlm_probability)\n",
    "        self.phonetic_tokenizer = phonetic_tokenizer\n",
    "        self.word_to_phonetic = word_to_phonetic\n",
    "\n",
    "    def __call__(self, examples):\n",
    "        # Tokenize normal and phonetic text\n",
    "        normal_texts = [e['normal_text'] for e in examples]\n",
    "        phonetic_texts = [e['phonetic_text'] for e in examples]\n",
    "        \n",
    "        # Tokenize both\n",
    "        normal_encodings = self.tokenizer(normal_texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        phonetic_encodings = self.phonetic_tokenizer(phonetic_texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        \n",
    "        # Generate MLM masks for normal text\n",
    "        input_ids = normal_encodings.input_ids\n",
    "        labels = input_ids.clone()  # Original labels for computing loss\n",
    "        probability_matrix = torch.full(labels.shape, self.mlm_probability)\n",
    "        special_tokens_mask = self.tokenizer.get_special_tokens_mask(input_ids.tolist(), already_has_special_tokens=True)\n",
    "        probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n",
    "        \n",
    "        masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "        input_ids[masked_indices] = self.tokenizer.mask_token_id  # Replace with mask token\n",
    "\n",
    "        # Handle corresponding phonetic tokens\n",
    "        phonetic_labels = phonetic_encodings.input_ids.clone()\n",
    "        for idx, (normal_sentence, phonetic_sentence) in enumerate(zip(normal_texts, phonetic_texts)):\n",
    "            # Get masked words\n",
    "            for token_idx in masked_indices[idx].nonzero():\n",
    "                word_id = normal_encodings.word_ids(batch_index=idx)[token_idx.item()]\n",
    "                if word_id is not None:  # Ignore special tokens\n",
    "                    word = self.tokenizer.decode(normal_encodings.input_ids[idx][word_id])\n",
    "                    phonetic_tokens = self.word_to_phonetic.get(word, [])\n",
    "                    # Find and mask in phonetic text\n",
    "                    for p_token in phonetic_tokens:\n",
    "                        p_index = phonetic_encodings.input_ids[idx].tolist().index(p_token)\n",
    "                        phonetic_encodings.input_ids[idx][p_index] = self.phonetic_tokenizer.mask_token_id\n",
    "\n",
    "        # Return modified normal and phonetic encodings\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'labels': labels,\n",
    "            'phonetic_input_ids': phonetic_encodings.input_ids,\n",
    "            'phonetic_labels': phonetic_labels\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,\n",
      "         0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,\n",
      "         0.1500]])\n",
      "[0]\n",
      "tensor([[0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,\n",
      "         0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,\n",
      "         0.1500]])\n",
      "tensor([[False, False,  True,  True, False, False, False,  True, False, False,\n",
      "         False, False, False, False, False, False, False, False, False]])\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, my name is Paul. I am a student at the University of Toronto.\"\n",
    "encoded = normal_tokenizer(text, return_tensors=\"pt\")\n",
    "labels = encoded.input_ids.clone()\n",
    "probability_matrix = torch.full(labels.shape, 0.15)\n",
    "print(probability_matrix)\n",
    "special_tokens_mask = normal_tokenizer.get_special_tokens_mask(encoded.input_ids.tolist(), already_has_special_tokens=True)\n",
    "print(special_tokens_mask)\n",
    "probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n",
    "print(probability_matrix)\n",
    "masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "print(masked_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1, 20585,     8,   950,  1140,   161,  2138,     9,    31,   298,\n",
      "            23,  3772,   159,    88,  1301,   100,  6221,     9,     2]])\n"
     ]
    }
   ],
   "source": [
    "print(encoded.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "print(normal_tokenizer.encode(normal_tokenizer.cls_token, add_special_tokens=False))\n",
    "print(normal_tokenizer.encode(phonetic_tokenizer.cls_token, add_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    PreTrainedTokenizerBase,\n",
    "    BertForMaskedLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    BertConfig,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "import numpy as np\n",
    "from datasets import load_from_disk\n",
    "from typing import List, Dict, Tuple\n",
    "import torch\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class CustomDataCollatorForLanguageModeling:\n",
    "    def __init__(\n",
    "        self,\n",
    "        normal_tokenizer: PreTrainedTokenizerBase,\n",
    "        phonetic_tokenizer: PreTrainedTokenizerBase,\n",
    "        max_length: int = 128,\n",
    "        mask_probability: float = 0.15,\n",
    "    ):\n",
    "        self.normal_tokenizer = normal_tokenizer\n",
    "        self.phonetic_tokenizer = phonetic_tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.mask_probability = mask_probability\n",
    "        self.normal_cache = defaultdict(int)\n",
    "        self.phonetic_cache = defaultdict(int)\n",
    "\n",
    "    def _create_aligned_masks(\n",
    "        self,\n",
    "        normal_text: str,\n",
    "        phonetic_text: str,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Create masks following standard BERT masking strategy:\n",
    "        - Select 15% of tokens for potential masking\n",
    "        - Of those tokens:\n",
    "            - 80% are replaced with [MASK]\n",
    "            - 10% are replaced with random token\n",
    "            - 10% are left unchanged\n",
    "        Maintains alignment between normal and phonetic texts\n",
    "\n",
    "        Args:\n",
    "            normal_text: Original text\n",
    "            phonetic_text: Phonetic transcription of the text\n",
    "\n",
    "        Returns:\n",
    "            Tuple containing:\n",
    "            - normal_mask: Masking tensor for normal text\n",
    "            - phonetic_mask: Masking tensor for phonetic text\n",
    "            - normal_encoding: Token IDs for normal text\n",
    "            - phonetic_encoding: Token IDs for phonetic text\n",
    "        \"\"\"\n",
    "        # Split texts into words\n",
    "        # normal_words = re.findall(r\"\\w+\", normal_text, re.UNICODE)\n",
    "        # phonetic_words = phonetic_text.split()\n",
    "\n",
    "        normal_words = re.findall(r\"\\w+|[^\\w\\s]\", normal_text, re.UNICODE)\n",
    "        phonetic_words = re.findall(r\"\\w+|[^\\w\\s]\", phonetic_text, re.UNICODE)\n",
    "\n",
    "        # Get token lengths for each word\n",
    "        normal_token_lengths = [self._get_step_size(w, type=0) for w in normal_words]\n",
    "        phonetic_token_lengths = [self._get_step_size(w, type=1) for w in phonetic_words]\n",
    "\n",
    "        # Create cumulative sums for position mapping\n",
    "        normal_cumsum = np.cumsum([0] + normal_token_lengths[:-1])\n",
    "        phonetic_cumsum = np.cumsum([0] + phonetic_token_lengths[:-1])\n",
    "\n",
    "        # Tokenize both texts\n",
    "        normal_encoding = self.normal_tokenizer(\n",
    "            normal_text,\n",
    "            truncation=True,\n",
    "            add_special_tokens=False,\n",
    "            max_length=self.max_length // 2 - 2,\n",
    "            return_tensors=\"pt\",\n",
    "        )[\"input_ids\"]\n",
    "\n",
    "        phonetic_encoding = self.phonetic_tokenizer(\n",
    "            phonetic_text,\n",
    "            truncation=True,\n",
    "            add_special_tokens=False,\n",
    "            max_length=self.max_length // 2 - 2,\n",
    "            return_tensors=\"pt\",\n",
    "        )[\"input_ids\"]\n",
    "\n",
    "        # Initialize mask tensors (1 for MASK, 2 for random, 3 for unchanged)\n",
    "        normal_mask = torch.zeros(normal_encoding.size(1), dtype=torch.long)\n",
    "        phonetic_mask = torch.zeros(phonetic_encoding.size(1), dtype=torch.long)\n",
    "\n",
    "        # Calculate number of words to mask (15% of the shorter sequence)\n",
    "        num_words = min(len(normal_words), len(phonetic_words))\n",
    "        num_to_mask = max(1, int(num_words * self.mask_probability))\n",
    "\n",
    "        # Randomly select word positions to mask\n",
    "        mask_indices = random.sample(range(num_words), num_to_mask)\n",
    "\n",
    "        # Pre-calculate mask types for efficiency\n",
    "        # 1: MASK, 2: random, 3: unchanged\n",
    "        mask_types = np.random.choice(\n",
    "            [1, 2, 3], size=len(mask_indices), p=[0.8, 0.1, 0.1]  \n",
    "        )\n",
    "\n",
    "        # Apply masks\n",
    "        for word_idx, mask_type in zip(mask_indices, mask_types):\n",
    "            # Mask normal text\n",
    "            normal_start = normal_cumsum[word_idx]\n",
    "            normal_end = normal_start + normal_token_lengths[word_idx]\n",
    "            normal_mask[normal_start:normal_end] = mask_type\n",
    "\n",
    "            # Mask phonetic text\n",
    "            phonetic_start = phonetic_cumsum[word_idx]\n",
    "            phonetic_end = phonetic_start + phonetic_token_lengths[word_idx]\n",
    "            phonetic_mask[phonetic_start:phonetic_end] = mask_type\n",
    "\n",
    "        return normal_mask, phonetic_mask, normal_encoding, phonetic_encoding\n",
    "\n",
    "    def _get_step_size(self, word: str, type: int) -> int:\n",
    "        \"\"\"return the number of tokens in a word\"\"\"\n",
    "        cache = self.normal_cache if type == 0 else self.phonetic_cache\n",
    "        tokenizer = self.normal_tokenizer if type == 0 else self.phonetic_tokenizer\n",
    "        if word in cache:\n",
    "            return cache[word]\n",
    "        tokens = tokenizer(word, add_special_tokens=False)['input_ids']\n",
    "        cache[word] = len(tokens)\n",
    "        return cache[word]\n",
    "        \n",
    "\n",
    "    def __call__(self, examples: List[Dict[str, str]]) -> Dict[str, torch.Tensor]:\n",
    "        # Tokenize and process examples\n",
    "        batch_input_ids, batch_attention_masks, batch_token_type_ids, batch_labels = [], [], [], []\n",
    "\n",
    "        for example in examples:\n",
    "            normal_text = example[\"original_text\"]\n",
    "            phonetic_text = example[\"text\"]\n",
    "\n",
    "            # Create masks\n",
    "            normal_mask, phonetic_mask, normal_encoding, phonetic_encoding = (\n",
    "                self._create_aligned_masks(normal_text, phonetic_text)\n",
    "            )\n",
    "\n",
    "            # Combine normal and phonetic text\n",
    "            final_input_ids = torch.cat(\n",
    "                [\n",
    "                    torch.tensor([self.normal_tokenizer.cls_token_id]),  # [CLS]\n",
    "                    normal_encoding[0],\n",
    "                    torch.tensor([self.normal_tokenizer.sep_token_id]),  # [SEP]\n",
    "                    phonetic_encoding[0],\n",
    "                    torch.tensor([self.normal_tokenizer.sep_token_id]),  # Final [SEP]\n",
    "                ],\n",
    "            )\n",
    "\n",
    "            # Create attention mask\n",
    "            attention_mask = torch.ones(len(final_input_ids))\n",
    "\n",
    "            # Create token type IDs\n",
    "            # +1 for [SEP]\n",
    "            normal_type_ids = torch.zeros(normal_encoding.size(1))\n",
    "            phonetic_type_ids = torch.ones(phonetic_encoding.size(1))\n",
    "            token_type_ids = torch.cat(\n",
    "                [\n",
    "                    torch.tensor([0]),\n",
    "                    normal_type_ids,\n",
    "                    torch.tensor([0]),\n",
    "                    phonetic_type_ids,\n",
    "                    torch.tensor([1]),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # Create labels\n",
    "            labels = final_input_ids.clone()\n",
    "\n",
    "            # Apply masks\n",
    "            combined_mask = torch.cat(\n",
    "                [\n",
    "                    torch.tensor([0]),  # For [CLS]\n",
    "                    normal_mask,\n",
    "                    torch.tensor([0]),  # For [SEP]\n",
    "                    phonetic_mask,\n",
    "                    torch.tensor([0]),  # For final [SEP]\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # Get vocabulary size for random token selection\n",
    "            vocab_size = len(self.normal_tokenizer.vocab)\n",
    "\n",
    "            # Apply different masking strategies\n",
    "            for i in range(len(final_input_ids)):\n",
    "                if combined_mask[i] == 1:  # 80% - Replace with [MASK]\n",
    "                    final_input_ids[i] = self.normal_tokenizer.mask_token_id\n",
    "                elif combined_mask[i] == 2:  # 10% - Replace with random token\n",
    "                    final_input_ids[i] = random.randint(0, vocab_size - 1)\n",
    "\n",
    "            # Set labels\n",
    "            labels = torch.where(combined_mask > 0, labels, -100)\n",
    "\n",
    "            # Pad if necessary\n",
    "            if len(final_input_ids) < self.max_length:\n",
    "                padding_length = self.max_length - len(final_input_ids)\n",
    "                attention_mask = torch.cat([attention_mask, torch.zeros(padding_length)])\n",
    "                token_type_ids = torch.cat([token_type_ids, torch.zeros(padding_length)])\n",
    "                labels = torch.cat([labels, torch.tensor([-100] * padding_length)])\n",
    "                final_input_ids = torch.cat(\n",
    "                    [\n",
    "                        final_input_ids,\n",
    "                        torch.tensor([self.normal_tokenizer.pad_token_id] * padding_length),\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "            # Add to batch\n",
    "            batch_input_ids.append(final_input_ids)\n",
    "            batch_attention_masks.append(attention_mask)\n",
    "            batch_token_type_ids.append(token_type_ids)\n",
    "            batch_labels.append(labels)\n",
    "\n",
    "        # Stack tensors\n",
    "        return {\n",
    "            \"input_ids\": torch.stack(batch_input_ids).long(),\n",
    "            \"attention_mask\": torch.stack(batch_attention_masks).long(),\n",
    "            \"token_type_ids\": torch.stack(batch_token_type_ids).long(),\n",
    "            \"labels\": torch.stack(batch_labels).long(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = CustomDataCollatorForLanguageModeling(normal_tokenizer, phonetic_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['vælkɪɹiə kɹɑnəkəlz ajɪi',\n",
       "  'sɛnd͡ʒ now vælkɪɹiə 3 : ʌnɹɪkɔɹdɪd kɹɑnəkəlz  d͡ʒæpəniz : 3 , lɪt . vælkɪɹiə ʌv ðə bætəlfild 3  , kɑmənli ɹəfɹ̩d tə æz vælkɪɹiə kɹɑnəkəlz ajɪi awtsajd d͡ʒəpæn , ɪz ə tæktɪkəl ɹowl  plejɪŋ vɪdiow ɡejm dɪvɛləpt baj siɡə ænd midiə . vɪʒən fɔɹ ðə plejstejʃən pɔɹtəbəl . ɹilist ɪn d͡ʒænjuɛɹi 2011 ɪn d͡ʒəpæn , ɪt ɪz ðə θɹ̩d ɡejm ɪn ðə vælkɪɹiə sɪɹiz . ɛmplojɪŋ ðə sejm fjuʒən ʌv tæktɪkəl ænd ɹil  tajm ɡɑmplej æz ɪts pɹɛdəsɛsɹ̩z , ðə stɔɹi ɹʌnz pɛɹəlɛl tə ðə fɹ̩st ɡejm ænd fɑlowz ðə \" nejmləs \" , ə pinəl',\n",
       "  'mɪlətɛɹi junət sɹ̩vɪŋ ðə nejʃən ʌv ɡæliə dʊɹɪŋ ðə sɛkənd jʊɹowpæn wɔɹ hu pɹ̩fɔɹm sikɹət blæk ɑpɹ̩ejʃənz ænd ɑɹ pɪtəd əɡɛnst ðə ɪmpɪɹiəl junət \" kələməti ɹejvən \" .',\n",
       "  'ðə ɡejm bɪɡæn dɪvɛləpmənt ɪn 2010 , kæɹiɪŋ owvɹ̩ ə lɑɹd͡ʒ pɔɹʃən ʌv ðə wɹ̩k dʌn ɑn vælkɪɹiə kɹɑnəkəlz ɪi . wajl ɪt ɹɪtejnd ðə stændɹ̩d fit͡ʃɹ̩z ʌv ðə sɪɹiz , ɪt ɔlsow ʌndɹ̩wɛnt mʌltəpəl əd͡ʒʌstmənts , sʌt͡ʃ æz mejkɪŋ ðə ɡejm mɔɹ fɹ̩ɡɪvɪŋ fɔɹ sɪɹiz nukʌmɹ̩z . kɛɹɪktɹ̩ dɪzajnɹ̩ ɹejtə hownd͡ʒu ænd kəmpowzɹ̩ hɪtowʃi sɑkɪmowtow bowθ ɹɪtɹ̩nd fɹʌm pɹiviəs ɛntɹiz , əlɔŋ wɪð vælkɪɹiə kɹɑnəkəlz ɪi dɹ̩ɛktɹ̩ təkɛʃi owzɑwə . ə lɑɹd͡ʒ tim ʌv ɹajtɹ̩z hændəld ðə skɹɪpt . ðə ɡejm  ɛs owpənɪŋ θim wɑz sʌŋ baj mej  ɛn .',\n",
       "  'ɪt mɛt wɪð pɑzətɪv sejlz ɪn d͡ʒəpæn , ænd wɑz pɹejzd baj bowθ d͡ʒæpəniz ænd wɛstɹ̩n kɹɪtɪks . æftɹ̩ ɹilis , ɪt ɹəsivd dawnlowdəbəl kɑntɛnt , əlɔŋ wɪð æn ɪkspændəd ədɪʃən ɪn nowvɛmbɹ̩ ʌv ðæt jɪɹ . ɪt wɑz ɔlsow ədæptəd ɪntu mæŋɡə ænd æn ɹ̩ɪd͡ʒənəl vɪdiow ænəmejʃən sɪɹiz . du tə low sejlz ʌv vælkɪɹiə kɹɑnəkəlz ɪi , vælkɪɹiə kɹɑnəkəlz ajɪi wɑz nɑt lowkəlajzd , bʌt ə fæn tɹænzlejʃən kəmpætəbəl wɪð ðə ɡejm  ɛs ɪkspændəd ədɪʃən wɑz ɹilist ɪn 2014 . midiə . vɪʒən wʊd ɹɪtɹ̩n tə ðə fɹænt͡ʃajz wɪð ðə dɪvɛləpmənt ʌv vælkɪɹiə : æʒɹ̩ ɹɛvəluʃən fɔɹ'],\n",
       " 'original_text': ['valkyria chronicles iii',\n",
       "  'senj no valkyria 3 : unrecorded chronicles  japanese : 3 , lit . valkyria of the battlefield 3  , commonly referred to as valkyria chronicles iii outside japan , is a tactical role  playing video game developed by sega and media . vision for the playstation portable . released in january 2011 in japan , it is the third game in the valkyria series . employing the same fusion of tactical and real  time gameplay as its predecessors , the story runs parallel to the first game and follows the \" nameless \" , a penal',\n",
       "  'military unit serving the nation of gallia during the second europan war who perform secret black operations and are pitted against the imperial unit \" calamaty raven \" .',\n",
       "  \"the game began development in 2010 , carrying over a large portion of the work done on valkyria chronicles ii . while it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . character designer raita honjou and composer hitoshi sakimoto both returned from previous entries , along with valkyria chronicles ii director takeshi ozawa . a large team of writers handled the script . the game ' s opening theme was sung by may ' n .\",\n",
       "  \"it met with positive sales in japan , and was praised by both japanese and western critics . after release , it received downloadable content , along with an expanded edition in november of that year . it was also adapted into manga and an original video animation series . due to low sales of valkyria chronicles ii , valkyria chronicles iii was not localized , but a fan translation compatible with the game ' s expanded edition was released in 2014 . media . vision would return to the franchise with the development of valkyria : azure revolution for\"]}"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_from_disk(\"/home/toure215/BERT_phonetic/DATASETS/phonetic_wikitext_IPA\")\n",
    "dataset[\"train\"][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "həlow, maj nejm ɪz pɔlɪvɪk. aj æm ə studənt :æt ðə junəvɹ̩səti ʌv tɹ̩ɑntow.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Hello, my name is Paulevec. I am a student :at the University of Toronto.\"\n",
    "phonetic_text = epi.transliterate(text)\n",
    "def split_words_and_punctuation(text):\n",
    "    return re.findall(r\"\\w+|[^\\w\\s]\", text, re.UNICODE)\n",
    "\n",
    "text_list = split_words_and_punctuation(text)\n",
    "phonetic_text_list = split_words_and_punctuation(phonetic_text)\n",
    "# text = \" \".join(text_list)\n",
    "# print(text)\n",
    "\n",
    "# phonetic_text = \" \".join(\"\".join(epi.xsampa_list(word)) for word in text_list)\n",
    "print(phonetic_text)\n",
    "# print(phonetic_text_list)\n",
    "# print(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\n",
    "    {\"original_text\": text, \"text\": phonetic_text},\n",
    "]\n",
    "normal_ids = normal_tokenizer(text, add_special_tokens=False, padding=False, truncation=True, max_length=50)[\"input_ids\"]\n",
    "phonetic_ids = phonetic_tokenizer(phonetic_text, add_special_tokens=False, padding=False, truncation=True, max_length=50)[\"input_ids\"]\n",
    "\n",
    "normal_tokens = normal_tokenizer.convert_ids_to_tokens(normal_ids)\n",
    "phonetic_tokens = phonetic_tokenizer.convert_ids_to_tokens(phonetic_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', ',', 'my', 'name', 'is', 'paul', '##ev', '##ec', '.', 'i', 'am', 'a', 'student', ':', 'at', 'the', 'university', 'of', 'toronto', '.']\n",
      "['həlow', ',', 'maj', 'nejm', 'ɪz', 'pɔl', '##ɪv', '##ɪk', '.', 'aj', 'æm', 'ə', 'studənt', ':', 'æt', 'ðə', 'junəvɹsəti', 'ʌv', 'tɹɑntow', '.']\n",
      "[20585, 8, 950, 1140, 161, 2138, 2635, 116, 9, 31, 298, 23, 3772, 20, 159, 88, 1301, 100, 6221, 9]\n",
      "[19803, 7, 579, 573, 176, 2005, 195, 122, 8, 230, 938, 47, 3589, 19, 184, 104, 1247, 113, 5955, 8]\n",
      "[MASK]\n",
      "[MASK]\n",
      "{'input_ids': [2643, 108], 'token_type_ids': [0, 0], 'attention_mask': [1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(normal_tokens)\n",
    "print(phonetic_tokens)\n",
    "print(normal_ids)\n",
    "print(phonetic_ids)\n",
    "print(normal_tokenizer.decode(4))\n",
    "print(phonetic_tokenizer.decode(4))\n",
    "print(phonetic_tokenizer(phonetic_text_list[-2], add_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[   1,    4,    8,  950, 1140,  161, 2138, 2635,  116,    9,   31,  298,\n",
       "            23, 3772,   20,  159,   88, 1301,  100,    4,    9,    2,    4,    7,\n",
       "           579,  573,  176, 2005,  195,  122,    8,  230,  938,   47, 3589,   19,\n",
       "           184,  104, 1247,    4,    4,    8,    2,    3,    3,    3,    3,    3,\n",
       "             3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "             3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "             3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "             3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "             3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "             3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,    3,\n",
       "             3,    3,    3,    3,    3,    3,    3,    3]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'labels': tensor([[ -100, 20585,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  6221,\n",
       "           -100,  -100, 19803,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,   113,\n",
       "           5955,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "           -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100]])}"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_ipa = AutoTokenizer.from_pretrained(\"/home/toure215/BERT_phonetic/tokenizers/tokenizer_phonetic_BPE_IPA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'həlow', ',', 'maj', 'nejm', 'ɪz', 'pɔl', 'ɪvɪk', '.', 'aj', 'æm', 'ə', 'studənt', 'æt', 'ðə', 'junəvɹsəti', 'ʌv', 'tɹɑntow', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, my name is Paulevec. I am a student at the University of Toronto.\"\n",
    "text = epi.transliterate(text)\n",
    "tokenized_text = tokenizer_ipa(text)\n",
    "tokens = tokenizer_ipa.convert_ids_to_tokens(tokenized_text[\"input_ids\"])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30522\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenizer_ipa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
