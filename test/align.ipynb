{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import AutoTokenizer, DataCollatorForLanguageModeling\n",
    "import epitran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_tokenizer = AutoTokenizer.from_pretrained(\"/home/toure215/BERT_phonetic/tokenizers/tokenizer_normal_and_phonetic\")\n",
    "phonetic_tokenizer = AutoTokenizer.from_pretrained(\"/home/toure215/BERT_phonetic/tokenizers/tokenizer_normal_and_phonetic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len normal tokenizer vocab:  61044\n",
      "Len phonetic tokenizer vocab:  61044\n"
     ]
    }
   ],
   "source": [
    "print(\"Len normal tokenizer vocab: \", len(normal_tokenizer.get_vocab()))\n",
    "print(\"Len phonetic tokenizer vocab: \", len(phonetic_tokenizer.get_vocab()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "epi = epitran.Epitran(\"eng-Latn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [1, 39774, 8, 1748, 2127, 256, 4090, 9505, 169, 9, 31, 967, 23, 7232, 252, 119, 2439, 138, 11962, 9, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'offset_mapping': [(0, 0), (0, 5), (5, 6), (7, 9), (10, 14), (15, 17), (18, 22), (22, 24), (24, 26), (26, 27), (28, 29), (30, 32), (33, 34), (35, 42), (43, 45), (46, 49), (50, 60), (61, 63), (64, 71), (71, 72), (0, 0)]}\n",
      "['[CLS]', 'hello', ',', 'my', 'name', 'is', 'paul', '##ev', '##ec', '.', 'i', 'am', 'a', 'student', 'at', 'the', 'university', 'of', 'toronto', '.', '[SEP]']\n",
      "[None, 0, 1, 2, 3, 4, 5, 5, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, None]\n",
      "[(0, 0), (0, 5), (5, 6), (7, 9), (10, 14), (15, 17), (18, 22), (22, 24), (24, 26), (26, 27), (28, 29), (30, 32), (33, 34), (35, 42), (43, 45), (46, 49), (50, 60), (61, 63), (64, 71), (71, 72), (0, 0)]\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, my name is Paulevec. I am a student at the University of Toronto.\"\n",
    "tokenized_text = normal_tokenizer(text, return_offsets_mapping=True)\n",
    "print(tokenized_text)\n",
    "ids_to_tokens = normal_tokenizer.convert_ids_to_tokens(tokenized_text[\"input_ids\"])\n",
    "print(ids_to_tokens)\n",
    "print(tokenized_text.word_ids())\n",
    "print(tokenized_text.offset_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "nums = [1, 4, 5, 4]\n",
    "idx = nums.index(4)\n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xsampa_tokens(word, phonetic_tokenizer):\n",
    "    phonetic_word = \"\".joint(epi.xsampa_list(word))\n",
    "    tokenized_word = phonetic_tokenizer(phonetic_word, add_special_tokens=False)\n",
    "    ids = tokenized_word[\"input_ids\"]\n",
    "    return ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,\n",
      "         0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,\n",
      "         0.1500]])\n",
      "[0]\n",
      "tensor([[0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,\n",
      "         0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,\n",
      "         0.1500]])\n",
      "tensor([[False, False, False, False,  True, False, False,  True, False, False,\n",
      "         False, False,  True, False,  True, False, False, False, False]])\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, my name is Paul. I am a student at the University of Toronto.\"\n",
    "encoded = normal_tokenizer(text, return_tensors=\"pt\")\n",
    "labels = encoded.input_ids.clone()\n",
    "probability_matrix = torch.full(labels.shape, 0.15)\n",
    "print(probability_matrix)\n",
    "special_tokens_mask = normal_tokenizer.get_special_tokens_mask(encoded.input_ids.tolist(), already_has_special_tokens=True)\n",
    "print(special_tokens_mask)\n",
    "probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n",
    "print(probability_matrix)\n",
    "masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "print(masked_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1, 39774,     8,  1748,  2127,   256,  4090,     9,    31,   967,\n",
      "            23,  7232,   252,   119,  2439,   138, 11962,     9,     2]])\n"
     ]
    }
   ],
   "source": [
    "print(encoded.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "print(normal_tokenizer.encode(normal_tokenizer.cls_token, add_special_tokens=False))\n",
    "print(normal_tokenizer.encode(phonetic_tokenizer.cls_token, add_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    PreTrainedTokenizerBase,\n",
    "    BertForMaskedLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    BertConfig,\n",
    "    AutoTokenizer,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "import numpy as np\n",
    "from datasets import load_from_disk\n",
    "from typing import List, Dict, Tuple\n",
    "import torch\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "class CustomDataCollatorForLanguageModeling:\n",
    "    def __init__(\n",
    "        self,\n",
    "        tokenizer: PreTrainedTokenizerBase,\n",
    "        max_length: int = 128,\n",
    "        mask_probability: float = 0.15,\n",
    "    ):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.mask_probability = mask_probability\n",
    "        self.cache = defaultdict(int)\n",
    "\n",
    "    def _create_aligned_masks(\n",
    "        self,\n",
    "        normal_text: str,\n",
    "        phonetic_text: str,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Create masks following standard BERT masking strategy:\n",
    "        - Select 15% of tokens for potential masking\n",
    "        - Of those tokens:\n",
    "            - 80% are replaced with [MASK]\n",
    "            - 10% are replaced with random token\n",
    "            - 10% are left unchanged\n",
    "        Maintains alignment between normal and phonetic texts\n",
    "\n",
    "        Args:\n",
    "            normal_text: Original text\n",
    "            phonetic_text: Phonetic transcription of the text\n",
    "\n",
    "        Returns:\n",
    "            Tuple containing:\n",
    "            - normal_mask: Masking tensor for normal text\n",
    "            - phonetic_mask: Masking tensor for phonetic text\n",
    "            - normal_encoding: Token IDs for normal text\n",
    "            - phonetic_encoding: Token IDs for phonetic text\n",
    "        \"\"\"\n",
    "        # Split texts into words\n",
    "\n",
    "        normal_words = re.findall(r\"\\w+|[^\\w\\s]\", normal_text, re.UNICODE)\n",
    "        phonetic_words = re.findall(r\"\\w+|[^\\w\\s]\", phonetic_text, re.UNICODE)\n",
    "\n",
    "        # Get token lengths for each word\n",
    "        normal_token_lengths = [self._get_step_size(w) for w in normal_words]\n",
    "        phonetic_token_lengths = [self._get_step_size(w) for w in phonetic_words]\n",
    "\n",
    "        # Create cumulative sums for position mapping\n",
    "        normal_cumsum = np.cumsum([0] + normal_token_lengths[:-1])\n",
    "        phonetic_cumsum = np.cumsum([0] + phonetic_token_lengths[:-1])\n",
    "\n",
    "        # Tokenize both texts\n",
    "        normal_encoding = self.tokenizer(\n",
    "            normal_text,\n",
    "            truncation=True,\n",
    "            add_special_tokens=False,\n",
    "            max_length=self.max_length // 2 - 2,\n",
    "            return_tensors=\"pt\",\n",
    "        )[\"input_ids\"]\n",
    "\n",
    "        phonetic_encoding = self.tokenizer(\n",
    "            phonetic_text,\n",
    "            truncation=True,\n",
    "            add_special_tokens=False,\n",
    "            max_length=self.max_length // 2 - 2,\n",
    "            return_tensors=\"pt\",\n",
    "        )[\"input_ids\"]\n",
    "\n",
    "        # Initialize mask tensors (1 for MASK, 2 for random, 3 for unchanged)\n",
    "        normal_mask = torch.zeros(normal_encoding.size(1), dtype=torch.long)\n",
    "        phonetic_mask = torch.zeros(phonetic_encoding.size(1), dtype=torch.long)\n",
    "\n",
    "        # Calculate number of words to mask (15% of the shorter sequence)\n",
    "        num_words = min(len(normal_words), len(phonetic_words))\n",
    "        num_to_mask = max(1, int(num_words * self.mask_probability))\n",
    "\n",
    "        # Randomly select word positions to mask\n",
    "        mask_indices = random.sample(range(num_words), num_to_mask)\n",
    "\n",
    "        # Pre-calculate mask types for efficiency\n",
    "        # 1: MASK, 2: random, 3: unchanged\n",
    "        mask_types = np.random.choice([1, 2, 3], size=len(mask_indices), p=[0.8, 0.1, 0.1])\n",
    "\n",
    "        # Apply masks\n",
    "        for word_idx, mask_type in zip(mask_indices, mask_types):\n",
    "            # Mask normal text\n",
    "            normal_start = normal_cumsum[word_idx]\n",
    "            normal_end = normal_start + normal_token_lengths[word_idx]\n",
    "            normal_mask[normal_start:normal_end] = mask_type\n",
    "\n",
    "            # Mask phonetic text\n",
    "            phonetic_start = phonetic_cumsum[word_idx]\n",
    "            phonetic_end = phonetic_start + phonetic_token_lengths[word_idx]\n",
    "            phonetic_mask[phonetic_start:phonetic_end] = mask_type\n",
    "\n",
    "        return normal_mask, phonetic_mask, normal_encoding, phonetic_encoding\n",
    "\n",
    "    def _get_step_size(self, word: str) -> int:\n",
    "        \"\"\"return the number of tokens in a word\"\"\"\n",
    "        if word in self.cache:\n",
    "            return self.cache[word]\n",
    "        tokens = self.tokenizer(word, add_special_tokens=False)[\"input_ids\"]\n",
    "        self.cache[word] = len(tokens)\n",
    "        return self.cache[word]\n",
    "\n",
    "    def __call__(self, examples: List[Dict[str, str]]) -> Dict[str, torch.Tensor]:\n",
    "        # Tokenize and process examples\n",
    "        batch_input_ids, batch_attention_masks, batch_token_type_ids, batch_labels = [], [], [], []\n",
    "\n",
    "        for example in examples:\n",
    "            normal_text = example[\"original_text\"]\n",
    "            phonetic_text = example[\"text\"]\n",
    "\n",
    "            # Create masks\n",
    "            normal_mask, phonetic_mask, normal_encoding, phonetic_encoding = (\n",
    "                self._create_aligned_masks(normal_text, phonetic_text)\n",
    "            )\n",
    "\n",
    "            # Combine normal and phonetic text\n",
    "            final_input_ids = torch.cat(\n",
    "                [\n",
    "                    torch.tensor([self.tokenizer.cls_token_id]),  # [CLS]\n",
    "                    normal_encoding[0],\n",
    "                    torch.tensor([self.tokenizer.sep_token_id]),  # [SEP]\n",
    "                    phonetic_encoding[0],\n",
    "                    torch.tensor([self.tokenizer.sep_token_id]),  # Final [SEP]\n",
    "                ],\n",
    "            )\n",
    "\n",
    "            # Create attention mask\n",
    "            attention_mask = torch.ones(len(final_input_ids))\n",
    "\n",
    "            # Create token type IDs\n",
    "            # +1 for [SEP]\n",
    "            normal_type_ids = torch.zeros(normal_encoding.size(1))\n",
    "            phonetic_type_ids = torch.ones(phonetic_encoding.size(1))\n",
    "            token_type_ids = torch.cat(\n",
    "                [\n",
    "                    torch.tensor([0]),\n",
    "                    normal_type_ids,\n",
    "                    torch.tensor([0]),\n",
    "                    phonetic_type_ids,\n",
    "                    torch.tensor([1]),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # Create labels\n",
    "            labels = final_input_ids.clone()\n",
    "\n",
    "            # Apply masks\n",
    "            combined_mask = torch.cat(\n",
    "                [\n",
    "                    torch.tensor([0]),  # For [CLS]\n",
    "                    normal_mask,\n",
    "                    torch.tensor([0]),  # For [SEP]\n",
    "                    phonetic_mask,\n",
    "                    torch.tensor([0]),  # For final [SEP]\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # Apply different masking strategies\n",
    "            mask_token_id = self.tokenizer.mask_token_id\n",
    "            for i in range(len(final_input_ids)):\n",
    "                mask_type = combined_mask[i]\n",
    "                if mask_type == 1:  # 80% - Replace with [MASK]\n",
    "                    final_input_ids[i] = mask_token_id\n",
    "                elif mask_type == 2:  # 10% - Replace with random token\n",
    "                    final_input_ids[i] = np.random.randint(0, self.tokenizer.vocab_size)\n",
    "\n",
    "            # Set labels\n",
    "            labels = torch.where(combined_mask > 0, labels, -100)\n",
    "\n",
    "            # Pad if necessary\n",
    "            if len(final_input_ids) < self.max_length:\n",
    "                padding_length = self.max_length - len(final_input_ids)\n",
    "                attention_mask = torch.cat([attention_mask, torch.zeros(padding_length)])\n",
    "                token_type_ids = torch.cat([token_type_ids, torch.zeros(padding_length)])\n",
    "                labels = torch.cat([labels, torch.tensor([-100] * padding_length)])\n",
    "                final_input_ids = torch.cat(\n",
    "                    [\n",
    "                        final_input_ids,\n",
    "                        torch.tensor([self.tokenizer.pad_token_id] * padding_length),\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "            # Add to batch\n",
    "            batch_input_ids.append(final_input_ids)\n",
    "            batch_attention_masks.append(attention_mask)\n",
    "            batch_token_type_ids.append(token_type_ids)\n",
    "            batch_labels.append(labels)\n",
    "\n",
    "        # Stack tensors\n",
    "        return {\n",
    "            \"input_ids\": torch.stack(batch_input_ids).long(),\n",
    "            \"attention_mask\": torch.stack(batch_attention_masks).long(),\n",
    "            \"token_type_ids\": torch.stack(batch_token_type_ids).long(),\n",
    "            \"labels\": torch.stack(batch_labels).long(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = CustomDataCollatorForLanguageModeling(normal_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['vælkɪɹiə kɹɑnəkəlz ajɪi',\n",
       "  'sɛnd͡ʒ now vælkɪɹiə 3 : ʌnɹɪkɔɹdɪd kɹɑnəkəlz  d͡ʒæpəniz : 3 , lɪt . vælkɪɹiə ʌv ðə bætəlfild 3  , kɑmənli ɹəfɹ̩d tə æz vælkɪɹiə kɹɑnəkəlz ajɪi awtsajd d͡ʒəpæn , ɪz ə tæktɪkəl ɹowl  plejɪŋ vɪdiow ɡejm dɪvɛləpt baj siɡə ænd midiə . vɪʒən fɔɹ ðə plejstejʃən pɔɹtəbəl',\n",
       "  '. ɹilist ɪn d͡ʒænjuɛɹi 2011 ɪn d͡ʒəpæn , ɪt ɪz ðə θɹ̩d ɡejm ɪn ðə vælkɪɹiə sɪɹiz . ɛmplojɪŋ ðə sejm fjuʒən ʌv tæktɪkəl ænd ɹil  tajm ɡɑmplej æz ɪts pɹɛdəsɛsɹ̩z , ðə stɔɹi ɹʌnz pɛɹəlɛl tə ðə fɹ̩st ɡejm ænd fɑlowz ðə \" nejmləs \" , ə pinəl',\n",
       "  'mɪlətɛɹi junət sɹ̩vɪŋ ðə nejʃən ʌv ɡæliə dʊɹɪŋ ðə sɛkənd jʊɹowpæn wɔɹ hu pɹ̩fɔɹm sikɹət blæk ɑpɹ̩ejʃənz ænd ɑɹ pɪtəd əɡɛnst ðə ɪmpɪɹiəl junət \" kələməti ɹejvən \" .',\n",
       "  'ðə ɡejm bɪɡæn dɪvɛləpmənt ɪn 2010 , kæɹiɪŋ owvɹ̩ ə lɑɹd͡ʒ pɔɹʃən ʌv ðə wɹ̩k dʌn ɑn vælkɪɹiə kɹɑnəkəlz ɪi . wajl ɪt ɹɪtejnd ðə stændɹ̩d fit͡ʃɹ̩z ʌv ðə sɪɹiz , ɪt ɔlsow ʌndɹ̩wɛnt mʌltəpəl əd͡ʒʌstmənts , sʌt͡ʃ æz mejkɪŋ ðə ɡejm mɔɹ fɹ̩ɡɪvɪŋ fɔɹ sɪɹiz nukʌmɹ̩z . kɛɹɪktɹ̩ dɪzajnɹ̩'],\n",
       " 'original_text': ['valkyria chronicles iii',\n",
       "  'senj no valkyria 3 : unrecorded chronicles  japanese : 3 , lit . valkyria of the battlefield 3  , commonly referred to as valkyria chronicles iii outside japan , is a tactical role  playing video game developed by sega and media . vision for the playstation portable',\n",
       "  '. released in january 2011 in japan , it is the third game in the valkyria series . employing the same fusion of tactical and real  time gameplay as its predecessors , the story runs parallel to the first game and follows the \" nameless \" , a penal',\n",
       "  'military unit serving the nation of gallia during the second europan war who perform secret black operations and are pitted against the imperial unit \" calamaty raven \" .',\n",
       "  'the game began development in 2010 , carrying over a large portion of the work done on valkyria chronicles ii . while it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . character designer']}"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_from_disk(\"/home/toure215/BERT_phonetic/DATASETS/phonetic_wikitext2\")\n",
    "dataset[\"train\"][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "həlow, maj nejm ɪz pɔlɪvɪk. aj æm ə studənt :æt ðə junəvɹ̩səti ʌv tɹ̩ɑntow.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Hello, my name is Paulevec. I am a student :at the University of Toronto.\"\n",
    "phonetic_text = epi.transliterate(text)\n",
    "def split_words_and_punctuation(text):\n",
    "    return re.findall(r\"\\w+|[^\\w\\s]\", text, re.UNICODE)\n",
    "\n",
    "text_list = split_words_and_punctuation(text)\n",
    "phonetic_text_list = split_words_and_punctuation(phonetic_text)\n",
    "# text = \" \".join(text_list)\n",
    "# print(text)\n",
    "\n",
    "# phonetic_text = \" \".join(\"\".join(epi.xsampa_list(word)) for word in text_list)\n",
    "print(phonetic_text)\n",
    "# print(phonetic_text_list)\n",
    "# print(text_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\n",
    "    {\"original_text\": text, \"text\": phonetic_text},\n",
    "]\n",
    "normal_ids = normal_tokenizer(\n",
    "    text, add_special_tokens=False, padding=False, truncation=True, max_length=50\n",
    ")[\"input_ids\"]\n",
    "phonetic_ids = phonetic_tokenizer(\n",
    "    phonetic_text,\n",
    "    add_special_tokens=False,\n",
    "    padding=False,\n",
    "    truncation=True,\n",
    "    max_length=50,\n",
    "    return_tensors=\"pt\",\n",
    ")[\"input_ids\"]\n",
    "phonetic_ids = phonetic_ids\n",
    "normal_tokens = normal_tokenizer.convert_ids_to_tokens(normal_ids)\n",
    "phonetic_tokens = phonetic_tokenizer.convert_ids_to_tokens(phonetic_ids.squeeze().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', ',', 'my', 'name', 'is', 'paul', '##ev', '##ec', '.', 'i', 'am', 'a', 'student', ':', 'at', 'the', 'university', 'of', 'toronto', '.']\n",
      "['həlow', ',', 'maj', 'nejm', 'ɪz', 'pɔl', '##ɪv', '##ɪk', '.', 'aj', 'æm', 'ə', 'studənt', ':', 'æt', 'ðə', 'junəvɹsəti', 'ʌv', 'tɹɑntow', '.']\n",
      "[39774, 8, 1748, 2127, 256, 4090, 9505, 169, 9, 31, 967, 23, 7232, 20, 252, 119, 2439, 138, 11962, 9]\n",
      "tensor([[39741,     8,   711,  1067,   278,  3944,   306,   165,     9,   381,\n",
      "          2714,    54,  7238,    20,   291,   120,  2438,   142, 11966,     9]])\n",
      "[MASK]\n",
      "[MASK]\n",
      "{'input_ids': [5184, 126], 'token_type_ids': [0, 0], 'attention_mask': [1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(normal_tokens)\n",
    "print(phonetic_tokens)\n",
    "print(normal_ids)\n",
    "print(phonetic_ids)\n",
    "print(normal_tokenizer.decode(4))\n",
    "print(phonetic_tokenizer.decode(4))\n",
    "print(phonetic_tokenizer(phonetic_text_list[-2], add_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1, 39774,     8,  1748,  2127,   256,  4090,  9505,   169,     9,\n",
       "             31,   967,    23,  7232,    20,   252,   119,  2439,     4, 11962,\n",
       "              9,     2, 39741,     8,   711,  1067,   278,  3944,   306,   165,\n",
       "              9,   381,  2714,    54,  7238,    20,   291,   120,  2438,   142,\n",
       "          11966,     9,     2,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'labels': tensor([[-100, -100, -100, 1748, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100,  138, -100, -100, -100, -100, -100,\n",
       "           711, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100]])}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_ipa = AutoTokenizer.from_pretrained(\"/home/toure215/BERT_phonetic/tokenizers/tokenizer_phonetic_BPE_IPA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'həlow', ',', 'maj', 'nejm', 'ɪz', 'pɔl', 'ɪvɪk', '.', 'aj', 'æm', 'ə', 'studənt', 'æt', 'ðə', 'junəvɹsəti', 'ʌv', 'tɹɑntow', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, my name is Paulevec. I am a student at the University of Toronto.\"\n",
    "text = epi.transliterate(text)\n",
    "tokenized_text = tokenizer_ipa(text)\n",
    "tokens = tokenizer_ipa.convert_ids_to_tokens(tokenized_text[\"input_ids\"])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30522\n"
     ]
    }
   ],
   "source": [
    "print(len(tokenizer_ipa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
