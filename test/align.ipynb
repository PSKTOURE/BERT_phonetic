{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import AutoTokenizer, DataCollatorForLanguageModeling\n",
    "import epitran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_tokenizer = AutoTokenizer.from_pretrained(\"psktoure/BERT_WordPiece_wikitext\")\n",
    "phonetic_tokenizer = AutoTokenizer.from_pretrained(\"psktoure/BERT_WordPiece_phonetic_wikitext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len normal tokenizer vocab:  30522\n",
      "Len phonetic tokenizer vocab:  30522\n"
     ]
    }
   ],
   "source": [
    "print(\"Len normal tokenizer vocab: \", len(normal_tokenizer.get_vocab()))\n",
    "print(\"Len phonetic tokenizer vocab: \", len(phonetic_tokenizer.get_vocab()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "epi = epitran.Epitran(\"eng-Latn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [1, 12325, 8, 434, 935, 68, 10489, 1244, 25, 9, 31, 79, 23, 3467, 57, 51, 1184, 65, 5758, 9, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'offset_mapping': [(0, 0), (0, 5), (5, 6), (7, 9), (10, 14), (15, 17), (18, 21), (21, 25), (25, 26), (26, 27), (28, 29), (30, 32), (33, 34), (35, 42), (43, 45), (46, 49), (50, 60), (61, 63), (64, 71), (71, 72), (0, 0)]}\n",
      "['[CLS]', 'hello', ',', 'my', 'name', 'is', 'pau', 'leve', 'c', '.', 'i', 'am', 'a', 'student', 'at', 'the', 'university', 'of', 'toronto', '.', '[SEP]']\n",
      "[None, 0, 1, 2, 3, 4, 5, 5, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, None]\n",
      "[(0, 0), (0, 5), (5, 6), (7, 9), (10, 14), (15, 17), (18, 21), (21, 25), (25, 26), (26, 27), (28, 29), (30, 32), (33, 34), (35, 42), (43, 45), (46, 49), (50, 60), (61, 63), (64, 71), (71, 72), (0, 0)]\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, my name is Paulevec. I am a student at the University of Toronto.\"\n",
    "tokenized_text = normal_tokenizer(text, return_offsets_mapping=True)\n",
    "print(tokenized_text)\n",
    "ids_to_tokens = normal_tokenizer.convert_ids_to_tokens(tokenized_text[\"input_ids\"])\n",
    "print(ids_to_tokens)\n",
    "print(tokenized_text.word_ids())\n",
    "print(tokenized_text.offset_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "nums = [1, 4, 5, 4]\n",
    "idx = nums.index(4)\n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xsampa_tokens(word, phonetic_tokenizer):\n",
    "    phonetic_word = \"\".joint(epi.xsampa_list(word))\n",
    "    tokenized_word = phonetic_tokenizer(phonetic_word, add_special_tokens=False)\n",
    "    ids = tokenized_word[\"input_ids\"]\n",
    "    return ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "import random\n",
    "\n",
    "class CustomDataCollatorForMLM(DataCollatorForLanguageModeling):\n",
    "    def __init__(self, tokenizer, phonetic_tokenizer, word_to_phonetic, mlm_probability=0.15):\n",
    "        super().__init__(tokenizer=tokenizer, mlm_probability=mlm_probability)\n",
    "        self.phonetic_tokenizer = phonetic_tokenizer\n",
    "        self.word_to_phonetic = word_to_phonetic\n",
    "\n",
    "    def __call__(self, examples):\n",
    "        # Tokenize normal and phonetic text\n",
    "        normal_texts = [e['normal_text'] for e in examples]\n",
    "        phonetic_texts = [e['phonetic_text'] for e in examples]\n",
    "        \n",
    "        # Tokenize both\n",
    "        normal_encodings = self.tokenizer(normal_texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        phonetic_encodings = self.phonetic_tokenizer(phonetic_texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        \n",
    "        # Generate MLM masks for normal text\n",
    "        input_ids = normal_encodings.input_ids\n",
    "        labels = input_ids.clone()  # Original labels for computing loss\n",
    "        probability_matrix = torch.full(labels.shape, self.mlm_probability)\n",
    "        special_tokens_mask = self.tokenizer.get_special_tokens_mask(input_ids.tolist(), already_has_special_tokens=True)\n",
    "        probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n",
    "        \n",
    "        masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "        input_ids[masked_indices] = self.tokenizer.mask_token_id  # Replace with mask token\n",
    "\n",
    "        # Handle corresponding phonetic tokens\n",
    "        phonetic_labels = phonetic_encodings.input_ids.clone()\n",
    "        for idx, (normal_sentence, phonetic_sentence) in enumerate(zip(normal_texts, phonetic_texts)):\n",
    "            # Get masked words\n",
    "            for token_idx in masked_indices[idx].nonzero():\n",
    "                word_id = normal_encodings.word_ids(batch_index=idx)[token_idx.item()]\n",
    "                if word_id is not None:  # Ignore special tokens\n",
    "                    word = self.tokenizer.decode(normal_encodings.input_ids[idx][word_id])\n",
    "                    phonetic_tokens = self.word_to_phonetic.get(word, [])\n",
    "                    # Find and mask in phonetic text\n",
    "                    for p_token in phonetic_tokens:\n",
    "                        p_index = phonetic_encodings.input_ids[idx].tolist().index(p_token)\n",
    "                        phonetic_encodings.input_ids[idx][p_index] = self.phonetic_tokenizer.mask_token_id\n",
    "\n",
    "        # Return modified normal and phonetic encodings\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'labels': labels,\n",
    "            'phonetic_input_ids': phonetic_encodings.input_ids,\n",
    "            'phonetic_labels': phonetic_labels\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,\n",
      "         0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,\n",
      "         0.1500]])\n",
      "[0]\n",
      "tensor([[0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,\n",
      "         0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,\n",
      "         0.1500]])\n",
      "tensor([[False, False, False, False, False, False, False,  True, False, False,\n",
      "         False, False, False,  True, False,  True, False, False, False]])\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, my name is Paul. I am a student at the University of Toronto.\"\n",
    "encoded = normal_tokenizer(text, return_tensors=\"pt\")\n",
    "labels = encoded.input_ids.clone()\n",
    "probability_matrix = torch.full(labels.shape, 0.15)\n",
    "print(probability_matrix)\n",
    "special_tokens_mask = normal_tokenizer.get_special_tokens_mask(encoded.input_ids.tolist(), already_has_special_tokens=True)\n",
    "print(special_tokens_mask)\n",
    "probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n",
    "print(probability_matrix)\n",
    "masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "print(masked_indices)\n",
    "normal_tokenizer.pad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1, 12325,     8,   434,   935,    68,  2034,     9,    31,    79,\n",
      "            23,  3467,    57,    51,  1184,    65,  5758,     9,     2]])\n"
     ]
    }
   ],
   "source": [
    "print(encoded.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "print(normal_tokenizer.encode(normal_tokenizer.cls_token, add_special_tokens=False))\n",
    "print(normal_tokenizer.encode(phonetic_tokenizer.cls_token, add_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import (\n",
    "    PreTrainedTokenizerBase,\n",
    "    BertForMaskedLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    BertConfig,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "import numpy as np\n",
    "from datasets import load_from_disk\n",
    "from typing import List, Dict, Tuple\n",
    "import torch\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class CustomDataCollatorForLanguageModeling:\n",
    "    def __init__(\n",
    "        self,\n",
    "        normal_tokenizer: PreTrainedTokenizerBase,\n",
    "        phonetic_tokenizer: PreTrainedTokenizerBase,\n",
    "        max_length: int = 128,\n",
    "        mask_probability: float = 0.15,\n",
    "    ):\n",
    "        self.normal_tokenizer = normal_tokenizer\n",
    "        self.phonetic_tokenizer = phonetic_tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.mask_probability = mask_probability\n",
    "        self.normal_cache = defaultdict(int)\n",
    "        self.phonetic_cache = defaultdict(int)\n",
    "\n",
    "    def _create_aligned_masks(\n",
    "        self,\n",
    "        normal_text: str,\n",
    "        phonetic_text: str,\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Create masks following standard BERT masking strategy:\n",
    "        - Select 15% of tokens for potential masking\n",
    "        - Of those tokens:\n",
    "            - 80% are replaced with [MASK]\n",
    "            - 10% are replaced with random token\n",
    "            - 10% are left unchanged\n",
    "        Maintains alignment between normal and phonetic texts\n",
    "\n",
    "        Args:\n",
    "            normal_text: Original text\n",
    "            phonetic_text: Phonetic transcription of the text\n",
    "\n",
    "        Returns:\n",
    "            Tuple containing:\n",
    "            - normal_mask: Masking tensor for normal text\n",
    "            - phonetic_mask: Masking tensor for phonetic text\n",
    "            - normal_encoding: Token IDs for normal text\n",
    "            - phonetic_encoding: Token IDs for phonetic text\n",
    "        \"\"\"\n",
    "        # Split texts into words\n",
    "        normal_words = re.findall(r\"\\w+\", normal_text, re.UNICODE)\n",
    "        phonetic_words = phonetic_text.split()\n",
    "\n",
    "        # Get token lengths for each word\n",
    "        normal_token_lengths = [self._get_step_size(w, 0) for w in normal_words]\n",
    "        phonetic_token_lengths = [self._get_step_size(w, 1) for w in phonetic_words]\n",
    "\n",
    "        # Create cumulative sums for position mapping\n",
    "        normal_cumsum = np.cumsum([0] + normal_token_lengths[:-1])\n",
    "        phonetic_cumsum = np.cumsum([0] + phonetic_token_lengths[:-1])\n",
    "\n",
    "        # Tokenize both texts\n",
    "        normal_encoding = self.normal_tokenizer(\n",
    "            normal_text,\n",
    "            truncation=True,\n",
    "            add_special_tokens=False,\n",
    "            max_length=self.max_length // 2,\n",
    "            return_tensors=\"pt\",\n",
    "        )[\"input_ids\"]\n",
    "\n",
    "        phonetic_encoding = self.phonetic_tokenizer(\n",
    "            phonetic_text,\n",
    "            truncation=True,\n",
    "            add_special_tokens=False,\n",
    "            max_length=self.max_length // 2,\n",
    "            return_tensors=\"pt\",\n",
    "        )[\"input_ids\"]\n",
    "\n",
    "        # Initialize mask tensors (1 for MASK, 2 for random, 3 for unchanged)\n",
    "        normal_mask = torch.zeros(normal_encoding.size(1), dtype=torch.long)\n",
    "        phonetic_mask = torch.zeros(phonetic_encoding.size(1), dtype=torch.long)\n",
    "\n",
    "        # Calculate number of words to mask (15% of the shorter sequence)\n",
    "        num_words = min(len(normal_words), len(phonetic_words))\n",
    "        num_to_mask = max(1, int(num_words * self.mask_probability))\n",
    "\n",
    "        # Randomly select word positions to mask\n",
    "        mask_indices = random.sample(range(num_words), num_to_mask)\n",
    "\n",
    "        # Pre-calculate mask types for efficiency\n",
    "        # 1: MASK, 2: random, 3: unchanged\n",
    "        mask_types = np.random.choice(\n",
    "            [1, 2, 3], size=len(mask_indices), p=[0.8, 0.1, 0.1]  \n",
    "        )\n",
    "\n",
    "        # Apply masks\n",
    "        for word_idx, mask_type in zip(mask_indices, mask_types):\n",
    "            # Mask normal text\n",
    "            normal_start = normal_cumsum[word_idx]\n",
    "            normal_end = normal_start + normal_token_lengths[word_idx]\n",
    "            normal_mask[normal_start:normal_end] = mask_type\n",
    "\n",
    "            # Mask phonetic text\n",
    "            phonetic_start = phonetic_cumsum[word_idx]\n",
    "            phonetic_end = phonetic_start + phonetic_token_lengths[word_idx]\n",
    "            phonetic_mask[phonetic_start:phonetic_end] = mask_type\n",
    "\n",
    "        return normal_mask, phonetic_mask, normal_encoding, phonetic_encoding\n",
    "\n",
    "    def _get_step_size(self, word: str, type: int) -> int:\n",
    "        \"\"\"return the number of tokens in a word\"\"\"\n",
    "        cache = self.normal_cache if type == 0 else self.phonetic_cache\n",
    "        tokenizer = self.normal_tokenizer if type == 0 else self.phonetic_tokenizer\n",
    "        if word in cache:\n",
    "            return cache[word]\n",
    "        tokens = tokenizer(word, add_special_tokens=False)['input_ids']\n",
    "        cache[word] = len(tokens)\n",
    "        return cache[word]\n",
    "        \n",
    "\n",
    "    def __call__(self, examples: List[Dict[str, str]]) -> Dict[str, torch.Tensor]:\n",
    "        # Tokenize and process examples\n",
    "        batch_input_ids, batch_attention_masks, batch_token_type_ids, batch_labels = [], [], [], []\n",
    "\n",
    "        for example in examples:\n",
    "            normal_text = example[\"original_text\"]\n",
    "            phonetic_text = example[\"text\"]\n",
    "\n",
    "            # Create masks\n",
    "            normal_mask, phonetic_mask, normal_encoding, phonetic_encoding = (\n",
    "                self._create_aligned_masks(normal_text, phonetic_text)\n",
    "            )\n",
    "\n",
    "            # Combine normal and phonetic text\n",
    "            final_input_ids = torch.cat(\n",
    "                [\n",
    "                    torch.tensor([self.normal_tokenizer.cls_token_id]),  # [CLS]\n",
    "                    normal_encoding[0],\n",
    "                    torch.tensor([self.normal_tokenizer.sep_token_id]),  # [SEP]\n",
    "                    phonetic_encoding[0],\n",
    "                    torch.tensor([self.normal_tokenizer.sep_token_id]),  # Final [SEP]\n",
    "                ],\n",
    "            )\n",
    "\n",
    "            # Create attention mask\n",
    "            attention_mask = torch.ones(len(final_input_ids))\n",
    "\n",
    "            # Create token type IDs\n",
    "            # +1 for [SEP]\n",
    "            normal_type_ids = torch.zeros(normal_encoding.size(1))\n",
    "            phonetic_type_ids = torch.ones(phonetic_encoding.size(1))\n",
    "            token_type_ids = torch.cat(\n",
    "                [\n",
    "                    torch.tensor([0]),\n",
    "                    normal_type_ids,\n",
    "                    torch.tensor([0]),\n",
    "                    phonetic_type_ids,\n",
    "                    torch.tensor([1]),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # Create labels\n",
    "            labels = final_input_ids.clone()\n",
    "\n",
    "            # Apply masks\n",
    "            combined_mask = torch.cat(\n",
    "                [\n",
    "                    torch.tensor([0]),  # For [CLS]\n",
    "                    normal_mask,\n",
    "                    torch.tensor([0]),  # For [SEP]\n",
    "                    phonetic_mask,\n",
    "                    torch.tensor([0]),  # For final [SEP]\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            # Get vocabulary size for random token selection\n",
    "            vocab_size = len(self.normal_tokenizer.vocab)\n",
    "\n",
    "            # Apply different masking strategies\n",
    "            for i in range(len(final_input_ids)):\n",
    "                if combined_mask[i] == 1:  # 80% - Replace with [MASK]\n",
    "                    final_input_ids[i] = self.normal_tokenizer.mask_token_id\n",
    "                elif combined_mask[i] == 2:  # 10% - Replace with random token\n",
    "                    final_input_ids[i] = random.randint(0, vocab_size - 1)\n",
    "\n",
    "            # Set labels\n",
    "            labels = torch.where(combined_mask > 0, labels, -100)\n",
    "\n",
    "            # Pad if necessary\n",
    "            if len(final_input_ids) < self.max_length:\n",
    "                padding_length = self.max_length - len(final_input_ids)\n",
    "                attention_mask = torch.cat([attention_mask, torch.zeros(padding_length)])\n",
    "                token_type_ids = torch.cat([token_type_ids, torch.zeros(padding_length)])\n",
    "                labels = torch.cat([labels, torch.tensor([-100] * padding_length)])\n",
    "                final_input_ids = torch.cat(\n",
    "                    [\n",
    "                        final_input_ids,\n",
    "                        torch.tensor([self.normal_tokenizer.pad_token_id] * padding_length),\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "            # Add to batch\n",
    "            batch_input_ids.append(final_input_ids)\n",
    "            batch_attention_masks.append(attention_mask)\n",
    "            batch_token_type_ids.append(token_type_ids)\n",
    "            batch_labels.append(labels)\n",
    "\n",
    "        # Stack tensors\n",
    "        return {\n",
    "            \"input_ids\": torch.stack(batch_input_ids).long(),\n",
    "            \"attention_mask\": torch.stack(batch_attention_masks).long(),\n",
    "            \"token_type_ids\": torch.stack(batch_token_type_ids).long(),\n",
    "            \"labels\": torch.stack(batch_labels).long(),\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = CustomDataCollatorForLanguageModeling(normal_tokenizer, phonetic_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['v{lkIr\\\\i@ kr\\\\An@k@lz ajIi',\n",
       "  'sEndZ now v{lkIr\\\\i@   Vnr\\\\IkOr\\\\dId kr\\\\An@k@lz dZ{p@niz    lIt  v{lkIr\\\\i@ Vv D@ b{t@lfild   kAm@nli r\\\\@fr\\\\=d t@ {z v{lkIr\\\\i@ kr\\\\An@k@lz ajIi awtsajd dZ@p{n  Iz @ t{ktIk@l r\\\\owl plejIN vIdiow gejm dIvEl@pt baj sig@ {nd midi@  vIZ@n fOr\\\\ D@ plejstejS@n pOr\\\\t@b@l  r\\\\ilist In dZ{njuEr\\\\i  In dZ@p{n  It Iz D@ Tr\\\\=d gejm In D@ v{lkIr\\\\i@ sIr\\\\iz  EmplojIN D@ sejm fjuZ@n Vv t{ktIk@l {nd r\\\\il tajm gAmplej {z Its pr\\\\Ed@sEsr\\\\=z  D@ stOr\\\\i r\\\\Vnz pEr\\\\@lEl t@ D@ fr\\\\=st gejm {nd fAlowz D@  nejml@s   @ pin@l',\n",
       "  'mIl@tEr\\\\i jun@t sr\\\\=vIN D@ nejS@n Vv g{li@ dUr\\\\IN D@ sEk@nd jUr\\\\owp{n wOr\\\\ hu pr\\\\=fOr\\\\m sikr\\\\@t bl{k Apr\\\\=ejS@nz {nd Ar\\\\ pIt@d @gEnst D@ ImpIr\\\\i@l jun@t  k@l@m@ti r\\\\ejv@n  ',\n",
       "  'D@ gejm bIg{n dIvEl@pm@nt In   k{r\\\\iIN owvr\\\\= @ lAr\\\\dZ pOr\\\\S@n Vv D@ wr\\\\=k dVn An v{lkIr\\\\i@ kr\\\\An@k@lz Ii  wajl It r\\\\Itejnd D@ st{ndr\\\\=d fitSr\\\\=z Vv D@ sIr\\\\iz  It Olsow Vndr\\\\=wEnt mVlt@p@l @dZVstm@nts  sVtS {z mejkIN D@ gejm mOr\\\\ fr\\\\=gIvIN fOr\\\\ sIr\\\\iz nukVmr\\\\=z  kEr\\\\Iktr\\\\= dIzajnr\\\\= r\\\\ejt@ howndZu {nd k@mpowzr\\\\= hItowSi sAkImowtow bowT r\\\\Itr\\\\=nd fr\\\\Vm pr\\\\ivi@s Entr\\\\iz  @lON wID v{lkIr\\\\i@ kr\\\\An@k@lz Ii dr\\\\=Ektr\\\\= t@kESi owzAw@  @ lAr\\\\dZ tim Vv r\\\\ajtr\\\\=z h{nd@ld D@ skr\\\\Ipt  D@ gejm  Es owp@nIN Tim wAz sVN baj mej  En ',\n",
       "  'It mEt wID pAz@tIv sejlz In dZ@p{n  {nd wAz pr\\\\ejzd baj bowT dZ{p@niz {nd wEstr\\\\=n kr\\\\ItIks  {ftr\\\\= r\\\\ilis  It r\\\\@sivd dawnlowd@b@l kAntEnt  @lON wID {n Iksp{nd@d @dIS@n In nowvEmbr\\\\= Vv D{t jIr\\\\  It wAz Olsow @d{pt@d Intu m{Ng@ {nd {n r\\\\=IdZ@n@l vIdiow {n@mejS@n sIr\\\\iz  du t@ low sejlz Vv v{lkIr\\\\i@ kr\\\\An@k@lz Ii  v{lkIr\\\\i@ kr\\\\An@k@lz ajIi wAz nAt lowk@lajzd  bVt @ f{n tr\\\\{nzlejS@n k@mp{t@b@l wID D@ gejm  Es Iksp{nd@d @dIS@n wAz r\\\\ilist In   midi@  vIZ@n wUd r\\\\Itr\\\\=n t@ D@ fr\\\\{ntSajz wID D@ dIvEl@pm@nt Vv v{lkIr\\\\i@  {Zr\\\\= r\\\\Ev@luS@n fOr\\\\'],\n",
       " 'original_text': ['valkyria chronicles iii',\n",
       "  'senj no valkyria 3 : unrecorded chronicles  japanese : 3 , lit . valkyria of the battlefield 3  , commonly referred to as valkyria chronicles iii outside japan , is a tactical role  playing video game developed by sega and media . vision for the playstation portable . released in january 2011 in japan , it is the third game in the valkyria series . employing the same fusion of tactical and real  time gameplay as its predecessors , the story runs parallel to the first game and follows the \" nameless \" , a penal',\n",
       "  'military unit serving the nation of gallia during the second europan war who perform secret black operations and are pitted against the imperial unit \" calamaty raven \" .',\n",
       "  \"the game began development in 2010 , carrying over a large portion of the work done on valkyria chronicles ii . while it retained the standard features of the series , it also underwent multiple adjustments , such as making the game more forgiving for series newcomers . character designer raita honjou and composer hitoshi sakimoto both returned from previous entries , along with valkyria chronicles ii director takeshi ozawa . a large team of writers handled the script . the game ' s opening theme was sung by may ' n .\",\n",
       "  \"it met with positive sales in japan , and was praised by both japanese and western critics . after release , it received downloadable content , along with an expanded edition in november of that year . it was also adapted into manga and an original video animation series . due to low sales of valkyria chronicles ii , valkyria chronicles iii was not localized , but a fan translation compatible with the game ' s expanded edition was released in 2014 . media . vision would return to the franchise with the development of valkyria : azure revolution for\"]}"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_from_disk(\"/home/toure215/BERT_phonetic/DATASETS/phonetic_wikitext\")\n",
    "dataset[\"train\"][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello my name is Paulevec I am a student at the University of Toronto\n",
      "h@low maj nejm Iz pOlIvIk aj {m @ stud@nt {t D@ jun@vr\\=s@ti Vv tr\\=Antow\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Hello, my name is Paulevec. I am a student at the University of Toronto.\"\n",
    "def split_words_and_punctuation(text):\n",
    "    return re.findall(r'\\w+', text, re.UNICODE)\n",
    "\n",
    "text_list = split_words_and_punctuation(text)\n",
    "text = \" \".join(text_list)\n",
    "print(text)\n",
    "\n",
    "phonetic_text = \" \".join(\"\".join(epi.xsampa_list(word)) for word in text_list)\n",
    "print(phonetic_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [\n",
    "    {\"original_text\": text, \"text\": phonetic_text},\n",
    "]\n",
    "normal_ids = normal_tokenizer(text, add_special_tokens=False, padding=False, truncation=True, max_length=50)[\"input_ids\"]\n",
    "phonetic_ids = phonetic_tokenizer(phonetic_text, add_special_tokens=False, padding=False, truncation=True, max_length=50)[\"input_ids\"]\n",
    "\n",
    "normal_tokens = normal_tokenizer.convert_ids_to_tokens(normal_ids)\n",
    "phonetic_tokens = phonetic_tokenizer.convert_ids_to_tokens(phonetic_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'my', 'name', 'is', 'paul', '##ev', '##ec', 'i', 'am', 'a', 'student', 'at', 'the', 'university', 'of', 'toronto']\n",
      "['h', '@', 'low', 'maj', 'nejm', 'iz', 'poli', '##vik', 'aj', '{', 'm', '@', 'stud', '@', 'nt', '{', 't', 'd', '@', 'jun', '@', 'vr', '\\\\=', 's', '@', 'ti', 'vv', 'tr', '\\\\=', 'antow']\n",
      "[20585, 950, 1140, 161, 2138, 2635, 116, 31, 298, 23, 3772, 159, 88, 1301, 100, 6221]\n",
      "[14, 6, 265, 379, 416, 97, 12371, 11357, 96, 30, 19, 6, 652, 6, 87, 30, 25, 10, 6, 254, 6, 233, 56, 24, 6, 101, 64, 82, 56, 3290]\n",
      "[PAD]\n"
     ]
    }
   ],
   "source": [
    "print(normal_tokens)\n",
    "print(phonetic_tokens)\n",
    "print(normal_ids)\n",
    "print(phonetic_ids)\n",
    "print(normal_tokenizer.decode(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1, 20585,   950,  1140,   161,  2138,  2635,   116,    31, 21618,\n",
       "             23,  3772,   159,    88,     4,   100,  6221,     2,    14,     6,\n",
       "            265,   379,   416,    97, 12371, 11357,    96,  4743, 28520,     6,\n",
       "            652,     6,    87,    30,    25,    10,     6,     4,     4,     4,\n",
       "              4,     4,     4,     4,    64,    82,    56,  3290,     2,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3,     3,     3,\n",
       "              3,     3,     3,     3,     3,     3,     3,     3]]),\n",
       " 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1,\n",
       "          1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "          1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "          0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'labels': tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100,  298, -100, -100,\n",
       "          -100, -100, 1301, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100,   30,   19, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100,  254,    6,  233,   56,   24,    6,  101, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
       "          -100, -100, -100, -100, -100, -100, -100, -100]])}"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_ipa = AutoTokenizer.from_pretrained(\"/home/toure215/BERT_phonetic/tokenizers/tokenizer_phonetic_BPE_IPA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[CLS]', 'həlow', ',', 'maj', 'nejm', 'ɪz', 'pɔl', 'ɪvɪk', '.', 'aj', 'æm', 'ə', 'studənt', 'æt', 'ðə', 'junəvɹsəti', 'ʌv', 'tɹɑntow', '.', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, my name is Paulevec. I am a student at the University of Toronto.\"\n",
    "text = epi.transliterate(text)\n",
    "tokenized_text = tokenizer_ipa(text)\n",
    "tokens = tokenizer_ipa.convert_ids_to_tokens(tokenized_text[\"input_ids\"])\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
