{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset, load_from_disk\n",
    "from transformers import AutoTokenizer, DataCollatorForLanguageModeling\n",
    "import epitran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f637fc838704026883a9854ba1396d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.14k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "072f5a156e784a379d3cde81e249254b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7dd8f4b751ec4d12b0a74cdc04047272",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/695 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "normal_tokenizer = AutoTokenizer.from_pretrained(\"psktoure/BERT_BPE_wikitext\")\n",
    "phonetic_tokenizer = AutoTokenizer.from_pretrained(\"psktoure/BERT_BPE_phonetic_wikitext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len normal tokenizer vocab:  30522\n",
      "Len phonetic tokenizer vocab:  30522\n"
     ]
    }
   ],
   "source": [
    "print(\"Len normal tokenizer vocab: \", len(normal_tokenizer.get_vocab()))\n",
    "print(\"Len phonetic tokenizer vocab: \", len(phonetic_tokenizer.get_vocab()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "epi = epitran.Epitran(\"eng-Latn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': [1, 12325, 8, 434, 935, 68, 2034, 9, 31, 79, 23, 3467, 57, 51, 1184, 65, 5758, 9, 2], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "['[CLS]', 'hello', ',', 'my', 'name', 'is', 'paul', '.', 'i', 'am', 'a', 'student', 'at', 'the', 'university', 'of', 'toronto', '.', '[SEP]']\n",
      "[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, None]\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, my name is Paul. I am a student at the University of Toronto.\"\n",
    "tokenized_text = normal_tokenizer(text)\n",
    "print(tokenized_text)\n",
    "ids_to_tokens = normal_tokenizer.convert_ids_to_tokens(tokenized_text[\"input_ids\"])\n",
    "print(ids_to_tokens)\n",
    "print(tokenized_text.word_ids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "nums = [1, 4, 5, 4]\n",
    "idx = nums.index(4)\n",
    "print(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xsampa_tokens(word, phonetic_tokenizer):\n",
    "    phonetic_word = \"\".joint(epi.xsampa_list(word))\n",
    "    tokenized_word = phonetic_tokenizer(phonetic_word, add_special_tokens=False)\n",
    "    ids = tokenized_word[\"input_ids\"]\n",
    "    return ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "import random\n",
    "\n",
    "class CustomDataCollatorForMLM(DataCollatorForLanguageModeling):\n",
    "    def __init__(self, tokenizer, phonetic_tokenizer, word_to_phonetic, mlm_probability=0.15):\n",
    "        super().__init__(tokenizer=tokenizer, mlm_probability=mlm_probability)\n",
    "        self.phonetic_tokenizer = phonetic_tokenizer\n",
    "        self.word_to_phonetic = word_to_phonetic\n",
    "\n",
    "    def __call__(self, examples):\n",
    "        # Tokenize normal and phonetic text\n",
    "        normal_texts = [e['normal_text'] for e in examples]\n",
    "        phonetic_texts = [e['phonetic_text'] for e in examples]\n",
    "        \n",
    "        # Tokenize both\n",
    "        normal_encodings = self.tokenizer(normal_texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        phonetic_encodings = self.phonetic_tokenizer(phonetic_texts, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        \n",
    "        # Generate MLM masks for normal text\n",
    "        input_ids = normal_encodings.input_ids\n",
    "        labels = input_ids.clone()  # Original labels for computing loss\n",
    "        probability_matrix = torch.full(labels.shape, self.mlm_probability)\n",
    "        special_tokens_mask = self.tokenizer.get_special_tokens_mask(input_ids.tolist(), already_has_special_tokens=True)\n",
    "        probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n",
    "        \n",
    "        masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "        input_ids[masked_indices] = self.tokenizer.mask_token_id  # Replace with mask token\n",
    "\n",
    "        # Handle corresponding phonetic tokens\n",
    "        phonetic_labels = phonetic_encodings.input_ids.clone()\n",
    "        for idx, (normal_sentence, phonetic_sentence) in enumerate(zip(normal_texts, phonetic_texts)):\n",
    "            # Get masked words\n",
    "            for token_idx in masked_indices[idx].nonzero():\n",
    "                word_id = normal_encodings.word_ids(batch_index=idx)[token_idx.item()]\n",
    "                if word_id is not None:  # Ignore special tokens\n",
    "                    word = self.tokenizer.decode(normal_encodings.input_ids[idx][word_id])\n",
    "                    phonetic_tokens = self.word_to_phonetic.get(word, [])\n",
    "                    # Find and mask in phonetic text\n",
    "                    for p_token in phonetic_tokens:\n",
    "                        p_index = phonetic_encodings.input_ids[idx].tolist().index(p_token)\n",
    "                        phonetic_encodings.input_ids[idx][p_index] = self.phonetic_tokenizer.mask_token_id\n",
    "\n",
    "        # Return modified normal and phonetic encodings\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'labels': labels,\n",
    "            'phonetic_input_ids': phonetic_encodings.input_ids,\n",
    "            'phonetic_labels': phonetic_labels\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,\n",
      "         0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,\n",
      "         0.1500]])\n",
      "[0]\n",
      "tensor([[0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,\n",
      "         0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500, 0.1500,\n",
      "         0.1500]])\n",
      "tensor([[False, False, False, False, False, False, False,  True, False, False,\n",
      "         False, False, False,  True, False,  True, False, False, False]])\n"
     ]
    }
   ],
   "source": [
    "text = \"Hello, my name is Paul. I am a student at the University of Toronto.\"\n",
    "encoded = normal_tokenizer(text, return_tensors=\"pt\")\n",
    "labels = encoded.input_ids.clone()\n",
    "probability_matrix = torch.full(labels.shape, 0.15)\n",
    "print(probability_matrix)\n",
    "special_tokens_mask = normal_tokenizer.get_special_tokens_mask(encoded.input_ids.tolist(), already_has_special_tokens=True)\n",
    "print(special_tokens_mask)\n",
    "probability_matrix.masked_fill_(torch.tensor(special_tokens_mask, dtype=torch.bool), value=0.0)\n",
    "print(probability_matrix)\n",
    "masked_indices = torch.bernoulli(probability_matrix).bool()\n",
    "print(masked_indices)\n",
    "normal_tokenizer.pad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    1, 12325,     8,   434,   935,    68,  2034,     9,    31,    79,\n",
      "            23,  3467,    57,    51,  1184,    65,  5758,     9,     2]])\n"
     ]
    }
   ],
   "source": [
    "print(encoded.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "print(normal_tokenizer.encode(normal_tokenizer.cls_token, add_special_tokens=False))\n",
    "print(normal_tokenizer.encode(phonetic_tokenizer.cls_token, add_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
