{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, DataCollatorForLanguageModeling, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import epitran\n",
    "from functools import lru_cache\n",
    "from difflib import SequenceMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_phonetic = False\n",
    "epi = epitran.Epitran(\"eng-Latn\")\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def xsampa_list(word: str) -> list:\n",
    "    return epi.xsampa_list(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Verse</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ah why this boding start this sudden pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>that wings my pulse and shoots from vein to vein</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what mean regardless of yon midnight bell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>these earthborn visions saddening oer my cell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>what strange disorder prompts these thoughts t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Verse\n",
       "0          ah why this boding start this sudden pain\n",
       "1   that wings my pulse and shoots from vein to vein\n",
       "2          what mean regardless of yon midnight bell\n",
       "3      these earthborn visions saddening oer my cell\n",
       "4  what strange disorder prompts these thoughts t..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_dataset = pd.read_csv('/home/toure215/BERT_phonetic/DATASETS/verses/super_verses.csv')\n",
    "pd_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ah why this boding start this sudden pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>that wings my pulse and shoots from vein to vein</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what mean regardless of yon midnight bell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>these earthborn visions saddening oer my cell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>what strange disorder prompts these thoughts t...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          input_text\n",
       "0          ah why this boding start this sudden pain\n",
       "1   that wings my pulse and shoots from vein to vein\n",
       "2          what mean regardless of yon midnight bell\n",
       "3      these earthborn visions saddening oer my cell\n",
       "4  what strange disorder prompts these thoughts t..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_dataset = pd_dataset.rename({\"Verse\": \"input_text\"}, axis='columns')\n",
    "pd_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_last_word(verse: str) -> str:\n",
    "    return verse.split()[-1]\n",
    "\n",
    "def mask_last_word(verse: str) -> str:\n",
    "    words = verse.split()\n",
    "    words[-1] = '[MASK]'\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_dataset['target_word'] = pd_dataset['input_text'].apply(get_last_word)\n",
    "# pd_dataset['input_text'] = pd_dataset['input_text'].apply(mask_last_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_text</th>\n",
       "      <th>target_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ah why this boding start this sudden pain</td>\n",
       "      <td>pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>that wings my pulse and shoots from vein to vein</td>\n",
       "      <td>vein</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what mean regardless of yon midnight bell</td>\n",
       "      <td>bell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>these earthborn visions saddening oer my cell</td>\n",
       "      <td>cell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>what strange disorder prompts these thoughts t...</td>\n",
       "      <td>glow</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          input_text target_word\n",
       "0          ah why this boding start this sudden pain        pain\n",
       "1   that wings my pulse and shoots from vein to vein        vein\n",
       "2          what mean regardless of yon midnight bell        bell\n",
       "3      these earthborn visions saddening oer my cell        cell\n",
       "4  what strange disorder prompts these thoughts t...        glow"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(pd_dataset, test_size=0.1, random_state=42, shuffle=True)\n",
    "train, val = train_test_split(train, test_size=0.1, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_train = Dataset.from_pandas(train)\n",
    "hf_val = Dataset.from_pandas(val)\n",
    "hf_test = Dataset.from_pandas(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['input_text', 'target_word'],\n",
       "        num_rows: 440950\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['input_text', 'target_word'],\n",
       "        num_rows: 48995\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['input_text', 'target_word'],\n",
       "        num_rows: 54439\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hf_dataset = DatasetDict({\"train\": hf_train, \"validation\": hf_val, \"test\": hf_test}).remove_columns(['__index_level_0__'])\n",
    "hf_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/home/toure215/miniconda3/envs/bert/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_path = ['bert-base-uncased','psktoure/BERT_BPE_phonetic_wikitext-103-raw-v1','psktoure/BERT_WordLevel_phonetic_wikitext-103-raw-v1']\n",
    "\n",
    "if is_phonetic:\n",
    "    model = AutoModelForMaskedLM.from_pretrained(model_path[1])\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path[1])\n",
    "else:\n",
    "    model = AutoModelForMaskedLM.from_pretrained(model_path[0])\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(sentence: str) -> str:\n",
    "    words = sentence.split()\n",
    "    for i in range(len(words)):\n",
    "        words[i] = ''.join(xsampa_list(words[i]))\n",
    "    return ' '.join(words)\n",
    "\n",
    "def translate_function(examples):\n",
    "    examples['input_text'] = [translate_sentence(sentence) for sentence in examples['input_text']]\n",
    "    examples['target_word'] = [''.join(xsampa_list(word)) for word in examples['target_word']]\n",
    "    return examples\n",
    "    \n",
    "\n",
    "def tokenize_function(examples):\n",
    "    inputs = tokenizer(examples['input_text'], padding='max_length', truncation=True, max_length=50)\n",
    "    targets = tokenizer(examples['target_word'], padding='max_length', truncation=True, max_length=5)\n",
    "    inputs['labels'] = targets['input_ids']\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if is_phonetic:\n",
    "    hf_dataset = hf_dataset.map(translate_function, batched=True, num_proc=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_dataset = hf_dataset.map(tokenize_function, batched=True, num_proc=15, remove_columns=['input_text', 'target_word'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch   \n",
    "from transformers import PreTrainedTokenizerBase\n",
    "\n",
    "class CustomDataCollator:\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizerBase, padding=True, max_length=50):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.mask_token_id = tokenizer.mask_token_id\n",
    "        self.padding = padding\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __call__(self, examples):\n",
    "       \n",
    "        input_texts = [example['input_text'] for example in examples]\n",
    "        target_words = [example['target_word'] for example in examples]\n",
    "\n",
    "        batch = self.tokenizer(\n",
    "            input_texts,\n",
    "            padding=self.padding,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        labels = input_ids.clone()\n",
    "\n",
    "        for i, idx in enumerate(input_ids):\n",
    "            sep_token_indices = torch.where(idx == self.tokenizer.sep_token_id)[0]\n",
    "            if len(sep_token_indices) == 0:\n",
    "                raise ValueError(f\"[SEP] token not found in input_ids: {idx}\")\n",
    "            \n",
    "            masked_token_idx = sep_token_indices[0] - 1\n",
    "            input_ids[i, masked_token_idx] = self.mask_token_id \n",
    "            labels[i, :masked_token_idx] = -100 \n",
    "            labels[i, masked_token_idx + 1:] = -100\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": batch[\"attention_mask\"],\n",
    "            \"labels\": labels,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=True, mlm_probability=0.15)\n",
    "data_collator = CustomDataCollator(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mask_token_id:  103 \n",
      "mask_token: [MASK]\n",
      "[{'input_text': 'wide wave the flaming sword and send o send', 'target_word': 'send'}, {'input_text': 'the marble leaps or shrinks or burns', 'target_word': 'burns'}]\n",
      "input_ids : tensor([[  101,  2898,  4400,  1996, 19091,  4690,  1998,  4604,  1051,   103,\n",
      "           102],\n",
      "        [  101,  1996,  7720, 29195,  2030, 22802,  2015,  2030,   103,   102,\n",
      "             0]])\n",
      "attention_mask : tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0]])\n",
      "labels : tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, 4604, -100],\n",
      "        [-100, -100, -100, -100, -100, -100, -100, -100, 7641, -100, -100]])\n",
      "shine\n"
     ]
    }
   ],
   "source": [
    "sample = hf_dataset['train'][:2]\n",
    "sample_list = [{key: sample[key][i] for key in sample} for i in range(len(sample['input_text']))]\n",
    "print(\"mask_token_id: \", tokenizer.mask_token_id, \"\\nmask_token:\", tokenizer.mask_token)\n",
    "print(sample_list)\n",
    "c = data_collator(sample_list)\n",
    "for key in c:\n",
    "    print(key ,\":\", c[key])\n",
    "print(tokenizer.decode(12342))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rhyme_score(word1: str, word2: str) -> int:\n",
    "    if not is_phonetic:    \n",
    "        end1 = xsampa_list(word1)\n",
    "        end2 = xsampa_list(word2)\n",
    "    else:\n",
    "        end1 = word1\n",
    "        end2 = word2\n",
    "    length = min(len(end1), len(end2), 3)\n",
    "    end1 = end1[-length:]\n",
    "    end2 = end2[-length:]\n",
    "    return SequenceMatcher(None, end1, end2).ratio()\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    preds = torch.argmax(pred.predictions, dim=-1)\n",
    "    batch_size = 32  \n",
    "    total_rhyme_score = 0\n",
    "    count = 0\n",
    "\n",
    "    for i in range(0, len(preds), batch_size):\n",
    "        batch_preds = preds[i:i + batch_size]\n",
    "        batch_labels = pred.label_ids[i:i + batch_size]\n",
    "\n",
    "        # Decode predictions and labels in batches\n",
    "        decoded_preds = tokenizer.batch_decode(batch_preds, skip_special_tokens=True)\n",
    "        decoded_labels = tokenizer.batch_decode(batch_labels, skip_special_tokens=True)\n",
    "\n",
    "        # Compute rhyme scores for the batch\n",
    "        batch_rhyme_scores = [\n",
    "            rhyme_score(pred_word, target_word)\n",
    "            for pred_word, target_word in zip(decoded_preds, decoded_labels)\n",
    "        ]\n",
    "        \n",
    "        total_rhyme_score += sum(batch_rhyme_scores)\n",
    "        count += len(batch_rhyme_scores)\n",
    "\n",
    "    # Compute the mean rhyme score\n",
    "    mean_rhyme_score = total_rhyme_score / count if count > 0 else 0\n",
    "    return {\"rhyme_score\": mean_rhyme_score}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='/tmp/verses',\n",
    "    num_train_epochs=15,\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=256,\n",
    "    per_device_eval_batch_size=256,\n",
    "    eval_strategy='epoch',\n",
    "    logging_strategy='no',\n",
    "    save_strategy='no',\n",
    "    report_to='none',\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=hf_dataset['train'],\n",
    "    eval_dataset=hf_dataset['validation'],\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ece446f66bb4101a467ea827feedd6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25845 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52a763669ec04757a5702fec314fc2d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/192 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 5.191527366638184, 'eval_runtime': 18.5094, 'eval_samples_per_second': 2647.038, 'eval_steps_per_second': 10.373, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ca7074fbbf94941a9044bdb1ccffe81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/192 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 5.19736909866333, 'eval_runtime': 18.6659, 'eval_samples_per_second': 2624.844, 'eval_steps_per_second': 10.286, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ecf87688e664c3b99ef6ae6d2fd5ac2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/192 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 5.389641761779785, 'eval_runtime': 18.7071, 'eval_samples_per_second': 2619.059, 'eval_steps_per_second': 10.263, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26f394dd45014a48a0074a927f5b6f66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/192 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 5.762287616729736, 'eval_runtime': 18.762, 'eval_samples_per_second': 2611.394, 'eval_steps_per_second': 10.233, 'epoch': 4.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47b162a70bdc485abcef1e78a7f74176",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/192 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.129112720489502, 'eval_runtime': 18.7715, 'eval_samples_per_second': 2610.072, 'eval_steps_per_second': 10.228, 'epoch': 5.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "225f0632e7984b438573e0d486d873db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/192 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.395214557647705, 'eval_runtime': 18.8069, 'eval_samples_per_second': 2605.161, 'eval_steps_per_second': 10.209, 'epoch': 6.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b2cec01ec6849179a131925b2e6517a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/192 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.68097448348999, 'eval_runtime': 18.7488, 'eval_samples_per_second': 2613.237, 'eval_steps_per_second': 10.241, 'epoch': 7.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c74d8eca61b34f7d94a68674da53a29c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/192 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 6.88824987411499, 'eval_runtime': 18.8287, 'eval_samples_per_second': 2602.145, 'eval_steps_per_second': 10.197, 'epoch': 8.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2b0e99e8fd9426abb93031efc984fed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/192 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 7.077701568603516, 'eval_runtime': 18.8182, 'eval_samples_per_second': 2603.599, 'eval_steps_per_second': 10.203, 'epoch': 9.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0c04420e48a4f8a90ba3fa6ed33c925",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/192 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 7.221578121185303, 'eval_runtime': 18.8363, 'eval_samples_per_second': 2601.101, 'eval_steps_per_second': 10.193, 'epoch': 10.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0493ab3be8ca4ec8a6de204decd3d384",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/192 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 7.334627151489258, 'eval_runtime': 18.8381, 'eval_samples_per_second': 2600.848, 'eval_steps_per_second': 10.192, 'epoch': 11.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b51b50a932264e929bdd8967c13c3300",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/192 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 7.419206619262695, 'eval_runtime': 18.8577, 'eval_samples_per_second': 2598.146, 'eval_steps_per_second': 10.182, 'epoch': 12.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a356596092fd49b38d6db07cea569a57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/192 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 7.486563205718994, 'eval_runtime': 19.0466, 'eval_samples_per_second': 2572.373, 'eval_steps_per_second': 10.081, 'epoch': 13.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3425270ed2d9412bbef251415cd8c2f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/192 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 7.515157699584961, 'eval_runtime': 18.8864, 'eval_samples_per_second': 2594.191, 'eval_steps_per_second': 10.166, 'epoch': 14.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35bdc6308f1e405fa4017d11f65b4d56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/192 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 7.534763813018799, 'eval_runtime': 18.8661, 'eval_samples_per_second': 2596.986, 'eval_steps_per_second': 10.177, 'epoch': 15.0}\n",
      "{'train_runtime': 7267.347, 'train_samples_per_second': 910.133, 'train_steps_per_second': 3.556, 'train_loss': 1.7260270059489262, 'epoch': 15.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=25845, training_loss=1.7260270059489262, metrics={'train_runtime': 7267.347, 'train_samples_per_second': 910.133, 'train_steps_per_second': 3.556, 'total_flos': 7.01194194977256e+16, 'train_loss': 1.7260270059489262, 'epoch': 15.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rhyme(model, dataset, tokenizer):\n",
    "    model = model.to(\"cuda\")\n",
    "    model.eval()\n",
    "    rhyme_scores = []\n",
    "    batch_size = 8\n",
    "\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        # if i > 16:\n",
    "        #     break\n",
    "        print(f\"Processing example {i}/{len(dataset)} ...\", end=\"\\r\")\n",
    "        batch = dataset[i : i + batch_size]\n",
    "        batch_sequence = [{key: batch[key][j] for key in batch} for j in range(len(batch[\"input_text\"]))]\n",
    "        inputs = data_collator(batch_sequence)\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "\n",
    "        for j in range(len(batch[\"input_text\"])):\n",
    "            masked_token_index = torch.where(inputs[\"input_ids\"][j] == tokenizer.mask_token_id)[0].item()\n",
    "            predicted_index = logits[j, masked_token_index].argmax(-1).item()\n",
    "            predicted_word = tokenizer.decode(predicted_index)\n",
    "            target = tokenizer.decode(inputs[\"labels\"][j, masked_token_index])\n",
    "            if i < 16 and j < 8:\n",
    "                print('predicted_word:', predicted_word, '-- target_word:', target)\n",
    "            rhyme_scores.append(rhyme_score(predicted_word, target))\n",
    "\n",
    "    return {\"score\": np.mean(rhyme_scores)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103\n",
      "[MASK]\n",
      "{'input_text': 'is he whose visage in the lazy mist', 'target_word': 'mist'}\n",
      "{'input_ids': [101, 2003, 2002, 3005, 9425, 3351, 1999, 1996, 13971, 11094, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
      "[MASK]\n",
      "[SEP]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.mask_token_id)\n",
    "print(tokenizer.mask_token)\n",
    "print(hf_dataset['test'][0])\n",
    "print(tokenizer(hf_dataset['test'][0]['input_text']))\n",
    "print(tokenizer.decode(103))\n",
    "print(tokenizer.decode(102))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_word: waves -- target_word: mist\n",
      "predicted_word: ##od -- target_word: ##od\n",
      "predicted_word: ridge -- target_word: wall\n",
      "predicted_word: eyes -- target_word: hands\n",
      "predicted_word: ##s -- target_word: ##shire\n",
      "predicted_word: cold -- target_word: vase\n",
      "predicted_word: to -- target_word: over\n",
      "predicted_word: curled -- target_word: wife\n",
      "predicted_word: iii -- target_word: she\n",
      "predicted_word: wood -- target_word: plant\n",
      "predicted_word: laid -- target_word: fly\n",
      "predicted_word: walls -- target_word: mayhem\n",
      "predicted_word: to -- target_word: down\n",
      "predicted_word: girl -- target_word: trick\n",
      "predicted_word: lend -- target_word: there\n",
      "predicted_word: ##pers -- target_word: ##pers\n",
      "Processing example 54432/54439 ...\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': np.float64(0.31773789623860343)}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_rhyme(model, hf_dataset['test'], tokenizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "# import ctypes\n",
    "# import torch\n",
    "# gc.collect()\n",
    "# libc = ctypes.CDLL(\"libc.so.6\") # clearing cache\n",
    "# libc.malloc_trim(0)\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
