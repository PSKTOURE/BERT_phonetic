{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from tokenizers.models import WordLevel, BPE\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.trainers import WordLevelTrainer, BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "import re\n",
    "import os\n",
    "import hashlib\n",
    "import time\n",
    "import multiprocessing\n",
    "import argparse\n",
    "from datasets import load_from_disk\n",
    "from lingua import Language, LanguageDetectorBuilder\n",
    "import epitran\n",
    "from functools import lru_cache\n",
    "from transformers import PreTrainedTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 4358\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1801350\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 3760\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_processes = multiprocessing.cpu_count() - 1\n",
    "# Exact duplication removal (on individual sentences/paragraphs)\n",
    "def remove_exact_duplicates(examples):\n",
    "    seen = set()\n",
    "    deduped_examples = []\n",
    "    for sentence in examples[\"text\"]:\n",
    "        hash_val = hashlib.md5(sentence.encode()).hexdigest()\n",
    "        if hash_val not in seen:\n",
    "            seen.add(hash_val)\n",
    "            deduped_examples.append(sentence)\n",
    "    return {\"text\": deduped_examples}\n",
    "\n",
    "\n",
    "def filter_by_language(examples):\n",
    "    detector = LanguageDetectorBuilder.from_languages(Language.ENGLISH, Language.FRENCH).build()\n",
    "    return {\n",
    "        \"text\": [\n",
    "            sentence for sentence in examples[\"text\"] if detector.detect_language_of(sentence) == Language.ENGLISH\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic text cleaning\n",
    "def clean_text(examples):\n",
    "    cleaned_text = []\n",
    "    for sentence in examples[\"text\"]:\n",
    "        # Lowercase\n",
    "        #sentence = sentence.lower()\n",
    "        # Remove extra spaces\n",
    "        sentence = re.sub(r\"\\s+\", \" \", sentence)\n",
    "        # Remove URLs\n",
    "        sentence = re.sub(r\"http\\S+\", \"\", sentence)\n",
    "        # Remove special characters\n",
    "        sentence = re.sub(r\"[^a-zA-Z0-9,.!?;:\\'\\\" ]+\", \"\", sentence)\n",
    "        cleaned_text.append(sentence.strip())\n",
    "    return {\"text\": cleaned_text}\n",
    "\n",
    "def clean(dataset):\n",
    "    dataset = dataset.map(remove_exact_duplicates, batched=True, num_proc=num_processes)\n",
    "    dataset = dataset.map(filter_by_language, batched=True, num_proc=num_processes)\n",
    "    dataset = dataset.map(clean_text, batched=True, num_proc=num_processes)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_cleaned = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 4358\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1801350\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 3760\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "epi = epitran.Epitran(\"eng-Latn\")\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def xsampa_list(word: str) -> list:\n",
    "    return epi.xsampa_list(word)\n",
    "\n",
    "def translate_sentence(sentence: str) -> str:\n",
    "    return ' '.join(' '.join(xsampa_list(word)) for word in sentence.split())\n",
    "\n",
    "def translate_function(examples):\n",
    "    return {\"text\": [translate_sentence(sentence) for sentence in examples[\"text\"]]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "324e7a0bb1134a2887267a834c281ab5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=15):   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bd3463228964bcbaf49b8a9c1c9fcf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=15):   0%|          | 0/1801350 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49394cc7c3824e86a6a19220377a0899",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=15):   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 4358\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1801350\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 3760\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_translated = dataset_cleaned.map(translate_function, batched=True, num_proc=num_processes)\n",
    "dataset_translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['',\n",
       "  ' v { l k I r\\\\ i @ k r\\\\ A n @ k @ l z a j I i ',\n",
       "  '',\n",
       "  's E n dZ o n o w v { l k I r\\\\ i @   V n r\\\\ I k O r\\\\ d I d k r\\\\ A n @ k @ l z  dZ { p @ n i z    l I t  v { l k I r\\\\ i @ V v D @ b { t @ l f i l d    k A m @ n l i r\\\\ @ f r\\\\= d t @ { z v { l k I r\\\\ i @ k r\\\\ A n @ k @ l z a j I i a w t s a j d dZ @ p { n  I z @ t { k t I k @ l r\\\\ o w l  p l e j I N v I d i o w g e j m d I v E l @ p t b a j s i g @ { n d m i d i @ v I Z @ n f O r\\\\ D @ p l e j s t e j S @ n p O r\\\\ t @ b @ l  r\\\\ i l i s t I n dZ { n j u E r\\\\ i  I n dZ @ p { n  I t I z D @ T r\\\\= d g e j m I n D @ v { l k I r\\\\ i @ s I r\\\\ i z  E m p l o j I N D @ s e j m f j u Z @ n V v t { k t I k @ l { n d r\\\\ i l  t a j m g A m p l e j { z I t s p r\\\\ E d @ s E s r\\\\= z  D @ s t O r\\\\ i r\\\\ V n z p E r\\\\ @ l E l t @ D @ f r\\\\= s t g e j m { n d f A l o w z D @  n e j m l @ s   @ p i n @ l m I l @ t E r\\\\ i j u n @ t s r\\\\= v I N D @ n e j S @ n V v g { l i @ d U r\\\\ I N D @ s E k @ n d j U r\\\\ o w p { n w O r\\\\ h u p r\\\\= f O r\\\\ m s i k r\\\\ @ t b l { k A p r\\\\= e j S @ n z { n d A r\\\\ p I t @ d @ g E n s t D @ I m p I r\\\\ i @ l j u n @ t  k @ l @ m @ t i r\\\\ e j v @ n  ',\n",
       "  'D @ g e j m b I g { n d I v E l @ p m @ n t I n   k { r\\\\ i I N o w v r\\\\= @ l A r\\\\ dZ p O r\\\\ S @ n V v D @ w r\\\\= k d V n A n v { l k I r\\\\ i @ k r\\\\ A n @ k @ l z I i  w a j l I t r\\\\ I t e j n d D @ s t { n d r\\\\= d f i tS r\\\\= z V v D @ s I r\\\\ i z  I t O l s o w V n d r\\\\= w E n t m V l t @ p @ l @ dZ V s t m @ n t s  s V tS { z m e j k I N D @ g e j m m O r\\\\ f r\\\\= g I v I N f O r\\\\ s I r\\\\ i z n u k V m r\\\\= z  k E r\\\\ I k t r\\\\= d I z a j n r\\\\= r\\\\ e j t @ h o w n dZ u { n d k @ m p o w z r\\\\= h I t o w S i s A k I m o w t o w b o w T r\\\\ I t r\\\\= n d f r\\\\ V m p r\\\\ i v i @ s E n t r\\\\ i z  @ l O N w I D v { l k I r\\\\ i @ k r\\\\ A n @ k @ l z I i d r\\\\= E k t r\\\\= t @ k E S i o w z A w @  @ l A r\\\\ dZ t i m V v r\\\\ a j t r\\\\= z h { n d @ l d D @ s k r\\\\ I p t  D @ g e j m z o w p @ n I N T i m w A z s V N b a j m e j n ']}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_translated[\"train\"][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_corpus(dataset):\n",
    "    dataset = concatenate_datasets([dataset[\"train\"], dataset[\"test\"], dataset[\"validation\"]])\n",
    "    for i in range(0, len(dataset), 1000):\n",
    "        samples = dataset[i : i + 1000]\n",
    "        yield samples[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tokenizer(dataset):\n",
    "    tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "    tokenizer.post_processor = TemplateProcessing(\n",
    "        single=\"[CLS] $A [SEP]\",\n",
    "        pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "        special_tokens=[\n",
    "            (\"[CLS]\", 1),\n",
    "            (\"[SEP]\", 2),\n",
    "        ],\n",
    "    )\n",
    "    trainer = WordLevelTrainer(vocab_size=1000 ,special_tokens=[\"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\", \"[UNK]\"])\n",
    "    tokenizer.train_from_iterator(get_training_corpus(dataset), trainer)\n",
    "    tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)\n",
    "    tokenizer.add_special_tokens(\n",
    "        {\n",
    "            \"pad_token\": \"[PAD]\",\n",
    "            \"mask_token\": \"[MASK]\",\n",
    "            \"cls_token\": \"[CLS]\",\n",
    "            \"sep_token\": \"[SEP]\",\n",
    "            \"unk_token\": \"[UNK]\",\n",
    "        }\n",
    "    )\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/toure215/miniconda3/envs/bert/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = train_tokenizer(dataset_translated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "155\n",
      "{'e_': 51, 'g_w_h': 147, 'T_': 143, 'a_X': 50, 'i_G': 73, 'i_X': 95, 'f': 29, 'S_w': 142, 'u_t': 106, 'a_': 48, 'B': 54, 'p_G': 151, 'tS': 40, '`:': 145, 'd_': 104, 'N': 36, 'e_X': 98, 'n': 7, '|\\\\': 111, 'k_h': 133, '7': 108, '&': 138, '1': 71, 'h': 35, 'm': 20, 't': 8, '9': 60, 'X': 57, '!\\\\': 124, 'k_w': 121, 'u_X': 74, 'l_G': 96, 't_h': 152, '{:': 110, 'O': 32, 's_w': 135, 'l_': 149, 'D': 23, 'j': 14, 'A': 25, 's_G': 76, 'g_': 132, \"'\": 59, 'o': 33, 'f_G': 100, 'u': 34, 'z_G': 123, 's_h': 134, '@:': 65, '|\\\\|\\\\': 117, 's_': 102, 'n_G': 97, 'u_k': 153, '[UNK]': 4, ':': 53, 'i_k': 148, '_h': 109, 'E': 22, '2': 49, 'V': 27, '8': 92, 'S': 37, 'G': 70, 'R_w': 141, '\\\\`': 99, 'g': 38, 'm_G': 85, 'v_w': 154, 'z': 19, \"`'\": 130, 'H': 112, 'f_w': 146, 'K': 75, 'P': 80, '@\\\\': 119, '[SEP]': 2, 'J': 67, '?': 66, '_H': 113, \"'=\\\\\": 86, 'k': 15, 'v': 26, '{': 17, '~:': 88, '}': 107, 'y': 55, 'i': 18, '_T': 91, 'o_X': 72, '>': 89, '_': 93, 'b_h': 131, 'n_w': 116, 'R': 58, 'g_w': 115, 'l': 13, 'b': 28, 'j_w': 101, 'c': 45, '6': 63, 'F': 125, 'S_': 127, 's': 10, 'n_': 150, '~': 52, 'X_w': 144, 'U': 42, '_L': 114, \"\\\\'\": 82, '<': 84, 'Y': 90, 'I': 9, 'e': 31, 'o_': 46, '\\\\': 11, 'u_': 47, '_G': 94, '`': 64, 'b_': 103, '?\\\\': 68, 'T': 41, 'r': 6, 'G_w': 126, 'L': 83, '3': 77, '[PAD]': 0, 'dZ': 39, 'd_h': 120, 'z_': 137, '?\\\\:': 118, '[MASK]': 3, 'K_': 140, 'Q': 61, '@': 5, '4_G': 69, 'W': 105, '[CLS]': 1, '\\\\=': 21, '5': 79, '\"': 44, 'i_': 56, 'w': 16, 'd': 12, 'y_': 78, 'a': 30, '_w': 129, '\\\\:': 128, '4': 62, 'p': 24, 't_w': 136, 't_': 122, 'Z': 43, 'M': 81, '_M': 87, 'B_G': 139}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.vocab_size)\n",
    "print(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/toure215/BERT_phonetic/tokenizers/tokenizer_phonetic_WordLevel/tokenizer_config.json',\n",
       " '/home/toure215/BERT_phonetic/tokenizers/tokenizer_phonetic_WordLevel/special_tokens_map.json',\n",
       " '/home/toure215/BERT_phonetic/tokenizers/tokenizer_phonetic_WordLevel/tokenizer.json')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"/home/toure215/BERT_phonetic/tokenizers/tokenizer_phonetic_WordLevel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunked_text(examples):\n",
    "    chunked_text = []\n",
    "    for sentence in examples[\"text\"]:\n",
    "        words = sentence.split()\n",
    "        chunks = [words[i : i + 200] for i in range(0, len(words), 200)]\n",
    "        chunked_text.extend([\" \".join(chunk) for chunk in chunks])\n",
    "    return {\"text\": chunked_text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47e5bbec10c34515b062f7ac208e9bad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=15):   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba47572c0ec94d4b8f054e699f36e5bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=15):   0%|          | 0/1801350 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b4677521b0e41dc9911e835428db9db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=15):   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86c61eea42164eefbbf2edd551e6c106",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/6226 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "482d709379ca4dda9613a3f5074bc993",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/2567227 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0eee71e21c37436e8fcbefc9c18251e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/5417 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "07a7d7ab6b8246af941ee54366ed7a90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/6226 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49fe28f7e4e745c48da31ac4ac077ff6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2567227 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "41afe6e0b62c4f50ab7699672bccc1f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/5417 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 6226\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 2567227\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 5417\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_chunked = (\n",
    "    dataset_translated.map(chunked_text, batched=True, num_proc=num_processes)\n",
    "    .flatten_indices()\n",
    "    .filter(lambda x: len(x[\"text\"]) > 0)\n",
    ")\n",
    "\n",
    "dataset_chunked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "202d05c0948a41d2a380334eaa4f574e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/6226 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57b732850a974c0fa37c9758f45ee583",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/2567227 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e76f54a0d02d40e6b80e8a2396c87461",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5417 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_chunked.save_to_disk(\"/home/toure215/BERT_phonetic/DATASETS/phoneme_wikitext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"/home/toure215/BERT_phonetic/tokenizers/tokenizer_phonetic_WordLevel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h @ l o w h a w A r\\ j u\n",
      "[1, 35, 5, 13, 33, 16, 35, 30, 16, 25, 6, 11, 14, 34, 2]\n",
      "[CLS] h @ l o w h a w A r \\ j u [SEP]\n"
     ]
    }
   ],
   "source": [
    "s = translate_sentence(\"Hello, how are you?\")\n",
    "print(s)\n",
    "e = tokenizer.encode(s)\n",
    "d = tokenizer.decode(e)\n",
    "print(e)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['v { l k I r\\\\ i @ k r\\\\ A n @ k @ l z a j I i', 's E n dZ o n o w v { l k I r\\\\ i @ V n r\\\\ I k O r\\\\ d I d k r\\\\ A n @ k @ l z dZ { p @ n i z l I t v { l k I r\\\\ i @ V v D @ b { t @ l f i l d k A m @ n l i r\\\\ @ f r\\\\= d t @ { z v { l k I r\\\\ i @ k r\\\\ A n @ k @ l z a j I i a w t s a j d dZ @ p { n I z @ t { k t I k @ l r\\\\ o w l p l e j I N v I d i o w g e j m d I v E l @ p t b a j s i g @ { n d m i d i @ v I Z @ n f O r\\\\ D @ p l e j s t e j S @ n p O r\\\\ t @ b @ l r\\\\ i', 'l i s t I n dZ { n j u E r\\\\ i I n dZ @ p { n I t I z D @ T r\\\\= d g e j m I n D @ v { l k I r\\\\ i @ s I r\\\\ i z E m p l o j I N D @ s e j m f j u Z @ n V v t { k t I k @ l { n d r\\\\ i l t a j m g A m p l e j { z I t s p r\\\\ E d @ s E s r\\\\= z D @ s t O r\\\\ i r\\\\ V n z p E r\\\\ @ l E l t @ D @ f r\\\\= s t g e j m { n d f A l o w z D @ n e j m l @ s @ p i n @ l m I l @ t E r\\\\ i j u n @ t s r\\\\= v I N D @ n e j S @ n V v g { l i @', 'd U r\\\\ I N D @ s E k @ n d j U r\\\\ o w p { n w O r\\\\ h u p r\\\\= f O r\\\\ m s i k r\\\\ @ t b l { k A p r\\\\= e j S @ n z { n d A r\\\\ p I t @ d @ g E n s t D @ I m p I r\\\\ i @ l j u n @ t k @ l @ m @ t i r\\\\ e j v @ n', 'D @ g e j m b I g { n d I v E l @ p m @ n t I n k { r\\\\ i I N o w v r\\\\= @ l A r\\\\ dZ p O r\\\\ S @ n V v D @ w r\\\\= k d V n A n v { l k I r\\\\ i @ k r\\\\ A n @ k @ l z I i w a j l I t r\\\\ I t e j n d D @ s t { n d r\\\\= d f i tS r\\\\= z V v D @ s I r\\\\ i z I t O l s o w V n d r\\\\= w E n t m V l t @ p @ l @ dZ V s t m @ n t s s V tS { z m e j k I N D @ g e j m m O r\\\\ f r\\\\= g I v I N f O r\\\\ s I r\\\\ i z n u k V m r\\\\= z k E r\\\\ I k t r\\\\= d I z a j n']}\n"
     ]
    }
   ],
   "source": [
    "print(dataset_chunked[\"train\"][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
