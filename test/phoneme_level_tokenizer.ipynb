{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from tokenizers.models import WordLevel, BPE\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.trainers import WordLevelTrainer, BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "import re\n",
    "import os\n",
    "import hashlib\n",
    "import time\n",
    "import multiprocessing\n",
    "import argparse\n",
    "from datasets import load_from_disk\n",
    "from lingua import Language, LanguageDetectorBuilder\n",
    "import epitran\n",
    "from functools import lru_cache\n",
    "from transformers import PreTrainedTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 4358\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1801350\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 3760\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_processes = multiprocessing.cpu_count() - 1\n",
    "# Exact duplication removal (on individual sentences/paragraphs)\n",
    "def remove_exact_duplicates(examples):\n",
    "    seen = set()\n",
    "    deduped_examples = []\n",
    "    for sentence in examples[\"text\"]:\n",
    "        hash_val = hashlib.md5(sentence.encode()).hexdigest()\n",
    "        if hash_val not in seen:\n",
    "            seen.add(hash_val)\n",
    "            deduped_examples.append(sentence)\n",
    "    return {\"text\": deduped_examples}\n",
    "\n",
    "\n",
    "def filter_by_language(examples):\n",
    "    detector = LanguageDetectorBuilder.from_languages(Language.ENGLISH, Language.FRENCH).build()\n",
    "    return {\n",
    "        \"text\": [\n",
    "            sentence for sentence in examples[\"text\"] if detector.detect_language_of(sentence) == Language.ENGLISH\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic text cleaning\n",
    "def clean_text(examples):\n",
    "    cleaned_text = []\n",
    "    for sentence in examples[\"text\"]:\n",
    "        # Lowercase\n",
    "        #sentence = sentence.lower()\n",
    "        # Remove extra spaces\n",
    "        sentence = re.sub(r\"\\s+\", \" \", sentence)\n",
    "        # Remove URLs\n",
    "        sentence = re.sub(r\"http\\S+\", \"\", sentence)\n",
    "        # Remove special characters\n",
    "        sentence = re.sub(r\"[^a-zA-Z0-9,.!?;:\\'\\\" ]+\", \"\", sentence)\n",
    "        cleaned_text.append(sentence.strip())\n",
    "    return {\"text\": cleaned_text}\n",
    "\n",
    "def clean(dataset):\n",
    "    dataset = dataset.map(remove_exact_duplicates, batched=True, num_proc=num_processes)\n",
    "    dataset = dataset.map(filter_by_language, batched=True, num_proc=num_processes)\n",
    "    dataset = dataset.map(clean_text, batched=True, num_proc=num_processes)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_cleaned = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 4358\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1801350\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 3760\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "epi = epitran.Epitran(\"eng-Latn\")\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def xsampa_list(word: str) -> list:\n",
    "    return epi.xsampa_list(word)\n",
    "\n",
    "def translate_sentence(sentence: str) -> str:\n",
    "    return ' '.join(' '.join(xsampa_list(word)) for word in sentence.split())\n",
    "\n",
    "def translate_function(examples):\n",
    "    return {\"text\": [translate_sentence(sentence) for sentence in examples[\"text\"]]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 4358\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1801350\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 3760\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_translated = dataset_cleaned.map(translate_function, batched=True, num_proc=num_processes)\n",
    "dataset_translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['',\n",
       "  ' v { l k I r\\\\ i @ k r\\\\ A n @ k @ l z a j I i ',\n",
       "  '',\n",
       "  's E n dZ o n o w v { l k I r\\\\ i @   V n r\\\\ I k O r\\\\ d I d k r\\\\ A n @ k @ l z  dZ { p @ n i z    l I t  v { l k I r\\\\ i @ V v D @ b { t @ l f i l d    k A m @ n l i r\\\\ @ f r\\\\= d t @ { z v { l k I r\\\\ i @ k r\\\\ A n @ k @ l z a j I i a w t s a j d dZ @ p { n  I z @ t { k t I k @ l r\\\\ o w l  p l e j I N v I d i o w g e j m d I v E l @ p t b a j s i g @ { n d m i d i @ v I Z @ n f O r\\\\ D @ p l e j s t e j S @ n p O r\\\\ t @ b @ l  r\\\\ i l i s t I n dZ { n j u E r\\\\ i  I n dZ @ p { n  I t I z D @ T r\\\\= d g e j m I n D @ v { l k I r\\\\ i @ s I r\\\\ i z  E m p l o j I N D @ s e j m f j u Z @ n V v t { k t I k @ l { n d r\\\\ i l  t a j m g A m p l e j { z I t s p r\\\\ E d @ s E s r\\\\= z  D @ s t O r\\\\ i r\\\\ V n z p E r\\\\ @ l E l t @ D @ f r\\\\= s t g e j m { n d f A l o w z D @  n e j m l @ s   @ p i n @ l m I l @ t E r\\\\ i j u n @ t s r\\\\= v I N D @ n e j S @ n V v g { l i @ d U r\\\\ I N D @ s E k @ n d j U r\\\\ o w p { n w O r\\\\ h u p r\\\\= f O r\\\\ m s i k r\\\\ @ t b l { k A p r\\\\= e j S @ n z { n d A r\\\\ p I t @ d @ g E n s t D @ I m p I r\\\\ i @ l j u n @ t  k @ l @ m @ t i r\\\\ e j v @ n  ',\n",
       "  'D @ g e j m b I g { n d I v E l @ p m @ n t I n   k { r\\\\ i I N o w v r\\\\= @ l A r\\\\ dZ p O r\\\\ S @ n V v D @ w r\\\\= k d V n A n v { l k I r\\\\ i @ k r\\\\ A n @ k @ l z I i  w a j l I t r\\\\ I t e j n d D @ s t { n d r\\\\= d f i tS r\\\\= z V v D @ s I r\\\\ i z  I t O l s o w V n d r\\\\= w E n t m V l t @ p @ l @ dZ V s t m @ n t s  s V tS { z m e j k I N D @ g e j m m O r\\\\ f r\\\\= g I v I N f O r\\\\ s I r\\\\ i z n u k V m r\\\\= z  k E r\\\\ I k t r\\\\= d I z a j n r\\\\= r\\\\ e j t @ h o w n dZ u { n d k @ m p o w z r\\\\= h I t o w S i s A k I m o w t o w b o w T r\\\\ I t r\\\\= n d f r\\\\ V m p r\\\\ i v i @ s E n t r\\\\ i z  @ l O N w I D v { l k I r\\\\ i @ k r\\\\ A n @ k @ l z I i d r\\\\= E k t r\\\\= t @ k E S i o w z A w @  @ l A r\\\\ dZ t i m V v r\\\\ a j t r\\\\= z h { n d @ l d D @ s k r\\\\ I p t  D @ g e j m z o w p @ n I N T i m w A z s V N b a j m e j n ']}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_translated[\"train\"][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_corpus(dataset):\n",
    "    dataset = concatenate_datasets([dataset[\"train\"], dataset[\"test\"], dataset[\"validation\"]])\n",
    "    for i in range(0, len(dataset), 1000):\n",
    "        samples = dataset[i : i + 1000]\n",
    "        yield samples[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tokenizer(dataset):\n",
    "    tokenizer = Tokenizer(BPE(unk_token=\"[UNK]\"))\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "    tokenizer.post_processor = TemplateProcessing(\n",
    "        single=\"[CLS] $A [SEP]\",\n",
    "        pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "        special_tokens=[\n",
    "            (\"[CLS]\", 1),\n",
    "            (\"[SEP]\", 2),\n",
    "        ],\n",
    "    )\n",
    "    trainer = BpeTrainer(vocab_size=1000 ,special_tokens=[\"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\", \"[UNK]\"])\n",
    "    tokenizer.train_from_iterator(get_training_corpus(dataset), trainer)\n",
    "    tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)\n",
    "    tokenizer.add_special_tokens(\n",
    "        {\n",
    "            \"pad_token\": \"[PAD]\",\n",
    "            \"mask_token\": \"[MASK]\",\n",
    "            \"cls_token\": \"[CLS]\",\n",
    "            \"sep_token\": \"[SEP]\",\n",
    "            \"unk_token\": \"[UNK]\",\n",
    "        }\n",
    "    )\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/toure215/miniconda3/envs/bert/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = train_tokenizer(dataset_translated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159\n",
      "{'u': 71, 'Z': 48, '[PAD]': 0, '_h': 102, 'o_X': 93, 'B': 25, '|\\\\|\\\\': 124, '|': 77, 'k_h': 138, 'F': 28, 'X_w': 148, 'p': 67, 'tS': 82, '_T': 107, 'K': 33, '[MASK]': 3, '3': 11, '=': 20, '[UNK]': 4, '}': 78, 'l_': 151, 'z_': 142, 'h': 59, 'w': 73, '!': 5, 'u_': 84, 'z': 75, '{': 76, 'n_G': 109, '_H': 121, 'S': 41, 'I': 31, 'e': 56, 'T': 42, '4': 12, '6': 14, '_w': 98, 'dZ': 81, '&': 7, 'l_G': 108, '`:': 149, 'u_X': 94, 'm_G': 100, 't_h': 154, '4_G': 92, 'R': 40, 'M': 35, 'b_': 116, 'j': 61, \"\\\\'\": 99, 'u_k': 156, '9': 17, 's_h': 140, 'V': 44, 't': 70, '?\\\\:': 130, 'c': 54, 'z_G': 129, '[SEP]': 2, 'o_': 83, \"'=\": 101, '>': 21, 'i_': 88, 'k_w': 127, 's': 69, 'g_w_h': 158, 'k': 62, '1': 9, '8': 16, '@\\\\': 125, 'G_w': 132, 'i_G': 95, 'b_h': 136, 'T_': 147, 'W': 45, '7': 15, 'r': 68, 'g': 58, '~': 79, 'B_G': 143, 'X': 46, 'P': 38, 'G': 29, 'K_': 144, 'R_w': 145, 'g_': 137, 'g_w': 119, '{:': 120, '|\\\\': 105, 'A': 24, 'U': 43, 'a_': 85, 'd_h': 126, 's_': 115, \"`'\": 135, 'D': 26, 'a': 52, 'E': 27, 't_': 128, 'b': 53, 'e_X': 111, 'i_X': 110, 'y_': 97, 's_w': 139, 'n_w': 123, 'e_': 87, 'v': 72, 'S_': 133, 'o': 66, 'J': 32, '!\\\\': 131, 'O': 37, 'S_w': 146, 'Y': 47, '\\\\:': 134, '5': 13, '\\\\=': 80, '_G': 89, '2': 10, '?\\\\': 91, 'a_X': 86, '\"': 6, '?': 22, 'N': 36, '[CLS]': 1, 'd': 55, \"'=\\\\\": 103, 'd_': 117, 'f_G': 113, '\\\\': 49, '<': 19, ':': 18, '@:': 90, '_': 50, 'L': 34, 'f_w': 150, 'i': 60, 'i_k': 157, \"'\": 8, 'n': 65, 'n_': 152, 'p_G': 153, 't_w': 141, 'u_t': 118, 'H': 30, '_M': 104, '`': 51, 'y': 74, '@': 23, 'f': 57, 'j_w': 114, 's_G': 96, '~:': 106, 'v_w': 155, 'Q': 39, '\\\\`': 112, '_L': 122, 'l': 63, 'm': 64}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.vocab_size)\n",
    "print(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/toure215/BERT_phonetic/tokenizers/tokenizer_phonetic_WordLevel/tokenizer_config.json',\n",
       " '/home/toure215/BERT_phonetic/tokenizers/tokenizer_phonetic_WordLevel/special_tokens_map.json',\n",
       " '/home/toure215/BERT_phonetic/tokenizers/tokenizer_phonetic_WordLevel/tokenizer.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"/home/toure215/BERT_phonetic/tokenizers/tokenizer_phonetic_WordLevel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunked_text(examples):\n",
    "    chunked_text = []\n",
    "    for sentence in examples[\"text\"]:\n",
    "        words = sentence.split()\n",
    "        chunks = [words[i : i + 256] for i in range(0, len(words), 256)]\n",
    "        chunked_text.extend([\" \".join(chunk) for chunk in chunks])\n",
    "    return {\"text\": chunked_text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 5292\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 2179886\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 4577\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_chunked = (\n",
    "    dataset_translated.map(chunked_text, batched=True, num_proc=num_processes)\n",
    "    .flatten_indices()\n",
    "    .filter(lambda x: len(x[\"text\"]) > 0)\n",
    ")\n",
    "\n",
    "dataset_chunked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac617b2dd3094cb28798235cbcec4457",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/5292 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4613cddf53f8492cae2829933e8dbcbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/2 shards):   0%|          | 0/2179886 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7302ef7fb1bf489e8d61e1f86ea95b18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/4577 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_chunked.save_to_disk(\"/home/toure215/BERT_phonetic/DATASETS/phoneme_wikitext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = PreTrainedTokenizerFast.from_pretrained(\"/home/toure215/BERT_phonetic/tokenizers/tokenizer_phonetic_WordLevel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h @ l o w h a w A r\\ j u\n",
      "[1, 59, 23, 63, 66, 73, 59, 52, 73, 24, 68, 49, 61, 71, 2]\n",
      "[CLS] h @ l o w h a w A r \\ j u [SEP]\n"
     ]
    }
   ],
   "source": [
    "s = translate_sentence(\"Hello, how are you?\")\n",
    "print(s)\n",
    "e = tokenizer.encode(s)\n",
    "d = tokenizer.decode(e)\n",
    "print(e)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['v { l k I r\\\\ i @ k r\\\\ A n @ k @ l z a j I i', 's E n dZ o n o w v { l k I r\\\\ i @ V n r\\\\ I k O r\\\\ d I d k r\\\\ A n @ k @ l z dZ { p @ n i z l I t v { l k I r\\\\ i @ V v D @ b { t @ l f i l d k A m @ n l i r\\\\ @ f r\\\\= d t @ { z v { l k I r\\\\ i @ k r\\\\ A n @ k @ l z a j I i a w t s a j d dZ @ p { n I z @ t { k t I k @ l r\\\\ o w l p l e j I N v I d i o w g e j m d I v E l @ p t b a j s i g @ { n d m i d i @ v I Z @ n f O r\\\\ D @ p l e j s t e j S @ n p O r\\\\ t @ b @ l r\\\\ i l i s t I n dZ { n j u E r\\\\ i I n dZ @ p { n I t I z D @ T r\\\\= d g e j m I n D @ v { l k I r\\\\ i @ s I r\\\\ i z E m p l o', 'j I N D @ s e j m f j u Z @ n V v t { k t I k @ l { n d r\\\\ i l t a j m g A m p l e j { z I t s p r\\\\ E d @ s E s r\\\\= z D @ s t O r\\\\ i r\\\\ V n z p E r\\\\ @ l E l t @ D @ f r\\\\= s t g e j m { n d f A l o w z D @ n e j m l @ s @ p i n @ l m I l @ t E r\\\\ i j u n @ t s r\\\\= v I N D @ n e j S @ n V v g { l i @ d U r\\\\ I N D @ s E k @ n d j U r\\\\ o w p { n w O r\\\\ h u p r\\\\= f O r\\\\ m s i k r\\\\ @ t b l { k A p r\\\\= e j S @ n z { n d A r\\\\ p I t @ d @ g E n s t D @ I m p I r\\\\ i @ l j u n @ t k @ l @ m @ t i r\\\\ e j v @ n', 'D @ g e j m b I g { n d I v E l @ p m @ n t I n k { r\\\\ i I N o w v r\\\\= @ l A r\\\\ dZ p O r\\\\ S @ n V v D @ w r\\\\= k d V n A n v { l k I r\\\\ i @ k r\\\\ A n @ k @ l z I i w a j l I t r\\\\ I t e j n d D @ s t { n d r\\\\= d f i tS r\\\\= z V v D @ s I r\\\\ i z I t O l s o w V n d r\\\\= w E n t m V l t @ p @ l @ dZ V s t m @ n t s s V tS { z m e j k I N D @ g e j m m O r\\\\ f r\\\\= g I v I N f O r\\\\ s I r\\\\ i z n u k V m r\\\\= z k E r\\\\ I k t r\\\\= d I z a j n r\\\\= r\\\\ e j t @ h o w n dZ u { n d k @ m p o w z r\\\\= h I t o w S i s A k I m o w t o w b o w T r\\\\ I t r\\\\= n d f r\\\\ V m p r\\\\', 'i v i @ s E n t r\\\\ i z @ l O N w I D v { l k I r\\\\ i @ k r\\\\ A n @ k @ l z I i d r\\\\= E k t r\\\\= t @ k E S i o w z A w @ @ l A r\\\\ dZ t i m V v r\\\\ a j t r\\\\= z h { n d @ l d D @ s k r\\\\ I p t D @ g e j m z o w p @ n I N T i m w A z s V N b a j m e j n']}\n"
     ]
    }
   ],
   "source": [
    "print(dataset_chunked[\"train\"][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
