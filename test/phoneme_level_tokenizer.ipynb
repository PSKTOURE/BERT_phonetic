{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datasets import load_dataset, concatenate_datasets\n",
    "from tokenizers.models import WordLevel, BPE\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.trainers import WordLevelTrainer, BpeTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from tokenizers.processors import TemplateProcessing\n",
    "import re\n",
    "import os\n",
    "import hashlib\n",
    "import time\n",
    "import multiprocessing\n",
    "import argparse\n",
    "from datasets import load_from_disk\n",
    "from lingua import Language, LanguageDetectorBuilder\n",
    "import epitran\n",
    "from functools import lru_cache\n",
    "from transformers import PreTrainedTokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 4358\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1801350\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 3760\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"wikitext\", \"wikitext-103-raw-v1\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_processes = multiprocessing.cpu_count() - 1\n",
    "# Exact duplication removal (on individual sentences/paragraphs)\n",
    "def remove_exact_duplicates(examples):\n",
    "    seen = set()\n",
    "    deduped_examples = []\n",
    "    for sentence in examples[\"text\"]:\n",
    "        hash_val = hashlib.md5(sentence.encode()).hexdigest()\n",
    "        if hash_val not in seen:\n",
    "            seen.add(hash_val)\n",
    "            deduped_examples.append(sentence)\n",
    "    return {\"text\": deduped_examples}\n",
    "\n",
    "\n",
    "def filter_by_language(examples):\n",
    "    detector = LanguageDetectorBuilder.from_languages(Language.ENGLISH, Language.FRENCH).build()\n",
    "    return {\n",
    "        \"text\": [\n",
    "            sentence for sentence in examples[\"text\"] if detector.detect_language_of(sentence) == Language.ENGLISH\n",
    "        ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic text cleaning\n",
    "def clean_text(examples):\n",
    "    cleaned_text = []\n",
    "    for sentence in examples[\"text\"]:\n",
    "        # Lowercase\n",
    "        #sentence = sentence.lower()\n",
    "        # Remove extra spaces\n",
    "        sentence = re.sub(r\"\\s+\", \" \", sentence)\n",
    "        # Remove URLs\n",
    "        sentence = re.sub(r\"http\\S+\", \"\", sentence)\n",
    "        # Remove special characters\n",
    "        sentence = re.sub(r\"[^a-zA-Z0-9,.!?;:\\'\\\" ]+\", \"\", sentence)\n",
    "        cleaned_text.append(sentence.strip())\n",
    "    return {\"text\": cleaned_text}\n",
    "\n",
    "def clean(dataset):\n",
    "    dataset = dataset.map(remove_exact_duplicates, batched=True, num_proc=num_processes)\n",
    "    dataset = dataset.map(filter_by_language, batched=True, num_proc=num_processes)\n",
    "    dataset = dataset.map(clean_text, batched=True, num_proc=num_processes)\n",
    "    return dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_cleaned = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 4358\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1801350\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 3760\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "epi = epitran.Epitran(\"eng-Latn\")\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def xsampa_list(word: str) -> list:\n",
    "    return epi.xsampa_list(word)\n",
    "\n",
    "def translate_sentence(sentence: str) -> str:\n",
    "    return ' '.join(' '.join(xsampa_list(word)) + ' [WORD]' for word in sentence.split())\n",
    "\n",
    "def translate_function(examples):\n",
    "    return {\"text\": [translate_sentence(sentence) for sentence in examples[\"text\"]]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe96ab7168d846da9cc7cb6150b4bab3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=15):   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f260964cc6fc4e31990587103c09f2dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=15):   0%|          | 0/1801350 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c82f071f5e04536a64b9f64142c5c05",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=15):   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 4358\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1801350\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 3760\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_translated = dataset_cleaned.map(translate_function, batched=True, num_proc=num_processes)\n",
    "dataset_translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ''}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_translated[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_training_corpus(dataset):\n",
    "    dataset = concatenate_datasets([dataset[\"train\"], dataset[\"test\"], dataset[\"validation\"]])\n",
    "    for i in range(0, len(dataset), 1000):\n",
    "        samples = dataset[i : i + 1000]\n",
    "        yield samples[\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_tokenizer(dataset):\n",
    "    tokenizer = Tokenizer(WordLevel(unk_token=\"[UNK]\"))\n",
    "    tokenizer.pre_tokenizer = Whitespace()\n",
    "    tokenizer.post_processor = TemplateProcessing(\n",
    "        single=\"[CLS] $A [SEP]\",\n",
    "        pair=\"[CLS] $A [SEP] $B:1 [SEP]:1\",\n",
    "        special_tokens=[\n",
    "            (\"[CLS]\", 1),\n",
    "            (\"[SEP]\", 2),\n",
    "        ],\n",
    "    )\n",
    "    trainer = WordLevelTrainer(vocab_size=1000 ,special_tokens=[\"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\", \"[WORD]\", \"[UNK]\"])\n",
    "    tokenizer.train_from_iterator(get_training_corpus(dataset), trainer)\n",
    "    tokenizer = PreTrainedTokenizerFast(tokenizer_object=tokenizer)\n",
    "    tokenizer.add_special_tokens(\n",
    "        {\n",
    "            \"pad_token\": \"[PAD]\",\n",
    "            \"mask_token\": \"[MASK]\",\n",
    "            \"cls_token\": \"[CLS]\",\n",
    "            \"sep_token\": \"[SEP]\",\n",
    "            \"unk_token\": \"[UNK]\",\n",
    "        }\n",
    "    )\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/toure215/miniconda3/envs/bert/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "tokenizer = train_tokenizer(dataset_translated)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159\n",
      "{'k': 19, 'Y': 94, 'S': 41, '4': 66, '@': 9, 'i_X': 99, 'm': 24, 't_': 126, 'G_w': 130, 'z_G': 127, 'y': 59, 'g_w': 119, '_h': 113, 'n_w': 120, 'Q': 65, 'f_w': 150, 'l_': 153, 's_h': 138, 'p_G': 155, 'I': 13, 'i_k': 152, 'M': 85, '[MASK]': 3, 'n_G': 101, 'k_w': 125, 'a_': 52, 's': 14, '>': 93, 'tS': 44, 'z': 23, '{:': 114, 'n_': 154, 'O': 36, 'i_G': 77, 't_w': 140, 'G': 74, 'r': 10, 'l': 17, '@:': 69, '\\\\:': 132, '|\\\\': 115, '}': 111, 'S_w': 146, '[WORD]': 4, 'D': 27, 's_G': 80, 'u_t': 110, 'R_w': 145, 'g_w_h': 151, '9': 64, 'b_': 107, '?\\\\': 72, '\\\\': 15, '5': 83, 'k_h': 137, 'n': 11, 'V': 31, '!\\\\': 128, '~:': 92, 'K_': 144, 'W': 109, 'p': 28, 's_w': 139, 'A': 29, 'N': 40, 'f': 33, 'B_G': 143, 'e_X': 102, 'g_': 136, 'h': 39, 'v': 30, '_T': 95, 'B': 58, '[SEP]': 2, 'j': 18, '_w': 133, \"'\": 63, 'i_': 60, '4_G': 73, '[UNK]': 5, 'd': 16, 'e_': 55, '&': 142, '?\\\\:': 122, 'P': 84, '_L': 118, ']': 8, '_': 97, 'u_k': 157, '`': 68, '@\\\\': 123, 'b': 32, 'g': 42, 'T_': 147, 't_h': 156, \"'=\\\\\": 90, 't': 12, 'd_': 108, 'j_w': 105, 'u_X': 78, 'E': 26, '6': 67, 'm_G': 89, \"\\\\'\": 86, '_G': 98, 'i': 22, '{': 21, '[': 7, 'L': 87, '8': 96, 'R': 62, 'WORD': 6, 'd_h': 124, 'T': 45, 'f_G': 104, 'u': 38, '2': 53, 'u_': 51, '?': 70, 'F': 129, 'H': 116, '_M': 91, '[PAD]': 0, 'a_X': 54, '_H': 117, 'e': 35, '|\\\\|\\\\': 121, 'l_G': 100, 'o_': 50, 'y_': 82, 'c': 49, 'S_': 131, ':': 57, 'a': 34, 'dZ': 43, 'Z': 47, '1': 75, 'o': 37, 's_': 106, '7': 112, '\"': 48, '3': 81, 'J': 71, '`:': 149, \"`'\": 134, 'w': 20, '\\\\=': 25, 'z_': 141, 'v_w': 158, '~': 56, 'U': 46, 'X': 61, 'X_w': 148, 'b_h': 135, 'o_X': 76, '\\\\`': 103, '[CLS]': 1, 'K': 79, '<': 88}\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.vocab_size)\n",
    "print(tokenizer.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('/home/toure215/BERT_phonetic/tokenizers/tokenizer_phonetic_WordLevel/tokenizer_config.json',\n",
       " '/home/toure215/BERT_phonetic/tokenizers/tokenizer_phonetic_WordLevel/special_tokens_map.json',\n",
       " '/home/toure215/BERT_phonetic/tokenizers/tokenizer_phonetic_WordLevel/tokenizer.json')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.save_pretrained(\"/home/toure215/BERT_phonetic/tokenizers/tokenizer_phonetic_WordLevel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunked_text(examples):\n",
    "    chunked_text = []\n",
    "    for sentence in examples[\"text\"]:\n",
    "        words = sentence.split()\n",
    "        chunks = [words[i : i + 200] for i in range(0, len(words), 200)]\n",
    "        chunked_text.extend([\" \".join(chunk) for chunk in chunks])\n",
    "    return {\"text\": chunked_text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e05b6495b4c4969acf6c04082a78d5f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=15):   0%|          | 0/4358 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5169a8dc8ea8445991ded0c0c554a5e1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=15):   0%|          | 0/1801350 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6eb13fb08dc1424ca18312d4dfd468ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=15):   0%|          | 0/3760 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3269dea6ca364dacaadce8303c85722b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/7395 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7ac930cd2c748f5910c4172161d124d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/3059184 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fba7fb5746cc4c7c8cfda7958e3315df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/6440 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d558e00a8da94634b72782ee674c91bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/7395 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fc2de0a54494e95ace42258cde97e33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/3059184 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b21844c7e1743aea4a5246b6ccb43dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/6440 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 7395\n",
       "    })\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 3059184\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 6440\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_chunked = (\n",
    "    dataset_translated.map(chunked_text, batched=True, num_proc=num_processes)\n",
    "    .flatten_indices()\n",
    "    .filter(lambda x: len(x[\"text\"]) > 0)\n",
    ")\n",
    "\n",
    "dataset_chunked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "296fcf138ce6428dab06f7c69a0fc261",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/7395 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "91c3cbbbe2954019a5c39f24c81738d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/3 shards):   0%|          | 0/3059184 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e81d3e7ef5a4662a95d763479e95ec1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/6440 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_chunked.save_to_disk(\"/home/toure215/BERT_phonetic/DATASETS/phonetic_WordLevel_wikitext\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "159"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h @ l o w [WORD] h a w [WORD] A r\\ [WORD] j u [WORD]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 39, 9, 17, 37, 20, 4, 39, 34, 20, 4, 29, 10, 15, 4, 18, 38, 4, 2]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = translate_sentence(\"Hello, how are you?\")\n",
    "print(s)\n",
    "tokenizer.encode(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['[WORD] v { l k I r\\\\ i @ [WORD] k r\\\\ A n @ k @ l z [WORD] a j I i [WORD] [WORD]', 's E n dZ o [WORD] n o w [WORD] v { l k I r\\\\ i @ [WORD] [WORD] [WORD] V n r\\\\ I k O r\\\\ d I d [WORD] k r\\\\ A n @ k @ l z [WORD] [WORD] dZ { p @ n i z [WORD] [WORD] [WORD] [WORD] l I t [WORD] [WORD] v { l k I r\\\\ i @ [WORD] V v [WORD] D @ [WORD] b { t @ l f i l d [WORD] [WORD] [WORD] [WORD] k A m @ n l i [WORD] r\\\\ @ f r\\\\= d [WORD] t @ [WORD] { z [WORD] v { l k I r\\\\ i @ [WORD] k r\\\\ A n @ k @ l z [WORD] a j I i [WORD] a w t s a j d [WORD] dZ @ p { n [WORD] [WORD] I z [WORD] @ [WORD] t { k t I k @ l [WORD] r\\\\ o w l [WORD] [WORD] p l e j I N [WORD] v I d i o w [WORD] g e j m [WORD] d I v E l @ p t [WORD] b a j [WORD] s i g @ [WORD] { n d [WORD] m i d i @ v I Z @ n [WORD] f O r\\\\ [WORD] D @ [WORD] p l e j s t e j S @ n [WORD] p O r\\\\ t @ b @ l [WORD] [WORD] r\\\\ i l i s t [WORD] I n', '[WORD] dZ { n j u E r\\\\ i [WORD] [WORD] I n [WORD] dZ @ p { n [WORD] [WORD] I t [WORD] I z [WORD] D @ [WORD] T r\\\\= d [WORD] g e j m [WORD] I n [WORD] D @ [WORD] v { l k I r\\\\ i @ [WORD] s I r\\\\ i z [WORD] [WORD] E m p l o j I N [WORD] D @ [WORD] s e j m [WORD] f j u Z @ n [WORD] V v [WORD] t { k t I k @ l [WORD] { n d [WORD] r\\\\ i l [WORD] [WORD] t a j m [WORD] g A m p l e j [WORD] { z [WORD] I t s [WORD] p r\\\\ E d @ s E s r\\\\= z [WORD] [WORD] D @ [WORD] s t O r\\\\ i [WORD] r\\\\ V n z [WORD] p E r\\\\ @ l E l [WORD] t @ [WORD] D @ [WORD] f r\\\\= s t [WORD] g e j m [WORD] { n d [WORD] f A l o w z [WORD] D @ [WORD] [WORD] n e j m l @ s [WORD] [WORD] [WORD] @ [WORD] p i n @ l [WORD] m I l @ t E r\\\\ i [WORD] j u n @ t [WORD] s r\\\\= v I N [WORD] D @ [WORD] n e j S @ n [WORD] V v [WORD] g { l i @ [WORD] d U r\\\\ I N [WORD] D', '@ [WORD] s E k @ n d [WORD] j U r\\\\ o w p { n [WORD] w O r\\\\ [WORD] h u [WORD] p r\\\\= f O r\\\\ m [WORD] s i k r\\\\ @ t [WORD] b l { k [WORD] A p r\\\\= e j S @ n z [WORD] { n d [WORD] A r\\\\ [WORD] p I t @ d [WORD] @ g E n s t [WORD] D @ [WORD] I m p I r\\\\ i @ l [WORD] j u n @ t [WORD] [WORD] k @ l @ m @ t i [WORD] r\\\\ e j v @ n [WORD] [WORD] [WORD]', 'D @ [WORD] g e j m [WORD] b I g { n [WORD] d I v E l @ p m @ n t [WORD] I n [WORD] [WORD] [WORD] k { r\\\\ i I N [WORD] o w v r\\\\= [WORD] @ [WORD] l A r\\\\ dZ [WORD] p O r\\\\ S @ n [WORD] V v [WORD] D @ [WORD] w r\\\\= k [WORD] d V n [WORD] A n [WORD] v { l k I r\\\\ i @ [WORD] k r\\\\ A n @ k @ l z [WORD] I i [WORD] [WORD] w a j l [WORD] I t [WORD] r\\\\ I t e j n d [WORD] D @ [WORD] s t { n d r\\\\= d [WORD] f i tS r\\\\= z [WORD] V v [WORD] D @ [WORD] s I r\\\\ i z [WORD] [WORD] I t [WORD] O l s o w [WORD] V n d r\\\\= w E n t [WORD] m V l t @ p @ l [WORD] @ dZ V s t m @ n t s [WORD] [WORD] s V tS [WORD] { z [WORD] m e j k I N [WORD] D @ [WORD] g e j m [WORD] m O r\\\\ [WORD] f r\\\\= g I v I N [WORD] f O r\\\\ [WORD] s I r\\\\ i z [WORD] n u k V m r\\\\= z [WORD] [WORD] k E r\\\\ I k t r\\\\= [WORD] d I z a j n r\\\\= [WORD] r\\\\ e j t @']}\n"
     ]
    }
   ],
   "source": [
    "print(dataset_chunked[\"train\"][:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
