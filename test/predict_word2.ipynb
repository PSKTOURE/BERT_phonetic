{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, DataCollatorForLanguageModeling, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import epitran\n",
    "from functools import lru_cache\n",
    "from difflib import SequenceMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_phonetic = False\n",
    "epi = epitran.Epitran(\"eng-Latn\")\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def xsampa_list(word: str) -> list:\n",
    "    return epi.xsampa_list(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ah why this boding start this sudden pain</td>\n",
       "      <td>that wings my pulse and shoots from vein to vein</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ah why this boding start this sudden pain</td>\n",
       "      <td>those parts of thee that the worlds eye doth view</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>what mean regardless of yon midnight bell</td>\n",
       "      <td>these earthborn visions saddening o'er my cell</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>what mean regardless of yon midnight bell</td>\n",
       "      <td>to save their matrons from the brutal rape</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>what strange disorder prompts these thoughts t...</td>\n",
       "      <td>these sighs to murmur and these tears to flow</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                          sentence1  \\\n",
       "0   0          ah why this boding start this sudden pain   \n",
       "1   1          ah why this boding start this sudden pain   \n",
       "2   2          what mean regardless of yon midnight bell   \n",
       "3   3          what mean regardless of yon midnight bell   \n",
       "4   4  what strange disorder prompts these thoughts t...   \n",
       "\n",
       "                                           sentence2  label  \n",
       "0   that wings my pulse and shoots from vein to vein      1  \n",
       "1  those parts of thee that the worlds eye doth view      0  \n",
       "2     these earthborn visions saddening o'er my cell      1  \n",
       "3         to save their matrons from the brutal rape      0  \n",
       "4      these sighs to murmur and these tears to flow      1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_dataset = pd.read_csv('/home/toure215/BERT_phonetic/DATASETS/verses/verse_dataset.csv')\n",
    "pd_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ah why this boding start this sudden pain</td>\n",
       "      <td>that wings my pulse and shoots from vein to vein</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what mean regardless of yon midnight bell</td>\n",
       "      <td>these earthborn visions saddening o'er my cell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>what strange disorder prompts these thoughts t...</td>\n",
       "      <td>these sighs to murmur and these tears to flow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>'tis she 'tis eloisa's form restor'd</td>\n",
       "      <td>strike the soft sweet harmonic chord</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>she comes in all her killing charms confest</td>\n",
       "      <td>glares thro' the gloom and pours upon my breast</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentence1  \\\n",
       "0          ah why this boding start this sudden pain   \n",
       "2          what mean regardless of yon midnight bell   \n",
       "4  what strange disorder prompts these thoughts t...   \n",
       "6               'tis she 'tis eloisa's form restor'd   \n",
       "8        she comes in all her killing charms confest   \n",
       "\n",
       "                                          sentence2  \n",
       "0  that wings my pulse and shoots from vein to vein  \n",
       "2    these earthborn visions saddening o'er my cell  \n",
       "4     these sighs to murmur and these tears to flow  \n",
       "6              strike the soft sweet harmonic chord  \n",
       "8   glares thro' the gloom and pours upon my breast  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_dataset = pd_dataset.loc[pd_dataset[\"label\"] == 1].drop(columns=[\"label\", \"id\"])\n",
    "pd_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ah why this boding start this sudden pain</td>\n",
       "      <td>that wings my pulse and shoots from vein to vein</td>\n",
       "      <td>pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what mean regardless of yon midnight bell</td>\n",
       "      <td>these earthborn visions saddening o'er my cell</td>\n",
       "      <td>bell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>what strange disorder prompts these thoughts t...</td>\n",
       "      <td>these sighs to murmur and these tears to flow</td>\n",
       "      <td>glow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>'tis she 'tis eloisa's form restor'd</td>\n",
       "      <td>strike the soft sweet harmonic chord</td>\n",
       "      <td>restor'd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>she comes in all her killing charms confest</td>\n",
       "      <td>glares thro' the gloom and pours upon my breast</td>\n",
       "      <td>confest</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentence1  \\\n",
       "0          ah why this boding start this sudden pain   \n",
       "2          what mean regardless of yon midnight bell   \n",
       "4  what strange disorder prompts these thoughts t...   \n",
       "6               'tis she 'tis eloisa's form restor'd   \n",
       "8        she comes in all her killing charms confest   \n",
       "\n",
       "                                          sentence2     label  \n",
       "0  that wings my pulse and shoots from vein to vein      pain  \n",
       "2    these earthborn visions saddening o'er my cell      bell  \n",
       "4     these sighs to murmur and these tears to flow      glow  \n",
       "6              strike the soft sweet harmonic chord  restor'd  \n",
       "8   glares thro' the gloom and pours upon my breast   confest  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_last_word(text):\n",
    "    return text.split()[-1]\n",
    "\n",
    "pd_dataset[\"label\"] = pd_dataset[\"sentence1\"].apply(get_last_word)\n",
    "pd_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(pd_dataset, test_size=0.1, random_state=42, shuffle=True)\n",
    "train, val = train_test_split(train, test_size=0.1, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(train)\n",
    "val_dataset = Dataset.from_pandas(val)\n",
    "test_dataset = Dataset.from_pandas(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label'],\n",
       "        num_rows: 80595\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label'],\n",
       "        num_rows: 8955\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label'],\n",
       "        num_rows: 9951\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": val_dataset,\n",
    "    \"test\": test_dataset\n",
    "})\n",
    "dataset = dataset.remove_columns(column_names=['__index_level_0__'])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at psktoure/BERT_WordPiece_wikitext-103-raw-v1 and are newly initialized: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_path = ['bert-base-uncased','psktoure/BERT_BPE_phonetic_wikitext-103-raw-v1','psktoure/BERT_WordPiece_wikitext-103-raw-v1']\n",
    "\n",
    "if is_phonetic:\n",
    "    model = AutoModelForMaskedLM.from_pretrained(model_path[1])\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path[1])\n",
    "else:\n",
    "    model = AutoModelForMaskedLM.from_pretrained(model_path[-1])\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(sentence: str) -> str:\n",
    "    words = sentence.split()\n",
    "    for i in range(len(words)):\n",
    "        words[i] = ''.join(xsampa_list(words[i]))\n",
    "    return ' '.join(words)\n",
    "\n",
    "def translate_function(examples):\n",
    "    examples['sentence1'] = [translate_sentence(sentence) for sentence in examples['sentence1']]\n",
    "    examples['sentence2'] = [translate_sentence(sentence) for sentence in examples['sentence2']]\n",
    "    examples['label'] = [''.join(xsampa_list(word)) for word in examples['label']]\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_phonetic:\n",
    "    dataset = dataset.map(translate_function, batched=True, num_proc=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dataset.save_to_disk('/home/toure215/BERT_phonetic/DATASETS/verses/rhyming_verses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch   \n",
    "from transformers import PreTrainedTokenizerBase\n",
    "\n",
    "class CustomDataCollator:\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizerBase, padding=True, max_length=256):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.mask_token_id = tokenizer.mask_token_id\n",
    "        self.padding = padding\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __call__(self, examples):\n",
    "       \n",
    "        sentence1 = [example[\"sentence1\"] for example in examples]\n",
    "        sentence2 = [example[\"sentence2\"] for example in examples]\n",
    "        targets = [example[\"label\"] for example in examples]\n",
    "\n",
    "        encoded_targets = self.tokenizer(targets, add_special_tokens=False)\n",
    "            \n",
    "        batch = self.tokenizer(\n",
    "            sentence1,\n",
    "            sentence2,\n",
    "            padding=self.padding,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        labels = input_ids.clone()\n",
    "\n",
    "        for i, idx in enumerate(input_ids):\n",
    "            sep_token_indices = torch.where(idx == self.tokenizer.sep_token_id)[0]\n",
    "            start = sep_token_indices[0] - len(encoded_targets[i])\n",
    "            end = sep_token_indices[0]\n",
    "            input_ids[i, start:end] = self.mask_token_id \n",
    "            labels[i, :start] = -100 \n",
    "            labels[i, end:] = -100\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": batch[\"attention_mask\"],\n",
    "            \"labels\": labels,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence1: [\"its native fierceness still the face retain'd\"]\n",
      "sentence2: [\"sleep conscience sleep each awful thought be drown'd\"]\n",
      "label: [\"retain'd\"]\n"
     ]
    }
   ],
   "source": [
    "sample = dataset['train'][:1]\n",
    "for key, value in sample.items():\n",
    "    print(f'{key}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids : tensor([[    1,  7267, 10378, 18782,  8302,  8185,  7057,  9646,     4,     4,\n",
      "             4,     2, 12858, 22081, 12858,  7878, 28059,  8765,  7117, 15694,\n",
      "            11,    46,     2]])\n",
      "attention_mask : tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n",
      "labels : tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100, 15075,    11,\n",
      "            46,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "          -100,  -100,  -100]])\n",
      "[60, 38, 7123, 4042, 4046, 7062]\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "data_collator = CustomDataCollator(tokenizer)\n",
    "sample_list = [{key: sample[key][i] for key in sample} for i in range(len(sample['sentence1']))]\n",
    "c = data_collator(sample_list)\n",
    "for key in c:\n",
    "    print(key ,\":\", c[key])\n",
    "e = tokenizer.encode(\"r\\\\Itejnd\", add_special_tokens=False)\n",
    "print(e) \n",
    "print(tokenizer.sep_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100, reduction=\"mean\")\n",
    "    \n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        loss = self.loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "/home/toure215/miniconda3/envs/bert/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/tmp/fine_tuned_bert\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_strategy='no',\n",
    "    remove_unused_columns=False,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['validation'],\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ba810282409448686393e427d6bc77b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3780 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "645aa4736f294c53a0a4715dfaccf3bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.3619561195373535, 'eval_runtime': 2.252, 'eval_samples_per_second': 3976.467, 'eval_steps_per_second': 62.167, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "972b0696f09e401db866e458f0c1a0bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.938920736312866, 'eval_runtime': 2.248, 'eval_samples_per_second': 3983.594, 'eval_steps_per_second': 62.278, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "016e70698dc44533a94e1def44e7f90f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 2.7815353870391846, 'eval_runtime': 2.2336, 'eval_samples_per_second': 4009.31, 'eval_steps_per_second': 62.68, 'epoch': 3.0}\n",
      "{'train_runtime': 167.5698, 'train_samples_per_second': 1442.891, 'train_steps_per_second': 22.558, 'train_loss': 3.4605228484623014, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3780, training_loss=3.4605228484623014, metrics={'train_runtime': 167.5698, 'train_samples_per_second': 1442.891, 'train_steps_per_second': 22.558, 'total_flos': 3838217597329500.0, 'train_loss': 3.4605228484623014, 'epoch': 3.0})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rhyme_score(word1: str, word2: str) -> int:\n",
    "    if not is_phonetic:    \n",
    "        end1 = xsampa_list(word1)\n",
    "        end2 = xsampa_list(word2)\n",
    "    else:\n",
    "        end1 = word1\n",
    "        end2 = word2\n",
    "    length = min(len(end1), len(end2), 3)\n",
    "    end1 = end1[-length:]\n",
    "    end2 = end2[-length:]\n",
    "    return SequenceMatcher(None, end1, end2).ratio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rhyme(model, dataset, tokenizer):\n",
    "    model = model.to(\"cuda\")\n",
    "    model.eval()\n",
    "    rhyme_scores = []\n",
    "    batch_size = 256\n",
    "\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        print(f\"Processing example {i}/{len(dataset)} ...\", end=\"\\r\")\n",
    "        batch = dataset[i : i + batch_size]\n",
    "        batch_sequence = [{key: batch[key][j] for key in batch} for j in range(len(batch[\"sentence1\"]))]\n",
    "        inputs = data_collator(batch_sequence)\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "\n",
    "        for j in range(len(batch[\"sentence1\"])):\n",
    "            masked_token_index = torch.where(inputs[\"input_ids\"][j] == tokenizer.mask_token_id)[0]\n",
    "            predicted_index = logits[j, masked_token_index].argmax(-1)\n",
    "            predicted_word = tokenizer.decode(predicted_index)\n",
    "            target = tokenizer.decode(inputs[\"labels\"][j, masked_token_index])\n",
    "            if i < 16 and j < 8:\n",
    "                print('predicted_word:', predicted_word, '-- target_word:', target)\n",
    "            rhyme_scores.append(rhyme_score(predicted_word, target))\n",
    "\n",
    "    return {\"score\": np.mean(rhyme_scores)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rhyme_indices(model, dataset, tokenizer, k=5):\n",
    "    import numpy as np\n",
    "    from torch.utils.data import DataLoader\n",
    "    \n",
    "    model = model.to(\"cuda\")\n",
    "    model.eval()\n",
    "    res = []\n",
    "    batch_size = 256\n",
    "\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        print(f\"Processing batch {i}/{len(dataset)}...\", end=\"\\r\")\n",
    "        batch = dataset[i : i + batch_size]\n",
    "        batch_sequence = [{key: batch[key][j] for key in batch} for j in range(len(batch[\"sentence1\"]))]\n",
    "        inputs = data_collator(batch_sequence)\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            labels = inputs[\"labels\"]\n",
    "\n",
    "        count = 0\n",
    "\n",
    "        for j in range(len(batch[\"sentence1\"])):\n",
    "            # Identify the position of the masked token\n",
    "            masked_token_index = (inputs[\"input_ids\"][j] == tokenizer.mask_token_id).nonzero(as_tuple=True)[0]\n",
    "\n",
    "            # Get the target index\n",
    "            # target_index = inputs[\"labels\"][j, masked_token_index]\n",
    "            \n",
    "            targets = labels[j, masked_token_index]\n",
    "            # Get the top-k predictions\n",
    "            top_k_indices = logits[j, masked_token_index].topk(k).indices.squeeze(0)\n",
    "            if i < 16 and j < 8:\n",
    "                print('targets:', targets, '-- top_k_indices:', top_k_indices)\n",
    "\n",
    "            # Check if the target index is in the top-k predictions\n",
    "            ok = True\n",
    "            for idx, target in enumerate(targets):\n",
    "                if target not in top_k_indices[idx]:\n",
    "                    ok = False\n",
    "            if ok:\n",
    "                count += 1\n",
    "\n",
    "        # Append accuracy for this batch\n",
    "        res.append(count / len(batch[\"sentence1\"]))\n",
    "\n",
    "    # Return the mean Top-k Accuracy\n",
    "    return {\"score\": np.mean(res)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "targets: tensor([7272], device='cuda:0') -- top_k_indices: tensor([7272, 7597, 8663, 8575, 7958], device='cuda:0')\n",
      "targets: tensor([12407], device='cuda:0') -- top_k_indices: tensor([ 9905,  7702, 10258, 12407,  7762], device='cuda:0')\n",
      "targets: tensor([9255], device='cuda:0') -- top_k_indices: tensor([ 9595, 12129, 13672,  9255, 21120], device='cuda:0')\n",
      "targets: tensor([8788, 4040], device='cuda:0') -- top_k_indices: tensor([[ 8788,  7924, 27959, 16311, 14205],\n",
      "        [ 4040,  7881,  7065, 10824,  7309]], device='cuda:0')\n",
      "targets: tensor([ 7141, 11049, 10895], device='cuda:0') -- top_k_indices: tensor([[ 7314,  7750,  7141,  7937,  8002],\n",
      "        [ 9422,    11,  7282,  7397, 13393],\n",
      "        [ 7317,  8274,  4052,    11,  4042]], device='cuda:0')\n",
      "targets: tensor([9266], device='cuda:0') -- top_k_indices: tensor([ 9266, 15699,  8800, 15638,  8370], device='cuda:0')\n",
      "targets: tensor([11263], device='cuda:0') -- top_k_indices: tensor([11263, 12708, 11978, 16210, 10549], device='cuda:0')\n",
      "targets: tensor([28822,    11,    46], device='cuda:0') -- top_k_indices: tensor([[11214, 16087, 28822, 10792,  7560],\n",
      "        [   11, 14584,  7108, 21284, 12302],\n",
      "        [   46,  7061,  4047,  7173,    56]], device='cuda:0')\n",
      "Processing batch 9728/9951...\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': np.float64(0.38042572151316545)}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_rhyme_indices(model, dataset['test'], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7141) False\n",
      "tensor(11049) False\n",
      "tensor(10895) False\n",
      "tensor(7750) True\n",
      "tensor(11) True\n",
      "tensor(4042) True\n"
     ]
    }
   ],
   "source": [
    "targets = torch.tensor([ 7141, 11049, 10895])\n",
    "targets2 = torch.tensor([ 7750, 11, 4042])\n",
    "top_k_indices = torch.tensor([[30141,  7750,  8958, 21849,  9905],\n",
    "        [ 9422,    11, 11381,  4052,  7401],\n",
    "        [ 7317,  4052,  8274,    46,  4042]])\n",
    "\n",
    "for idx, target in enumerate(targets):\n",
    "    print(target, target in top_k_indices[idx])\n",
    "\n",
    "for idx, target in enumerate(targets2):\n",
    "    print(target, target in top_k_indices[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7272) True\n"
     ]
    }
   ],
   "source": [
    "targets = torch.tensor([7272])\n",
    "top_k_indices = torch.tensor([7272, 8663, 7597, 8575, 8632])\n",
    "for idx, target in enumerate(targets):\n",
    "    print(target, target in top_k_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
