{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, DataCollatorForLanguageModeling, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import epitran\n",
    "from functools import lru_cache\n",
    "from difflib import SequenceMatcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_phonetic = False\n",
    "epi = epitran.Epitran(\"eng-Latn\")\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def xsampa_list(word: str) -> list:\n",
    "    return epi.xsampa_list(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>ah why this boding start this sudden pain</td>\n",
       "      <td>that wings my pulse and shoots from vein to vein</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>ah why this boding start this sudden pain</td>\n",
       "      <td>those parts of thee that the worlds eye doth view</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>what mean regardless of yon midnight bell</td>\n",
       "      <td>these earthborn visions saddening o'er my cell</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>what mean regardless of yon midnight bell</td>\n",
       "      <td>to save their matrons from the brutal rape</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>what strange disorder prompts these thoughts t...</td>\n",
       "      <td>these sighs to murmur and these tears to flow</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                          sentence1  \\\n",
       "0   0          ah why this boding start this sudden pain   \n",
       "1   1          ah why this boding start this sudden pain   \n",
       "2   2          what mean regardless of yon midnight bell   \n",
       "3   3          what mean regardless of yon midnight bell   \n",
       "4   4  what strange disorder prompts these thoughts t...   \n",
       "\n",
       "                                           sentence2  label  \n",
       "0   that wings my pulse and shoots from vein to vein      1  \n",
       "1  those parts of thee that the worlds eye doth view      0  \n",
       "2     these earthborn visions saddening o'er my cell      1  \n",
       "3         to save their matrons from the brutal rape      0  \n",
       "4      these sighs to murmur and these tears to flow      1  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_dataset = pd.read_csv('/home/toure215/BERT_phonetic/DATASETS/verses/verse_dataset.csv')\n",
    "pd_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ah why this boding start this sudden pain</td>\n",
       "      <td>that wings my pulse and shoots from vein to vein</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what mean regardless of yon midnight bell</td>\n",
       "      <td>these earthborn visions saddening o'er my cell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>what strange disorder prompts these thoughts t...</td>\n",
       "      <td>these sighs to murmur and these tears to flow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>'tis she 'tis eloisa's form restor'd</td>\n",
       "      <td>strike the soft sweet harmonic chord</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>she comes in all her killing charms confest</td>\n",
       "      <td>glares thro' the gloom and pours upon my breast</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentence1  \\\n",
       "0          ah why this boding start this sudden pain   \n",
       "2          what mean regardless of yon midnight bell   \n",
       "4  what strange disorder prompts these thoughts t...   \n",
       "6               'tis she 'tis eloisa's form restor'd   \n",
       "8        she comes in all her killing charms confest   \n",
       "\n",
       "                                          sentence2  \n",
       "0  that wings my pulse and shoots from vein to vein  \n",
       "2    these earthborn visions saddening o'er my cell  \n",
       "4     these sighs to murmur and these tears to flow  \n",
       "6              strike the soft sweet harmonic chord  \n",
       "8   glares thro' the gloom and pours upon my breast  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_dataset = pd_dataset.loc[pd_dataset[\"label\"] == 1].drop(columns=[\"label\", \"id\"])\n",
    "pd_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence1</th>\n",
       "      <th>sentence2</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ah why this boding start this sudden pain</td>\n",
       "      <td>that wings my pulse and shoots from vein to vein</td>\n",
       "      <td>pain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>what mean regardless of yon midnight bell</td>\n",
       "      <td>these earthborn visions saddening o'er my cell</td>\n",
       "      <td>bell</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>what strange disorder prompts these thoughts t...</td>\n",
       "      <td>these sighs to murmur and these tears to flow</td>\n",
       "      <td>glow</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>'tis she 'tis eloisa's form restor'd</td>\n",
       "      <td>strike the soft sweet harmonic chord</td>\n",
       "      <td>restor'd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>she comes in all her killing charms confest</td>\n",
       "      <td>glares thro' the gloom and pours upon my breast</td>\n",
       "      <td>confest</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           sentence1  \\\n",
       "0          ah why this boding start this sudden pain   \n",
       "2          what mean regardless of yon midnight bell   \n",
       "4  what strange disorder prompts these thoughts t...   \n",
       "6               'tis she 'tis eloisa's form restor'd   \n",
       "8        she comes in all her killing charms confest   \n",
       "\n",
       "                                          sentence2     label  \n",
       "0  that wings my pulse and shoots from vein to vein      pain  \n",
       "2    these earthborn visions saddening o'er my cell      bell  \n",
       "4     these sighs to murmur and these tears to flow      glow  \n",
       "6              strike the soft sweet harmonic chord  restor'd  \n",
       "8   glares thro' the gloom and pours upon my breast   confest  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_last_word(text):\n",
    "    return text.split()[-1]\n",
    "\n",
    "pd_dataset[\"label\"] = pd_dataset[\"sentence1\"].apply(get_last_word)\n",
    "pd_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(pd_dataset, test_size=0.1, random_state=42, shuffle=True)\n",
    "train, val = train_test_split(train, test_size=0.1, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset.from_pandas(train)\n",
    "val_dataset = Dataset.from_pandas(val)\n",
    "test_dataset = Dataset.from_pandas(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label'],\n",
       "        num_rows: 80595\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label'],\n",
       "        num_rows: 8955\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label'],\n",
       "        num_rows: 9951\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = DatasetDict({\n",
    "    \"train\": train_dataset,\n",
    "    \"validation\": val_dataset,\n",
    "    \"test\": test_dataset\n",
    "})\n",
    "dataset = dataset.remove_columns(column_names=['__index_level_0__'])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From 👉v4.50👈 onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    }
   ],
   "source": [
    "model_path = ['bert-base-uncased','psktoure/BERT_BPE_phonetic_wikitext-103-raw-v1','psktoure/BERT_WordLevel_phonetic_wikitext-103-raw-v1']\n",
    "\n",
    "if is_phonetic:\n",
    "    model = AutoModelForMaskedLM.from_pretrained(model_path[1])\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path[1])\n",
    "else:\n",
    "    model = AutoModelForMaskedLM.from_pretrained(model_path[0])\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(sentence: str) -> str:\n",
    "    words = sentence.split()\n",
    "    for i in range(len(words)):\n",
    "        words[i] = ''.join(xsampa_list(words[i]))\n",
    "    return ' '.join(words)\n",
    "\n",
    "def translate_function(examples):\n",
    "    examples['sentence1'] = [translate_sentence(sentence) for sentence in examples['sentence1']]\n",
    "    examples['sentence2'] = [translate_sentence(sentence) for sentence in examples['sentence2']]\n",
    "    examples['label'] = [''.join(xsampa_list(word)) for word in examples['label']]\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8864bc28bb4a480d9875df7dff679cf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=15):   0%|          | 0/80595 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a51b6d1ecd564ad0abb87175d3ccfebb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=15):   0%|          | 0/8955 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14a8c38134d042f39a35b6441b7d4c2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=15):   0%|          | 0/9951 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if is_phonetic:\n",
    "    dataset = dataset.map(translate_function, batched=True, num_proc=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "032a6171f5024530b3efefeda4fb8012",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/80595 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "611f3f43aad04fafa3db137cf94d23ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/8955 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58fc89d874b9435c9f89e62f2c54546e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/9951 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset.save_to_disk('/home/toure215/BERT_phonetic/DATASETS/verses/rhyming_verses')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch   \n",
    "from transformers import PreTrainedTokenizerBase\n",
    "\n",
    "class CustomDataCollator:\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizerBase, padding=True, max_length=50):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.mask_token_id = tokenizer.mask_token_id\n",
    "        self.padding = padding\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __call__(self, examples):\n",
    "       \n",
    "        sentence1 = [example[\"sentence1\"] for example in examples]\n",
    "        sentence2 = [example[\"sentence2\"] for example in examples]\n",
    "        targets = [example[\"label\"] for example in examples]\n",
    "\n",
    "        encoded_targets = self.tokenizer(\n",
    "            targets,\n",
    "            add_special_tokens=False,\n",
    "        )[\"input_ids\"]\n",
    "            \n",
    "        batch = self.tokenizer(\n",
    "            sentence1,\n",
    "            sentence2,\n",
    "            padding=self.padding,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        labels = input_ids.clone()\n",
    "\n",
    "        for i, idx in enumerate(input_ids):\n",
    "            sep_token_indices = torch.where(idx == self.tokenizer.sep_token_id)[0]\n",
    "            start = sep_token_indices[0] - len(encoded_targets[i])\n",
    "            end = sep_token_indices[0]\n",
    "            input_ids[i, start:end] = self.mask_token_id \n",
    "            labels[i, :start] = -100 \n",
    "            labels[i, end:] = -100\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": batch[\"attention_mask\"],\n",
    "            \"labels\": labels,\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence1: ['Its nejtIv fIr\\\\sn@s stIl D@ fejs r\\\\Itejnd']\n",
      "sentence2: ['slip kAnS@ns slip itS Af@l TOt bi dr\\\\awnd']\n",
      "label: ['r\\\\Itejnd']\n"
     ]
    }
   ],
   "source": [
    "sample = dataset['train'][:1]\n",
    "for key, value in sample.items():\n",
    "    print(f'{key}: {value}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids : tensor([[   1,  170, 1749,  985,   18, 1416,    6,   35,  733,    8,    6, 1113,\n",
      "            4,    4,    4,    2, 3720, 2666,    6,   87, 3720,  330, 1309,    6,\n",
      "           29, 1142,  109,   96,   18,  165,    2]])\n",
      "attention_mask : tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1]])\n",
      "labels : tensor([[-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "           34,   18, 2777, -100, -100, -100, -100, -100, -100, -100, -100, -100,\n",
      "         -100, -100, -100, -100, -100, -100, -100]])\n",
      "[34, 22, 36, 19, 142, 0, 21]\n"
     ]
    }
   ],
   "source": [
    "data_collator = CustomDataCollator(tokenizer)\n",
    "sample_list = [{key: sample[key][i] for key in sample} for i in range(len(sample['sentence1']))]\n",
    "c = data_collator(sample_list)\n",
    "for key in c:\n",
    "    print(key ,\":\", c[key])\n",
    "print(tokenizer.encode(\"retain'd\", add_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/tmp/fine_tuned_bert\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_strategy='no',\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset['train'],\n",
    "    eval_dataset=dataset['validation'],\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a5600c3a8824c478aa5d8146e940363",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3780 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ca57d7d2d39040598677433f7b04044b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.4906039237976074, 'eval_runtime': 7.5937, 'eval_samples_per_second': 1179.273, 'eval_steps_per_second': 18.436, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a83850d2ec04349aaeefcd8b0dc5042",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.3292596340179443, 'eval_runtime': 7.6256, 'eval_samples_per_second': 1174.327, 'eval_steps_per_second': 18.359, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb519879382741c9babe4c692d87d987",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/140 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 1.2923369407653809, 'eval_runtime': 7.5686, 'eval_samples_per_second': 1183.183, 'eval_steps_per_second': 18.498, 'epoch': 3.0}\n",
      "{'train_runtime': 611.9658, 'train_samples_per_second': 395.096, 'train_steps_per_second': 6.177, 'train_loss': 1.387801494295635, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3780, training_loss=1.387801494295635, metrics={'train_runtime': 611.9658, 'train_samples_per_second': 395.096, 'train_steps_per_second': 6.177, 'total_flos': 5220059314830300.0, 'train_loss': 1.387801494295635, 'epoch': 3.0})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rhyme_score(word1: str, word2: str) -> int:\n",
    "    if not is_phonetic:    \n",
    "        end1 = xsampa_list(word1)\n",
    "        end2 = xsampa_list(word2)\n",
    "    else:\n",
    "        end1 = word1\n",
    "        end2 = word2\n",
    "    length = min(len(end1), len(end2), 3)\n",
    "    end1 = end1[-length:]\n",
    "    end2 = end2[-length:]\n",
    "    return SequenceMatcher(None, end1, end2).ratio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rhyme(model, dataset, tokenizer):\n",
    "    model = model.to(\"cuda\")\n",
    "    model.eval()\n",
    "    rhyme_scores = []\n",
    "    batch_size = 256\n",
    "\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        print(f\"Processing example {i}/{len(dataset)} ...\", end=\"\\r\")\n",
    "        batch = dataset[i : i + batch_size]\n",
    "        batch_sequence = [{key: batch[key][j] for key in batch} for j in range(len(batch[\"sentence1\"]))]\n",
    "        inputs = data_collator(batch_sequence)\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "\n",
    "        for j in range(len(batch[\"sentence1\"])):\n",
    "            masked_token_index = torch.where(inputs[\"input_ids\"][j] == tokenizer.mask_token_id)[0]\n",
    "            predicted_index = logits[j, masked_token_index].argmax(-1)\n",
    "            predicted_word = tokenizer.decode(predicted_index)\n",
    "            target = tokenizer.decode(inputs[\"labels\"][j, masked_token_index])\n",
    "            if i < 16 and j < 8:\n",
    "                print('predicted_word:', predicted_word, '-- target_word:', target)\n",
    "            rhyme_scores.append(rhyme_score(predicted_word, target))\n",
    "\n",
    "    return {\"score\": np.mean(rhyme_scores)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted_word: Ol -- target_word: Ol\n",
      "predicted_word: hIr \\ -- target_word: hIr \\\n",
      "predicted_word: mjuz -- target_word: nuz\n",
      "predicted_word: pejnz -- target_word: pejnz\n",
      "predicted_word: pow @ tr \\ i -- target_word: tSIm @ str \\ i\n",
      "predicted_word: tSejndZ -- target_word: tSejndZ\n",
      "predicted_word: juT -- target_word: juT\n",
      "predicted_word: vd -- target_word: bIlivd\n",
      "Processing example 9728/9951 ...\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': np.float64(0.9285163970120255)}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_rhyme(model, dataset['test'], tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
