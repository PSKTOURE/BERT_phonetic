{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/toure215/BERT_phonetic/test\n",
      "/home/toure215/BERT_phonetic\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import pyphone\n",
    "print(os.getcwd())\n",
    "print(os.path.dirname(os.getcwd()))\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "import epitran\n",
    "from datasets import load_from_disk\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "from functools import  lru_cache\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': [106664,\n",
       "  184645,\n",
       "  133811,\n",
       "  140929,\n",
       "  108312,\n",
       "  33067,\n",
       "  129173,\n",
       "  118678,\n",
       "  139076,\n",
       "  3648],\n",
       " 'sentence1': ['because it from no spurious passion came',\n",
       "  'she tells him he should hope the best',\n",
       "  'see the tall obeliscs from memphis old',\n",
       "  'where i at last shall rest my weary head',\n",
       "  'seagrass and spreading wrack are seen below',\n",
       "  \"all matter lives and shews its maker's power\",\n",
       "  \"by laws eternal to th' aerial kind\",\n",
       "  'with their own force his panting breast they arm',\n",
       "  'grutch not of mammon and his leaven',\n",
       "  'she bids each slumbering energy awake'],\n",
       " 'sentence2': ['in that kind hour thy fatal letter came',\n",
       "  'one that still needs his leading string and bib',\n",
       "  'makes truth and discretion the guide of her life',\n",
       "  \"the fairfac'd nuns to fornication draw\",\n",
       "  'what strange disorder prompts these thoughts to glow',\n",
       "  'softly the angelus sounded and over the roofs of the village',\n",
       "  'backgoback oshaugodaya',\n",
       "  'left me each tender fond affection warm',\n",
       "  'with ceaseless prayers the whole artillery given',\n",
       "  'one deep obliterating draught of lethe take'],\n",
       " 'label': [1, 0, 0, 0, 1, 0, 0, 1, 1, 1]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_from_disk(\"/home/toure215/BERT_phonetic/DATASETS/verses/hf_rhymes\")\n",
    "sample = dataset['train'][:10]\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "because it from no spurious passion came - in that kind hour thy fatal letter came - 1\n",
      "she tells him he should hope the best - one that still needs his leading string and bib - 0\n",
      "see the tall obeliscs from memphis old - makes truth and discretion the guide of her life - 0\n",
      "where i at last shall rest my weary head - the fairfac'd nuns to fornication draw - 0\n",
      "seagrass and spreading wrack are seen below - what strange disorder prompts these thoughts to glow - 1\n",
      "all matter lives and shews its maker's power - softly the angelus sounded and over the roofs of the village - 0\n",
      "by laws eternal to th' aerial kind - backgoback oshaugodaya - 0\n",
      "with their own force his panting breast they arm - left me each tender fond affection warm - 1\n",
      "grutch not of mammon and his leaven - with ceaseless prayers the whole artillery given - 1\n",
      "she bids each slumbering energy awake - one deep obliterating draught of lethe take - 1\n"
     ]
    }
   ],
   "source": [
    "for sentence1, sentenc2, label in zip(sample['sentence1'], sample['sentence2'], sample['label']):\n",
    "    print(f\"{sentence1} - {sentenc2} - {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/toure215/miniconda3/envs/bert/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebee51b69f14404b9a15ab4083e2eacf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/15921 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 143280\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 15921\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['label', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
       "        num_rows: 39801\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['sentence1'], examples['sentence2'], truncation=True, padding=False, max_length=128)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=['sentence1', 'sentence2', 'id'])\n",
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "datacollector = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/toure215/miniconda3/envs/bert/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    per_device_train_batch_size=256,\n",
    "    per_device_eval_batch_size=256,\n",
    "    num_train_epochs=3,\n",
    "    logging_strategy='no',\n",
    "    eval_strategy='steps',\n",
    "    output_dir='/tmp/test',\n",
    "    save_strategy='no',\n",
    "    fp16=True,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset['validation'],\n",
    "    data_collator=datacollector,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4b6cfdb5a0e4ba4a55558bed0d0be52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1680 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9f7b70872454c4497e88dc89a9f57f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.021815825253725052, 'eval_runtime': 1.661, 'eval_samples_per_second': 9585.335, 'eval_steps_per_second': 37.93, 'epoch': 0.89}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3538d727672a466180c8aef31bd37f6a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.012306553311645985, 'eval_runtime': 1.7442, 'eval_samples_per_second': 9127.834, 'eval_steps_per_second': 36.119, 'epoch': 1.79}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49adc3549fdc40b7ae23ea47bdb97e56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.012638099491596222, 'eval_runtime': 1.6892, 'eval_samples_per_second': 9425.38, 'eval_steps_per_second': 37.297, 'epoch': 2.68}\n",
      "{'train_runtime': 122.5703, 'train_samples_per_second': 3506.886, 'train_steps_per_second': 13.706, 'train_loss': 0.028571730568295435, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1680, training_loss=0.028571730568295435, metrics={'train_runtime': 122.5703, 'train_samples_per_second': 3506.886, 'train_steps_per_second': 13.706, 'total_flos': 7220745803315520.0, 'train_loss': 0.028571730568295435, 'epoch': 3.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "276bab8e71394aea89199ec57fda07e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/156 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39801, 2) (39801,)\n",
      "0.9961810004773749\n"
     ]
    }
   ],
   "source": [
    "predictions = trainer.predict(tokenized_dataset['test'])\n",
    "preds, labels = predictions.predictions, predictions.label_ids\n",
    "print(preds.shape, labels.shape)\n",
    "preds = np.argmax(preds, axis=1)\n",
    "print((preds == labels).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bIkOz It fr\\Vm now spjUr\\i@s p{S@n kejm -- In D{t kajnd awr\\= Daj fejt@l lEtr\\= kejm -- 1\n",
      "Si tElz hIm hi SUd howp D@ bEst -- wVn D{t stIl nidz hIz lidIN str\\IN {nd bIb -- 0\n",
      "si D@ tOl AbElIsks fr\\Vm mEmf@s owld -- mejks tr\\uT {nd dIskr\\ES@n D@ gajd Vv hr\\= lajf -- 0\n",
      "wEr\\ aj {t l{st S{l r\\Est maj wIr\\i hEd -- D@ fEr\\f{k  di nVnz t@ fOr\\n@kejS@n dr\\O -- 0\n",
      "sigr\\@s {nd spr\\EdIN r\\{k Ar\\ sin bIlow -- wVt str\\ejndZ dIsOr\\dr\\= pr\\Ampts Diz TOts t@ glow -- 1\n",
      "Ol m{tr\\= lajvz {nd Sjuz Its mejkr\\=  Es pawr\\= -- sOftli D@ {ndZ@l@s sawnd@d {nd owvr\\= D@ r\\ufs Vv D@ vIl@dZ -- 0\n",
      "baj lOz Itr\\=n@l t@ tiejtS  Er\\i@l kajnd -- b{kgowb{k OSOgowdaj@ -- 0\n",
      "wID DEr\\ own fOr\\s hIz p{ntIN br\\Est Dej Ar\\m -- lEft mi itS tEndr\\= fAnd @fEkS@n wOr\\m -- 1\n",
      "gr\\VtS nAt Vv m{m@n {nd hIz liv@n -- wID sislIs pr\\Er\\z D@ howl Ar\\tIlr\\=i gIv@n -- 1\n",
      "Si bIdz itS slVmbr\\=IN Enr\\=dZi @wejk -- wVn dip @blitr\\=ejtIN dr\\{ft Vv lED tejk -- 1\n",
      "D{t hilz D@ wawnd {nd kjUr\\z nAt D@ dIsgr\\ejs -- gAz  di An Daj blVS@z Ar\\m  di wID Evr\\=i gr\\ejs -- 1\n",
      "An lr\\=nIN  Es wIN wi pIr\\s tiejtS  Empajr\\il skaj -- Dej tOk DEmsElvz t@ sVmTIN lajk bIlif -- 0\n",
      "Vv lVgZr\\=i baj fIts ImpejS@nt hiv -- IkstINgwIS Evr\\=i gIlti sEns {nd liv -- 1\n",
      "sow wEn wVns D@ sVn Vv dZEs -- wEn hi Diz fejvr\\=  di Sejdz @pIr\\z t@ blEs -- 1\n",
      "@pAlow DEr\\ wID ejm sow klEvr\\= -- wEn  twAz Olmowst tu lejt own  di Daj dEzr\\=t -- 0\n",
      "hIr\\ s{li suz@n kVm kVm kwIk -- wEn sowlz baj Imp@ls sImp@TEtIk -- 1\n",
      "m{Ng@l D@ sr\\=k@lIN vajn {nd Intr\\=sEpt -- SUr\\li D@ mAs@nz tu {nd Dowz Vv idZ@pt -- 1\n",
      "{d t@ D@ wejvz {nd dr\\ajv D@ slowp{k  di tajd -- fr\\Vm lVv  Es wajld vIZ@nEr\\i wISIz str\\ej  di -- 1\n",
      "Ar\\ Ol D@ tulz hIz kAnst@nt tojl Emplojz -- D@ mIdnajt l{mp now mOr\\ EndZoj  di Its blejz -- 1\n",
      "wi  l fOr\\m DEr\\ majndz wID studi@s kEr\\ -- lin {bst@n@ns wAn gr\\if lowtAtId kEr\\ -- 1\n"
     ]
    }
   ],
   "source": [
    "phonetic_dataset = load_from_disk(\"/home/toure215/BERT_phonetic/DATASETS/verses/phonetic_hf_rhymes\")\n",
    "sample2 = phonetic_dataset['train'][:20]\n",
    "for sentence1, sentenc2, label in zip(sample2['sentence1'], sample2['sentence2'], sample2['label']):\n",
    "    print(f\"{sentence1} -- {sentenc2} -- {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Python wrapper for the website: https://www.homophone.com/\n",
    "Gets the homophones of a word.\n",
    "\"\"\"\n",
    "\n",
    "from urllib.request import urlopen\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from typing import Dict, List\n",
    "import re\n",
    "\n",
    "class Pyphones:\n",
    "    \n",
    "    def __init__(self, word):\n",
    "        self.word = word\n",
    "        self.url = \"https://www.homophone.com/search?page={}&type=&q={}\"\n",
    "        self.homophones = {self.word: []}\n",
    "        \n",
    "    def get_the_page(self, page_no=1):\n",
    "        \"\"\"\n",
    "        Get the page content.\n",
    "\n",
    "        Returns\n",
    "            str: the content of the page.\n",
    "        \"\"\"\n",
    "        url = self.url.format(page_no, self.word)\n",
    "        r = requests.get(url)\n",
    "        soup = BeautifulSoup(r.content, \"html.parser\")\n",
    "        return soup\n",
    "\n",
    "    def get_the_page_nos(self):\n",
    "        \"\"\"\n",
    "        Get the total number of pages\n",
    "\n",
    "        Returns\n",
    "            int: the total number of the pages.\n",
    "        \"\"\"\n",
    "        soup = self.get_the_page()\n",
    "        pages = soup.find_all('div', attrs={'class':'col-sm-9'})\n",
    "        if not pages:\n",
    "            return 0\n",
    "        total_pages = pages[0].find('h5').text.split('/')[-1].strip()\n",
    "        return int(total_pages)\n",
    "\n",
    "    def get_the_homophones(self):\n",
    "        \"\"\"\n",
    "        Get the homophones of the word.\n",
    "\n",
    "        Returns\n",
    "            dict: {word: [list_of_homophones]} against each word.\n",
    "        \"\"\"\n",
    "        total_pages = self.get_the_page_nos()\n",
    "        for ix in range(total_pages):\n",
    "            page_no = ix + 1\n",
    "            soup = self.get_the_page(page_no)\n",
    "            raw_homophones = soup.find_all('div', attrs={'class': 'well well-lg'})\n",
    "            for elem in range(len(raw_homophones)):\n",
    "                raw_homophones_2 = raw_homophones[elem].find_all('a', attrs={'class': 'btn word-btn'})\n",
    "                list_of_homophones = list(raw_homophones_2)\n",
    "                if any(list_of_homophones):\n",
    "                    local_homophones = []\n",
    "                    for tag_of_homophone in list_of_homophones:\n",
    "                        homophone = tag_of_homophone.text\n",
    "                        local_homophones.append(homophone)\n",
    "                    self.homophones[self.word].append(local_homophones)\n",
    "\n",
    "        return self.homophones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rough': [['borough', 'burro', 'burrow'], ['rough', 'ruff'], ['through', 'threw', 'thru']]}\n"
     ]
    }
   ],
   "source": [
    "py = Pyphones(\"rough\")\n",
    "homophones = py.get_the_homophones()\n",
    "print(homophones)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['borough', 'burro', 'burrow']\n",
      "['rough', 'ruff']\n",
      "['through', 'threw', 'thru']\n",
      "dict_values([[['borough', 'burro', 'burrow'], ['rough', 'ruff'], ['through', 'threw', 'thru']]])\n"
     ]
    }
   ],
   "source": [
    "for v in homophones[\"rough\"]:\n",
    "    print(v)\n",
    "\n",
    "print(homophones.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
