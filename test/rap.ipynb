{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import epitran\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict, load_dataset, load_from_disk\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoModelForMaskedLM,\n",
    ")\n",
    "import kagglehub\n",
    "from functools import lru_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path = kagglehub.dataset_download(\"jamiewelsh2/rap-lyrics\")\n",
    "\n",
    "# print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd_data_frame = pd.read_csv(\n",
    "#     \"/home/toure215/Documents/BERT_phonetic/DATASETS/rap/updated_rappers.csv\"\n",
    "# )\n",
    "# pd_data_frame.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(pd_data_frame))\n",
    "# print(pd_data_frame[\"song\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = load_dataset(\"Cropinky/rap_lyrics_english\")\n",
    "# ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(ds[\"train\"][:39])\n",
    "# ds[\"train\"] = ds[\"train\"][40:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds[\"train\"] = Dataset.from_dict(ds[\"train\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = ds.filter(\n",
    "#     lambda x: \"[\" not in x[\"text\"] or \"]\" not in x[\"text\"], num_proc=15\n",
    "# ).filter(lambda x: x[\"text\"] != \"\", num_proc=15)\n",
    "# ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds[\"train\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd_ds = ds[\"train\"].to_pandas()\n",
    "# pd_ds.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "epi = epitran.Epitran(\"eng-Latn\")\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def xsampa_list(word):\n",
    "    return epi.xsampa_list(word)\n",
    "\n",
    "\n",
    "def is_rhyming(word1, word2):\n",
    "    sound1 = xsampa_list(word1)\n",
    "    sound2 = xsampa_list(word2)\n",
    "    if len(sound1) < 2 or len(sound2) < 2:\n",
    "        return False\n",
    "    return sound1[-2:] == sound2[-2:]\n",
    "\n",
    "\n",
    "# Pre-compute phonetic endings for all verses\n",
    "def get_last_phonetic(word):\n",
    "    phonemes = xsampa_list(word)\n",
    "    return phonemes[-2:] if len(phonemes) >= 2 else phonemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd_ds[\"last_word\"] = pd_ds[\"text\"].apply(lambda x: x.split()[-1])\n",
    "# pd_ds[\"phonetic_ending\"] = pd_ds[\"last_word\"].apply(get_last_phonetic)\n",
    "# pd_ds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Convert phonetic_ending lists to tuples for hashing\n",
    "# pd_ds[\"phonetic_ending\"] = pd_ds[\"phonetic_ending\"].apply(tuple)\n",
    "\n",
    "# # Group verses by their phonetic endings for quick access to rhyming pairs\n",
    "# rhyme_groups = (\n",
    "#     pd_ds.groupby(\"phonetic_ending\").apply(lambda x: x.index.tolist()).to_dict()\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# phonetic_endings = list(rhyme_groups.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Build the dataset\n",
    "# import random\n",
    "\n",
    "# rap_ds = pd.DataFrame(columns=[\"id\", \"sentence1\", \"sentence2\", \"label\"])\n",
    "\n",
    "# for i in range(0, len(pd_ds), 2):\n",
    "#     last = len(rap_ds)\n",
    "#     word1 = pd_ds.iloc[i][\"last_word\"]\n",
    "#     phonetic1 = pd_ds.iloc[i][\"phonetic_ending\"]\n",
    "\n",
    "#     # Find a rhyming pair\n",
    "#     rhyming_indices = rhyme_groups.get(phonetic1, [])\n",
    "#     rhyming_idx = i  # Default to self if no other rhyme is found\n",
    "#     for idx in rhyming_indices:\n",
    "#         if idx != i:\n",
    "#             rhyming_idx = idx\n",
    "#             break\n",
    "\n",
    "#     rap_ds.loc[last] = [\n",
    "#         last,\n",
    "#         pd_ds.iloc[i][\"text\"],\n",
    "#         pd_ds.iloc[rhyming_idx][\"text\"],\n",
    "#         1,  # Label for rhyming\n",
    "#     ]\n",
    "\n",
    "#     # Find a non-rhyming pair by selecting from different phonetic endings\n",
    "#     non_rhyme_phonetic = phonetic1\n",
    "#     while non_rhyme_phonetic == phonetic1:\n",
    "#         non_rhyme_phonetic = random.choice(phonetic_endings)\n",
    "#     non_rhyme_idx = np.random.choice(rhyme_groups[non_rhyme_phonetic])\n",
    "\n",
    "#     rap_ds.loc[last + 1] = [\n",
    "#         last + 1,\n",
    "#         pd_ds.iloc[i][\"text\"],\n",
    "#         pd_ds.iloc[non_rhyme_idx][\"text\"],\n",
    "#         0,  # Label for non-rhyming\n",
    "#     ]\n",
    "\n",
    "# print(\"Final row count in rap_ds:\", len(rap_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# train, test = train_test_split(rap_ds, test_size=0.1, random_state=42)\n",
    "# train, val = train_test_split(train, test_size=0.1, random_state=42)\n",
    "\n",
    "# train = Dataset.from_pandas(train)\n",
    "# val = Dataset.from_pandas(val)\n",
    "# test = Dataset.from_pandas(test)\n",
    "\n",
    "# rap_ds_hf = DatasetDict({\"train\": train, \"validation\": val, \"test\": test})\n",
    "# rap_ds_hf.save_to_disk(\"/home/toure215/Documents/BERT_phonetic/DATASETS/rap/rap_ds_hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rap_ds_hf = load_from_disk(\n",
    "    \"/home/toure215/BERT_phonetic/DATASETS/rap/rap_ds_hf\"\n",
    ")\n",
    "\n",
    "rap_ds_rhyme = rap_ds_hf.filter(lambda x: x[\"label\"] == 1, num_proc=os.cpu_count() - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label'],\n",
       "        num_rows: 411009\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label'],\n",
       "        num_rows: 45717\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence1', 'sentence2', 'label'],\n",
       "        num_rows: 50881\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rap_ds_rhyme = rap_ds_rhyme.remove_columns([\"__index_level_0__\", \"id\"])\n",
    "\n",
    "\n",
    "def add_rhyme_label(example):\n",
    "    label = example[\"sentence1\"].split()[-1]\n",
    "    return {\n",
    "        \"sentence1\": example[\"sentence1\"],\n",
    "        \"sentence2\": example[\"sentence2\"],\n",
    "        \"label\": label,\n",
    "    }\n",
    "\n",
    "\n",
    "rap_ds_rhyme = rap_ds_rhyme.map(add_rhyme_label, num_proc=os.cpu_count() - 1)\n",
    "rap_ds_rhyme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertForMaskedLM has generative capabilities, as `prepare_inputs_for_generation` is explicitly overwritten. However, it doesn't directly inherit from `GenerationMixin`. From ðŸ‘‰v4.50ðŸ‘ˆ onwards, `PreTrainedModel` will NOT inherit from `GenerationMixin`, and this model will lose the ability to call `generate` and other related functions.\n",
      "  - If you're using `trust_remote_code=True`, you can get rid of this warning by loading the model with an auto class. See https://huggingface.co/docs/transformers/en/model_doc/auto#auto-classes\n",
      "  - If you are the owner of the model architecture code, please modify your model class such that it inherits from `GenerationMixin` (after `PreTrainedModel`, otherwise you'll get an exception).\n",
      "  - If you are not the owner of the model architecture class, please contact the model code owner to update it.\n"
     ]
    }
   ],
   "source": [
    "is_phonetic = True\n",
    "\n",
    "model_path = [\n",
    "    \"bert-base-uncased\",\n",
    "    \"psktoure/BERT_BPE_phonetic_wikitext-103-raw-v1\",\n",
    "    \"psktoure/BERT_WordPiece_wikitext-103-raw-v1\",\n",
    "]\n",
    "\n",
    "if is_phonetic:\n",
    "    model = AutoModelForMaskedLM.from_pretrained(model_path[1])\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path[1])\n",
    "else:\n",
    "    model = AutoModelForMaskedLM.from_pretrained(model_path[0])\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b28cca832aa34e93a32a46cb5d3a53b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=15):   0%|          | 0/411009 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90eee19ea8a4461ebd5852a2703f197e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=15):   0%|          | 0/45717 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "586a43ad82d84ea0a931f07d44ee289c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=15):   0%|          | 0/50881 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def translate_sentence(sentence: str) -> str:\n",
    "    words = sentence.split()\n",
    "    for i in range(len(words)):\n",
    "        words[i] = \"\".join(xsampa_list(words[i]))\n",
    "    return \" \".join(words)\n",
    "\n",
    "\n",
    "def translate_function(examples):\n",
    "    examples[\"sentence1\"] = [\n",
    "        translate_sentence(sentence) for sentence in examples[\"sentence1\"]\n",
    "    ]\n",
    "    examples[\"sentence2\"] = [\n",
    "        translate_sentence(sentence) for sentence in examples[\"sentence2\"]\n",
    "    ]\n",
    "    examples[\"label\"] = [\"\".join(xsampa_list(word)) for word in examples[\"label\"]]\n",
    "    return examples\n",
    "\n",
    "\n",
    "if is_phonetic:\n",
    "    rap_ds_rhyme = rap_ds_rhyme.map(\n",
    "        translate_function, num_proc=os.cpu_count() - 1, batched=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import PreTrainedTokenizerBase\n",
    "\n",
    "\n",
    "class CustomDataCollator:\n",
    "    def __init__(self, tokenizer: PreTrainedTokenizerBase, padding=True, max_length=64):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.mask_token_id = tokenizer.mask_token_id\n",
    "        self.padding = padding\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __call__(self, examples):\n",
    "\n",
    "        sentence1 = [example[\"sentence1\"] for example in examples]\n",
    "        sentence2 = [example[\"sentence2\"] for example in examples]\n",
    "        targets = [example[\"label\"] for example in examples]\n",
    "\n",
    "        encoded_targets = self.tokenizer(targets, add_special_tokens=False)\n",
    "\n",
    "        batch = self.tokenizer(\n",
    "            sentence1,\n",
    "            sentence2,\n",
    "            padding=self.padding,\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        input_ids = batch[\"input_ids\"]\n",
    "        labels = input_ids.clone()\n",
    "\n",
    "        for i, idx in enumerate(input_ids):\n",
    "            sep_token_indices = torch.where(idx == self.tokenizer.sep_token_id)[0]\n",
    "            start = sep_token_indices[0] - len(encoded_targets[i])\n",
    "            end = sep_token_indices[0]\n",
    "            input_ids[i, start:end] = self.mask_token_id\n",
    "            labels[i, :start] = -100\n",
    "            labels[i, end:] = -100\n",
    "\n",
    "        return {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"attention_mask\": batch[\"attention_mask\"],\n",
    "            \"labels\": labels,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTrainer(Trainer):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.loss_fct = torch.nn.CrossEntropyLoss(ignore_index=-100, reduction=\"mean\")\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        loss = self.loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))\n",
    "        return (loss, outputs) if return_outputs else loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/toure215/miniconda3/envs/bert/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "data_collator = CustomDataCollator(tokenizer)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/tmp/fine_tuned_bert\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"no\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=264,\n",
    "    per_device_eval_batch_size=264,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_strategy=\"no\",\n",
    "    remove_unused_columns=False,\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "# Initialize the Trainer\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=rap_ds_rhyme[\"train\"],\n",
    "    eval_dataset=rap_ds_rhyme[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4671' max='4671' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4671/4671 12:32, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.263351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.085101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.027311</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=4671, training_loss=1.2224574251364804, metrics={'train_runtime': 752.7146, 'train_samples_per_second': 1638.107, 'train_steps_per_second': 6.206, 'total_flos': 3.80848301034003e+16, 'train_loss': 1.2224574251364804, 'epoch': 3.0})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rhyme_indices(model, dataset, tokenizer, k=5):\n",
    "\n",
    "    model = model.to(\"cuda\")\n",
    "    model.eval()\n",
    "    res = []\n",
    "    batch_size = 256\n",
    "\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        print(f\"Processing batch {i}/{len(dataset)}...\", end=\"\\r\")\n",
    "        batch = dataset[i : i + batch_size]\n",
    "        batch_sequence = [\n",
    "            {key: batch[key][j] for key in batch}\n",
    "            for j in range(len(batch[\"sentence1\"]))\n",
    "        ]\n",
    "        inputs = data_collator(batch_sequence)\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            labels = inputs[\"labels\"]\n",
    "\n",
    "        count = 0\n",
    "\n",
    "        for j in range(len(batch[\"sentence1\"])):\n",
    "            # Identify the position of the masked token\n",
    "            masked_token_index = (\n",
    "                inputs[\"input_ids\"][j] == tokenizer.mask_token_id\n",
    "            ).nonzero(as_tuple=True)[0]\n",
    "\n",
    "            targets = labels[j, masked_token_index]\n",
    "            top_k_indices = logits[j, masked_token_index].topk(k).indices.squeeze(0)\n",
    "            if i < 16 and j < 8:\n",
    "                print(\"targets:\", targets, \"-- top_k_indices:\", top_k_indices)\n",
    "\n",
    "            ok = True\n",
    "            for idx, target in enumerate(targets):\n",
    "                if target not in top_k_indices[idx]:\n",
    "                    ok = False\n",
    "            if ok:\n",
    "                count += 1\n",
    "\n",
    "        res.append(count / len(batch[\"sentence1\"]))\n",
    "\n",
    "    return {\"score\": np.mean(res)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "targets: tensor([63], device='cuda:0') -- top_k_indices: tensor([  63, 7489,  451, 1465, 1315], device='cuda:0')\n",
      "targets: tensor([191], device='cuda:0') -- top_k_indices: tensor([10266,   191,  6197,  7365,  1123], device='cuda:0')\n",
      "targets: tensor([821], device='cuda:0') -- top_k_indices: tensor([ 275, 5786,  499,  821, 1343], device='cuda:0')\n",
      "targets: tensor([1738,   18], device='cuda:0') -- top_k_indices: tensor([[241,  66, 145, 233,  61],\n",
      "        [ 18,  40,  42,   6,  62]], device='cuda:0')\n",
      "targets: tensor([2886], device='cuda:0') -- top_k_indices: tensor([ 2886, 28578,   353,  6662,  8991], device='cuda:0')\n",
      "targets: tensor([181], device='cuda:0') -- top_k_indices: tensor([  181, 25987,  4511, 16038,  6242], device='cuda:0')\n",
      "targets: tensor([  28,    6, 4397,    6,   31], device='cuda:0') -- top_k_indices: tensor([[  28,   34, 1279,   56,  580],\n",
      "        [   6,   18,   42,   41,   59],\n",
      "        [4397,  808,  839,  383,  402],\n",
      "        [   6,   59,  129, 6214, 3911],\n",
      "        [  31,   81, 5908,   40,  663]], device='cuda:0')\n",
      "targets: tensor([37], device='cuda:0') -- top_k_indices: tensor([   37, 14501,    71,    15,  2557], device='cuda:0')\n",
      "Processing batch 50688/50881...\r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'score': np.float64(0.7898269277084906)}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_rhyme_indices(model, rap_ds_rhyme[\"test\"], tokenizer, k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'score': np.float64(0.7740060405655218)}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'score': np.float64(0.7740060405655218)}\n",
    "{'score': np.float64(0.7898269277084906)}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
