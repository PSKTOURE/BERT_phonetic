{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "ety = pd.read_csv('/home/toure215/BERT_phonetic/DATASETS/etymology/etymology.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4222599\n",
      "                  term_id   lang           term                    reltype  \\\n",
      "0  JN6Uml0yVsW5IXZMLr94gg  Latin  encyclopaedia              borrowed_from   \n",
      "1  JN6Uml0yVsW5IXZMLr94gg  Latin  encyclopaedia         group_derived_root   \n",
      "2  JN6Uml0yVsW5IXZMLr94gg  Latin  encyclopaedia  etymologically_related_to   \n",
      "3  JN6Uml0yVsW5IXZMLr94gg  Latin  encyclopaedia           group_affix_root   \n",
      "4  JN6Uml0yVsW5IXZMLr94gg  Latin  encyclopaedia  etymologically_related_to   \n",
      "\n",
      "          related_term_id   related_lang       related_term  position  \\\n",
      "0  cAiYCdUVXxe7CzwKH3rR1g  Ancient Greek     ἐγκυκλοπαιδεία         0   \n",
      "1                     NaN            NaN                NaN         0   \n",
      "2  dQuegarKXeyzqn_4Bpfatg  Ancient Greek  ἐγκύκλιος παιδείᾱ         0   \n",
      "3                     NaN            NaN                NaN         0   \n",
      "4  PIsyIrZQXnqu4Faflkgbvg  Ancient Greek          ἐγκύκλιος         0   \n",
      "\n",
      "                group_tag              parent_tag  parent_position  \n",
      "0                     NaN                     NaN              NaN  \n",
      "1  PdeKgdn0RDmyyejStXrMbQ                     NaN              NaN  \n",
      "2                     NaN  PdeKgdn0RDmyyejStXrMbQ              0.0  \n",
      "3  rt2vt0MrSpSGl3zvWLPIUQ  PdeKgdn0RDmyyejStXrMbQ              1.0  \n",
      "4                     NaN  rt2vt0MrSpSGl3zvWLPIUQ              0.0  \n"
     ]
    }
   ],
   "source": [
    "print(len(ety))\n",
    "print(ety.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1326502/628322389.py:1: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  eng_ety = ety[ety['lang'] == 'English'][ety['related_lang'] == 'English']\n"
     ]
    }
   ],
   "source": [
    "eng_ety = ety[ety['lang'] == 'English'][ety['related_lang'] == 'English']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "615480\n"
     ]
    }
   ],
   "source": [
    "print(len(eng_ety))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>related_term</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>thesaurus</td>\n",
       "      <td>treasure</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>livre</td>\n",
       "      <td>libra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>livre</td>\n",
       "      <td>lira</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>pond</td>\n",
       "      <td>pound</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>pond</td>\n",
       "      <td>ponder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>nonsense</td>\n",
       "      <td>non-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>nonsense</td>\n",
       "      <td>sense</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>nonsense</td>\n",
       "      <td>unsense</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>pound</td>\n",
       "      <td>pood</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>242</th>\n",
       "      <td>pound</td>\n",
       "      <td>poun</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243</th>\n",
       "      <td>pound</td>\n",
       "      <td>pown</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>pie</td>\n",
       "      <td>pica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>pie</td>\n",
       "      <td>speight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>cat</td>\n",
       "      <td>catastrophic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>cat</td>\n",
       "      <td>methcathinone</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>cat</td>\n",
       "      <td>catapult</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>cat</td>\n",
       "      <td>caterpillar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>free</td>\n",
       "      <td>friend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>476</th>\n",
       "      <td>floccinaucinihilipilification</td>\n",
       "      <td>-fication</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>word</td>\n",
       "      <td>verb</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              term   related_term\n",
       "71                       thesaurus       treasure\n",
       "74                           livre          libra\n",
       "75                           livre           lira\n",
       "191                           pond          pound\n",
       "192                           pond         ponder\n",
       "209                       nonsense           non-\n",
       "210                       nonsense          sense\n",
       "215                       nonsense        unsense\n",
       "233                          pound           pood\n",
       "242                          pound           poun\n",
       "243                          pound           pown\n",
       "278                            pie           pica\n",
       "287                            pie        speight\n",
       "381                            cat   catastrophic\n",
       "382                            cat  methcathinone\n",
       "383                            cat       catapult\n",
       "384                            cat    caterpillar\n",
       "417                           free         friend\n",
       "476  floccinaucinihilipilification      -fication\n",
       "498                           word           verb"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_ety = eng_ety[['term', 'related_term']]\n",
    "eng_ety.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "684647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1326502/1263653631.py:2: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  eng_french_ety = ety[(ety['lang'] == 'English') | (ety['lang'] == 'French')][(ety['related_lang'] == 'English') | (ety['related_lang'] == 'French')]\n"
     ]
    }
   ],
   "source": [
    "# select row where lang is eng or lang is french\n",
    "eng_french_ety = ety[(ety['lang'] == 'English') | (ety['lang'] == 'French')][(ety['related_lang'] == 'English') | (ety['related_lang'] == 'French')]\n",
    "print(len(eng_french_ety))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>related_term</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$cientology</td>\n",
       "      <td>[Scientology]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&amp; cetera</td>\n",
       "      <td>[&amp;, et cetera]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&amp;c.</td>\n",
       "      <td>[&amp;, etc., &amp; cetera]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&amp;lit</td>\n",
       "      <td>[literal, literally]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'Arriet</td>\n",
       "      <td>[Harriet]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>'Arry</td>\n",
       "      <td>[Harry]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>'Arryish</td>\n",
       "      <td>['Arry, ish]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>'Arryism</td>\n",
       "      <td>['Arry, ism]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>'Gater</td>\n",
       "      <td>[gate, er]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>'Murica</td>\n",
       "      <td>[America]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>'Obby 'Oss</td>\n",
       "      <td>[hobby horse, hoss]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>'Tec</td>\n",
       "      <td>[detective, 'tec]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>'ammiyya</td>\n",
       "      <td>[Ammiyya]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>'bortion</td>\n",
       "      <td>[abortion]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>'bot</td>\n",
       "      <td>[robot]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>'burb</td>\n",
       "      <td>[suburb]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>'bye</td>\n",
       "      <td>[goodbye]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>'coon</td>\n",
       "      <td>[raccoon]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>'drome</td>\n",
       "      <td>[aerodrome]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>'dswounds</td>\n",
       "      <td>['s , zounds]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           term          related_term\n",
       "0   $cientology         [Scientology]\n",
       "1      & cetera        [&, et cetera]\n",
       "2           &c.   [&, etc., & cetera]\n",
       "3          &lit  [literal, literally]\n",
       "4       'Arriet             [Harriet]\n",
       "5         'Arry               [Harry]\n",
       "6      'Arryish          ['Arry, ish]\n",
       "7      'Arryism          ['Arry, ism]\n",
       "8        'Gater            [gate, er]\n",
       "9       'Murica             [America]\n",
       "10   'Obby 'Oss   [hobby horse, hoss]\n",
       "11         'Tec     [detective, 'tec]\n",
       "12     'ammiyya             [Ammiyya]\n",
       "13     'bortion            [abortion]\n",
       "14         'bot               [robot]\n",
       "15        'burb              [suburb]\n",
       "16         'bye             [goodbye]\n",
       "17        'coon             [raccoon]\n",
       "18       'drome           [aerodrome]\n",
       "19    'dswounds         ['s , zounds]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_ety_grouped = eng_ety.groupby('term')['related_term'].apply(list).reset_index()\n",
    "eng_ety_grouped.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n",
      "1\n",
      "1.9801661787637415\n"
     ]
    }
   ],
   "source": [
    "print(eng_ety_grouped[\"related_term\"].map(set).map(len).max())\n",
    "print(eng_ety_grouped[\"related_term\"].map(set).map(len).min())\n",
    "print(eng_ety_grouped[\"related_term\"].map(set).map(len).mean())\n",
    "eng_ety_grouped[\"related_term\"] = eng_ety_grouped[\"related_term\"].map(set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>term</th>\n",
       "      <th>related_term</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$cientology</td>\n",
       "      <td>{Scientology}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&amp; cetera</td>\n",
       "      <td>{&amp;, et cetera}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&amp;c.</td>\n",
       "      <td>{etc., &amp; cetera, &amp;}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&amp;lit</td>\n",
       "      <td>{literal, literally}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'Arriet</td>\n",
       "      <td>{Harriet}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>'Arry</td>\n",
       "      <td>{Harry}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>'Arryish</td>\n",
       "      <td>{ish, 'Arry}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>'Arryism</td>\n",
       "      <td>{ism, 'Arry}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>'Gater</td>\n",
       "      <td>{er, gate}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>'Murica</td>\n",
       "      <td>{America}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>'Obby 'Oss</td>\n",
       "      <td>{hobby horse, hoss}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>'Tec</td>\n",
       "      <td>{detective, 'tec}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>'ammiyya</td>\n",
       "      <td>{Ammiyya}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>'bortion</td>\n",
       "      <td>{abortion}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>'bot</td>\n",
       "      <td>{robot}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>'burb</td>\n",
       "      <td>{suburb}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>'bye</td>\n",
       "      <td>{goodbye}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>'coon</td>\n",
       "      <td>{raccoon}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>'drome</td>\n",
       "      <td>{aerodrome}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>'dswounds</td>\n",
       "      <td>{'s , zounds}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           term          related_term\n",
       "0   $cientology         {Scientology}\n",
       "1      & cetera        {&, et cetera}\n",
       "2           &c.   {etc., & cetera, &}\n",
       "3          &lit  {literal, literally}\n",
       "4       'Arriet             {Harriet}\n",
       "5         'Arry               {Harry}\n",
       "6      'Arryish          {ish, 'Arry}\n",
       "7      'Arryism          {ism, 'Arry}\n",
       "8        'Gater            {er, gate}\n",
       "9       'Murica             {America}\n",
       "10   'Obby 'Oss   {hobby horse, hoss}\n",
       "11         'Tec     {detective, 'tec}\n",
       "12     'ammiyya             {Ammiyya}\n",
       "13     'bortion            {abortion}\n",
       "14         'bot               {robot}\n",
       "15        'burb              {suburb}\n",
       "16         'bye             {goodbye}\n",
       "17        'coon             {raccoon}\n",
       "18       'drome           {aerodrome}\n",
       "19    'dswounds         {'s , zounds}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_ety_grouped.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "ety_dict = eng_ety_grouped.set_index('term')['related_term'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$cientology {'Scientology'}\n",
      "& cetera {'&', 'et cetera'}\n",
      "&c. {'etc.', '& cetera', '&'}\n",
      "&lit {'literal', 'literally'}\n",
      "'Arriet {'Harriet'}\n",
      "'Arry {'Harry'}\n",
      "'Arryish {'ish', \"'Arry\"}\n",
      "'Arryism {'ism', \"'Arry\"}\n",
      "'Gater {'er', 'gate'}\n",
      "'Murica {'America'}\n",
      "'Obby 'Oss {'hobby horse', 'hoss'}\n",
      "'Tec {'detective', \"'tec\"}\n"
     ]
    }
   ],
   "source": [
    "for i, (k, v) in enumerate(ety_dict.items()):\n",
    "    print(k, v)\n",
    "    if i > 10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pd = pd.DataFrame(columns=['word1', 'word2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "for term, related_terms in ety_dict.items():\n",
    "    for related_term in related_terms:\n",
    "        last_row = new_pd.shape[0]\n",
    "        new_pd.loc[last_row] = [term, related_term]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "610808\n",
      "          word1        word2\n",
      "0   $cientology  Scientology\n",
      "1      & cetera            &\n",
      "2      & cetera    et cetera\n",
      "3           &c.         etc.\n",
      "4           &c.     & cetera\n",
      "5           &c.            &\n",
      "6          &lit      literal\n",
      "7          &lit    literally\n",
      "8       'Arriet      Harriet\n",
      "9         'Arry        Harry\n",
      "10     'Arryish          ish\n",
      "11     'Arryish        'Arry\n",
      "12     'Arryism          ism\n",
      "13     'Arryism        'Arry\n",
      "14       'Gater           er\n",
      "15       'Gater         gate\n",
      "16      'Murica      America\n",
      "17   'Obby 'Oss  hobby horse\n",
      "18   'Obby 'Oss         hoss\n",
      "19         'Tec    detective\n",
      "610808\n"
     ]
    }
   ],
   "source": [
    "print(len(new_pd))\n",
    "print(new_pd.head(20))\n",
    "new_pd = new_pd.drop_duplicates()\n",
    "print(len(new_pd))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pd[\"label\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "terms = list(ety_dict.keys())\n",
    "for term in terms:\n",
    "    non_ety_terms = random.sample(terms, 2)\n",
    "    while any(non_ety_term in ety_dict[term] for non_ety_term in non_ety_terms):\n",
    "        non_ety_terms = random.sample(terms, 2)\n",
    "    new_pd.loc[new_pd.shape[0]] = [term, non_ety_terms[0], 0]\n",
    "    new_pd.loc[new_pd.shape[0]] = [term, non_ety_terms[1], 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1227734\n",
      "1227707\n",
      "                word1               word2  label\n",
      "0         Lincolniana             illsome      0\n",
      "1              lively                life      1\n",
      "2         thermoprobe      rhyparographic      0\n",
      "3          Cisplatina            Bluerina      0\n",
      "4            dreggish                dreg      1\n",
      "5              -pamil      unmythologized      0\n",
      "6              becare          myofibrous      0\n",
      "7         autohyponym          hypersigil      0\n",
      "8   interfenestration        fenestration      1\n",
      "9   antipoliomyelitic  connectionlessness      0\n",
      "10       transpyloric           minislice      0\n",
      "11         mathbabble                math      1\n",
      "12         caecostomy              caecum      1\n",
      "13   photodissolution         dissolution      1\n",
      "14       interwreathe               inter      1\n",
      "15          palpation       hemophthalmos      0\n",
      "16         unshowered            showered      1\n",
      "17          toothache           amidation      0\n",
      "18           acontial                  al      1\n",
      "19           indecent                  in      1\n"
     ]
    }
   ],
   "source": [
    "print(len(new_pd))\n",
    "new_pd = new_pd.drop_duplicates()\n",
    "# remove empty strings\n",
    "new_pd = new_pd[new_pd['word1'] != '']\n",
    "new_pd = new_pd[new_pd['word2'] != '']\n",
    "new_pd = new_pd.dropna()\n",
    "print(len(new_pd))\n",
    "new_pd = new_pd.sample(frac=1).reset_index(drop=True)\n",
    "print(new_pd.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pd.to_csv('/home/toure215/BERT_phonetic/DATASETS/etymology/etymology_pairs.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset, DatasetDict\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = train_test_split(new_pd, test_size=0.2)\n",
    "train, val = train_test_split(train, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_hf = Dataset.from_pandas(train)\n",
    "val_hf = Dataset.from_pandas(val)\n",
    "test_hf = Dataset.from_pandas(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['word1', 'word2', 'label', '__index_level_0__'],\n",
      "        num_rows: 883948\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['word1', 'word2', 'label', '__index_level_0__'],\n",
      "        num_rows: 98217\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['word1', 'word2', 'label', '__index_level_0__'],\n",
      "        num_rows: 245542\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset_hf = DatasetDict({'train': train_hf, 'validation': val_hf, 'test': test_hf})\n",
    "print(dataset_hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['word1', 'word2', 'label'],\n",
      "        num_rows: 883948\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['word1', 'word2', 'label'],\n",
      "        num_rows: 98217\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['word1', 'word2', 'label'],\n",
      "        num_rows: 245542\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "dataset_hf = dataset_hf.remove_columns('__index_level_0__')\n",
    "print(dataset_hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d0eb29358874567b7ee1654755f890b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/883948 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cf7c90b9c874157934581e3c175fb0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/98217 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3c0b1ab48944c2099dcf81ffa5f3ebb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/245542 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_hf.save_to_disk('/home/toure215/BERT_phonetic/DATASETS/etymology/etymology_pairs_hf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/toure215/miniconda3/envs/bert/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "bert_model = AutoModelForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ed58261eb0f4123b6d870d411d9a9c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=15):   0%|          | 0/883968 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f317ebe06d5245539ba7fa70432719cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=15):   0%|          | 0/98219 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03795ce4f0c84a0faef672e612ad5097",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=15):   0%|          | 0/245547 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "\treturn tokenizer(examples['word1'], examples['word2'], padding=False, truncation=True, max_length=128)\n",
    "\n",
    "dataset_hf_tokenized = dataset_hf.map(tokenize_function, remove_columns=['word1', 'word2'], num_proc=15)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/toure215/miniconda3/envs/bert/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/toure215/miniconda3/envs/bert/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"/tmp/ety_bert\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=256,\n",
    "    per_device_eval_batch_size=256,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_strategy=\"no\",\n",
    "    save_strategy=\"no\",\n",
    "    report_to=\"none\",\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=bert_model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_hf_tokenized[\"train\"],\n",
    "    eval_dataset=dataset_hf_tokenized[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "845ded1e6eba4de0ab48b46779bf1033",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10359 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d969bf884e9487abefd45444aab2d8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/384 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.035456135869026184, 'eval_runtime': 7.4245, 'eval_samples_per_second': 13229.04, 'eval_steps_per_second': 51.721, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3452e20c72948509e91e1805b250f28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/384 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.033208396285772324, 'eval_runtime': 7.2221, 'eval_samples_per_second': 13599.74, 'eval_steps_per_second': 53.17, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8227c8d65ea48498a051dc1ac8acf61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/384 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.041510239243507385, 'eval_runtime': 7.4456, 'eval_samples_per_second': 13191.592, 'eval_steps_per_second': 51.574, 'epoch': 3.0}\n",
      "{'train_runtime': 521.2088, 'train_samples_per_second': 5087.988, 'train_steps_per_second': 19.875, 'train_loss': 0.026343557661778105, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10359, training_loss=0.026343557661778105, metrics={'train_runtime': 521.2088, 'train_samples_per_second': 5087.988, 'train_steps_per_second': 19.875, 'total_flos': 2.253467255841792e+16, 'train_loss': 0.026343557661778105, 'epoch': 3.0})"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5072ac46eed4a2caa44e5defe6f9f55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/960 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9898756653512362\n"
     ]
    }
   ],
   "source": [
    "predictions = trainer.predict(dataset_hf_tokenized[\"test\"])\n",
    "preds, labels = predictions.predictions, predictions.label_ids\n",
    "preds = np.argmax(preds, axis=1)\n",
    "print((preds == labels).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "del bert_model\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at psktoure/BERT_BPE_phonetic_wikitext-103-raw-v1 and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "phonetic_bert = AutoModelForSequenceClassification.from_pretrained('psktoure/BERT_BPE_phonetic_wikitext-103-raw-v1', num_labels=2)\n",
    "tokenizer = AutoTokenizer.from_pretrained('psktoure/BERT_BPE_phonetic_wikitext-103-raw-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "import epitran\n",
    "\n",
    "epi = epitran.Epitran(\"eng-Latn\")\n",
    "\n",
    "\n",
    "@lru_cache(maxsize=None)\n",
    "def xsampa(word):\n",
    "    return \"\".join(epi.xsampa_list(word))\n",
    "\n",
    "\n",
    "def translate_to_phonetic(examples):\n",
    "    examples[\"word1\"] = [xsampa(word) for word in examples[\"word1\"]]\n",
    "    examples[\"word2\"] = [xsampa(word) for word in examples[\"word2\"]]\n",
    "    return examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e312eb0df1b8406b890347b53ef18bb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=15):   0%|          | 0/883948 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36c0ac8965724e73a71ce2038697fb81",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=15):   0%|          | 0/98217 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b01801eaa1054824a08c1fb44e363556",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=15):   0%|          | 0/245542 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_hf_phonetic = dataset_hf.map(translate_to_phonetic, num_proc=15, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddaa6fff6bc24a4390c54cfbf61a33a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/883948 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67bbe97c777e47e28f4aa33888c493cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/98217 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01e0b2d3ddb24ac1992fd78d80dbdb6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/245542 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_hf_phonetic.save_to_disk('/home/toure215/BERT_phonetic/DATASETS/etymology/etymology_pairs_hf_phonetic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['word1', 'word2', 'label'],\n",
       "        num_rows: 883948\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['word1', 'word2', 'label'],\n",
       "        num_rows: 98217\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['word1', 'word2', 'label'],\n",
       "        num_rows: 245542\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_hf_phonetic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84a9e78a12e24f05b6d26f5fae196570",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=15):   0%|          | 0/883948 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83c52bed411f45159cdc3696e437a134",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=15):   0%|          | 0/98217 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae05edeef2034ef68866cbd3708b20d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=15):   0%|          | 0/245542 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset_hf_phonetic_tokenized = dataset_hf_phonetic.map(tokenize_function, remove_columns=['word1', 'word2'], num_proc=15, batched=True)\n",
    "dataset_hf_phonetic_tokenized\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.model = phonetic_bert\n",
    "trainer.train_dataset = dataset_hf_phonetic_tokenized[\"train\"]\n",
    "trainer.eval_dataset = dataset_hf_phonetic_tokenized[\"validation\"]\n",
    "trainer.data_collator = data_collator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef27da2d84684827bbc013d789e35a72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10359 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52e7afc76e954340afc2e8a0b7448e55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/384 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7014560103416443, 'eval_model_preparation_time': 0.0027, 'eval_runtime': 8.7394, 'eval_samples_per_second': 11238.456, 'eval_steps_per_second': 43.939, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ded1a392cfeb47e0be89aa07d556a89e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/384 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7014560103416443, 'eval_model_preparation_time': 0.0027, 'eval_runtime': 8.6064, 'eval_samples_per_second': 11412.041, 'eval_steps_per_second': 44.618, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c0ab10d51ab47efa21de002d00a0ade",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/384 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.7014560103416443, 'eval_model_preparation_time': 0.0027, 'eval_runtime': 8.7516, 'eval_samples_per_second': 11222.76, 'eval_steps_per_second': 43.878, 'epoch': 3.0}\n",
      "{'train_runtime': 677.0837, 'train_samples_per_second': 3916.568, 'train_steps_per_second': 15.299, 'train_loss': 0.17895517239688918, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=10359, training_loss=0.17895517239688918, metrics={'train_runtime': 677.0837, 'train_samples_per_second': 3916.568, 'train_steps_per_second': 15.299, 'total_flos': 3.152444759800152e+16, 'train_loss': 0.17895517239688918, 'epoch': 3.0})"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f8308f710d141bd80c86187aab70629",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/960 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.502296959379658\n"
     ]
    }
   ],
   "source": [
    "predictions = trainer.predict(dataset_hf_phonetic_tokenized[\"test\"])\n",
    "preds, labels = predictions.predictions, predictions.label_ids\n",
    "preds = np.argmax(preds, axis=1)\n",
    "print((preds == labels).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'word1': ['r\\\\ejdiowsIlini@m',\n",
       "  'tr\\\\Ol',\n",
       "  'klEr\\\\@nvIl',\n",
       "  'Esk{pl@s',\n",
       "  'vIznOr\\\\i@n',\n",
       "  'TInspejs',\n",
       "  'InslAv',\n",
       "  'fAr\\\\m@kowkInEtIks',\n",
       "  'ETnAbAt@n@st',\n",
       "  'Ed@t@sowm'],\n",
       " 'word2': ['t@n@tr\\\\@s',\n",
       "  'dr\\\\{g',\n",
       "  '{str\\\\Alowgiz',\n",
       "  'sEnt@mitr\\\\=gr\\\\{msEk@nd',\n",
       "  'klAn@tajp',\n",
       "  'Vltr\\\\{s@l{stIk',\n",
       "  'bajkVspIdejt',\n",
       "  'k@nEtIks',\n",
       "  'ETnow',\n",
       "  'glajsr\\\\=@dZEn@s@s'],\n",
       " 'label': [0, 1, 0, 0, 0, 0, 0, 1, 1, 0]}"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_hf_phonetic['train'][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "del phonetic_bert\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
