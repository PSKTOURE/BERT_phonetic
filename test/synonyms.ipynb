{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset, DatasetDict, load_from_disk\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version, please consider updating (latest version: 0.3.6)\n",
      "Path to dataset files: /home/toure215/.cache/kagglehub/datasets/duketemon/wordnet-synonyms/versions/2\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"duketemon/wordnet-synonyms\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in dataset: ['synonyms.csv', 'synonyms.json']\n"
     ]
    }
   ],
   "source": [
    "files = os.listdir(path)\n",
    "print(\"Files in dataset:\", files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (127001, 3)\n",
      "         lemma part_of_speech                             synonyms\n",
      "0  .22-caliber      adjective  .22 caliber;.22 calibre;.22-calibre\n",
      "1  .22-calibre      adjective  .22 caliber;.22-caliber;.22 calibre\n",
      "2  .22 caliber      adjective  .22-caliber;.22 calibre;.22-calibre\n",
      "3  .22 calibre      adjective  .22 caliber;.22-caliber;.22-calibre\n",
      "4  .38-caliber      adjective  .38 caliber;.38 calibre;.38-calibre\n",
      "lenght of dataset: 127001\n",
      "Dataset columns: Index(['lemma', 'part_of_speech', 'synonyms'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "dataset_path = os.path.join(path, files[0])\n",
    "pd_dataset = pd.read_csv(dataset_path)\n",
    "print(\"Dataset shape:\", pd_dataset.shape)\n",
    "print(pd_dataset.head())\n",
    "print(\"lenght of dataset:\", len(pd_dataset))\n",
    "print(\"Dataset columns:\", pd_dataset.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "seen = set()\n",
    "synonyms = []\n",
    "for i, row in pd_dataset.iterrows():\n",
    "    word = row[\"lemma\"]\n",
    "    if isinstance(word, str) and (word in seen or word[0].isupper()):\n",
    "        continue\n",
    "    syns = set()\n",
    "    if isinstance(word, str):\n",
    "        syns.add(word)\n",
    "    row = row.fillna(\"\")\n",
    "    for syn in row[\"synonyms\"].split(\";\"):\n",
    "        s = syn.strip()\n",
    "        s = s.split(\"|\")\n",
    "        for w in s:\n",
    "            syns.add(w)\n",
    "            seen.add(w)\n",
    "    if isinstance(word, str):\n",
    "        seen.add(word)\n",
    "    synonyms.append(syns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(synonyms): 70313\n",
      "synonyms: [{'Karl Menninger', 'karl menninger', 'Menninger', 'Karl Augustus Menninger'}, {'snaffle', 'snaffle bit'}, {'resurrection plant', 'Anastatica hierochuntica', 'anastatica hierochuntica', 'rose of Jericho'}, {'flower arrangement', 'floral arrangement'}, {'Pan American Day', 'April 14', 'april 14'}, {'order Scleroparei', 'Scleroparei', 'order scleroparei'}, {\"binder's board\", 'binder board'}, {'civil law', 'jus civile', 'roman law', 'Roman law', 'Justinian code'}, {'malacca cane', 'malacca'}, {'quandang', 'quandong', 'quantong', 'native peach'}]\n"
     ]
    }
   ],
   "source": [
    "print(\"len(synonyms):\", len(synonyms))\n",
    "random_idx = np.random.randint(0, len(synonyms), size=10)\n",
    "random_synonyms = [synonyms[i] for i in random_idx]\n",
    "print(\"synonyms:\", random_synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(synonyms_pairs): 482803\n"
     ]
    }
   ],
   "source": [
    "synonyms_pairs = []\n",
    "for syns in synonyms:\n",
    "    syns = list(syns)\n",
    "    for i in range(len(syns)):\n",
    "        for j in range(i + 1, len(syns)):\n",
    "            synonyms_pairs.append((syns[i], syns[j]))\n",
    "\n",
    "print(\"len(synonyms_pairs):\", len(synonyms_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(non_synonyms_pairs): 482803\n"
     ]
    }
   ],
   "source": [
    "non_synonyms_pairs = []\n",
    "n = 0\n",
    "while n < len(synonyms_pairs):\n",
    "    i = np.random.randint(len(synonyms_pairs))\n",
    "    j = np.random.randint(len(synonyms_pairs))\n",
    "    if i == j:\n",
    "        continue\n",
    "    non_synonyms_pairs.append((synonyms_pairs[i][0], synonyms_pairs[j][1]))\n",
    "    n += 1\n",
    "\n",
    "print(\"len(non_synonyms_pairs):\", len(non_synonyms_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pd_dataset shape: (965606, 3)\n",
      "                    word1                 word2  label\n",
      "0                     ace      genus ozothamnus      0\n",
      "1             agriculture              pin down      0\n",
      "2                 gauguin               drunken      0\n",
      "3                  ruffle         genus Cacicus      0\n",
      "4        Ciudad de Mexico      volcan de colima      0\n",
      "5                 Semitic               semitic      1\n",
      "6              James Mill     milling machinery      1\n",
      "7                 Maltese       Maltese terrier      1\n",
      "8        chinese primrose      Primula sinensis      1\n",
      "9                  nobble              overturn      1\n",
      "10                 genius  Shigella dysentariae      0\n",
      "11                work on                  work      1\n",
      "12    family Haloragaceae          Black Prince      0\n",
      "13           shall-flower     tall meadow grass      0\n",
      "14  family Plumbaginaceae        Plumbaginaceae      1\n",
      "15                   bind        fragrant sumac      0\n",
      "16             gloomy Gus           extemporary      0\n",
      "17             Batidaceae                notice      0\n",
      "18             Oregon fir               ball up      0\n",
      "19              bacillary            baculiform      1\n"
     ]
    }
   ],
   "source": [
    "pd_synonyms = pd.DataFrame(synonyms_pairs, columns=[\"word1\", \"word2\"])\n",
    "pd_synonyms[\"label\"] = 1\n",
    "pd_non_synonyms = pd.DataFrame(non_synonyms_pairs, columns=[\"word1\", \"word2\"])\n",
    "pd_non_synonyms[\"label\"] = 0\n",
    "\n",
    "pd_dataset = pd.concat([pd_synonyms, pd_non_synonyms], ignore_index=True)\n",
    "pd_dataset = pd_dataset.sample(frac=1).reset_index(drop=True)\n",
    "print(\"pd_dataset shape:\", pd_dataset.shape)\n",
    "print(pd_dataset.head(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['word1', 'word2', 'label'],\n",
       "        num_rows: 695235\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['word1', 'word2', 'label'],\n",
       "        num_rows: 77249\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['word1', 'word2', 'label'],\n",
       "        num_rows: 193122\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(pd_dataset, test_size=0.2)\n",
    "train, val = train_test_split(train, test_size=0.1)\n",
    "\n",
    "train = Dataset.from_pandas(train, preserve_index=False)\n",
    "val = Dataset.from_pandas(val, preserve_index=False)\n",
    "test = Dataset.from_pandas(test, preserve_index=False)\n",
    "\n",
    "dataset = DatasetDict({\"train\": train, \"validation\": val, \"test\": test})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d01896da42d148a8b2e504168e38af73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/695235 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23d70678382640beba3df251c169bcff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/77249 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12a634b1f8dd40e3be846cda82b2fce6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/193122 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset.save_to_disk(\"/home/toure215/BERT_phonetic/DATASETS/synonyms/synonyms_dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/toure215/miniconda3/envs/bert/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", num_labels=2\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7814cfa6de0e4cca91acffeb6834d34a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=15):   0%|          | 0/695235 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "696c7c67d2af4c6ca258e499f26104b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=15):   0%|          | 0/77249 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7225e206be664a68b52826e0f245980e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=15):   0%|          | 0/193122 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[\"word1\"],\n",
    "        examples[\"word2\"],\n",
    "        padding=False,\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "    )\n",
    "\n",
    "\n",
    "dataset_tokenized = dataset.map(\n",
    "    tokenize_function, batched=True, remove_columns=[\"word1\", \"word2\"], num_proc=15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/toure215/miniconda3/envs/bert/lib/python3.10/site-packages/transformers/training_args.py:1545: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/home/toure215/miniconda3/envs/bert/lib/python3.10/site-packages/accelerate/accelerator.py:494: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "data_collector = DataCollatorWithPadding(tokenizer)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=f\"/tmp/bb\",\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=3,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    logging_strategy=\"no\",\n",
    "    save_strategy=\"no\",\n",
    "    overwrite_output_dir=True,\n",
    "    fp16=True,\n",
    "    seed=np.random.randint(1e6),\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset_tokenized[\"train\"],\n",
    "    eval_dataset=dataset_tokenized[\"validation\"],\n",
    "    data_collator=data_collector,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfd40077647a461f807af90d8ae4522e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ce2e85bef344b94abb69c886667841c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/302 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.131100133061409, 'eval_runtime': 6.7752, 'eval_samples_per_second': 11401.778, 'eval_steps_per_second': 44.575, 'epoch': 1.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "406db0d7f9ae4f00b329490281ea7e0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/302 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.11025826632976532, 'eval_runtime': 7.1807, 'eval_samples_per_second': 10757.857, 'eval_steps_per_second': 42.057, 'epoch': 2.0}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51dca6efa37545939abdcd7ab00812d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/302 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1121121272444725, 'eval_runtime': 7.0396, 'eval_samples_per_second': 10973.431, 'eval_steps_per_second': 42.9, 'epoch': 3.0}\n",
      "{'train_runtime': 569.3705, 'train_samples_per_second': 3663.177, 'train_steps_per_second': 14.311, 'train_loss': 0.11309030164850653, 'epoch': 3.0}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=8148, training_loss=0.11309030164850653, metrics={'train_runtime': 569.3705, 'train_samples_per_second': 3663.177, 'train_steps_per_second': 14.311, 'total_flos': 2.112621903741342e+16, 'train_loss': 0.11309030164850653, 'epoch': 3.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f24d1de895ff41d3a9ba3277197ade9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/755 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.966471971085635\n"
     ]
    }
   ],
   "source": [
    "predictions, labels, _ = trainer.predict(dataset_tokenized[\"test\"])\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "accuracy = np.mean(predictions == labels)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "homophone_dataset = load_from_disk(\n",
    "    \"/home/toure215/BERT_phonetic/DATASETS/homophones_data/hf_dataset\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6661af65a8044260859ffaf438c700f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=15):   0%|          | 0/22787 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddd25101b62e4b6bbb639b28138cd8ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=15):   0%|          | 0/2814 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21cd29b227ea4cd88aacc359b0ee8451",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map (num_proc=15):   0%|          | 0/2532 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = homophone_dataset.map(\n",
    "    tokenize_function, batched=True, remove_columns=[\"word1\", \"word2\"], num_proc=15\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "687b656c0246456abd2cbcd4a5651c52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on homophones: 0.35678749111584934\n"
     ]
    }
   ],
   "source": [
    "predictions, labels, _ = trainer.predict(tokenized_dataset[\"test\"])\n",
    "predictions = np.argmax(predictions, axis=1)\n",
    "accuracy = np.mean(predictions == labels)\n",
    "print(\"Accuracy on homophones:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
