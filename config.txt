# Training model args:
tm::dataset_path==DATASETS/wikitext
tm::tokenizer_path==tokenizers/tokenizer_WordPiece
tm::tokenizer_type==WordPiece
tm::is_phonetic==FALSE
tm::num_epochs==40
tm::max_steps==400000
tm::fp16==TRUE
tm::batch_size==256
tm::distillation_lambda==0.1
tm::lr==0.0001
tm::max_length==128
tm::log_dir==logs
tm::model_dir==models
tm::percent==0.2

# Fine_tuning args:
ft::model_path==psktoure/BERT_WordLevel_phonetic_bookcorpus_0.1
ft::tokenizer_path==psktoure/BERT_WordLevel_phonetic_bookcorpus_0.1
ft::is_phonetic==TRUE
ft::phoneme==FALSE
ft::num_iterations==3
ft::all==TRUE

# Training tokenizer args:
tt::dataset_path==DATASETS/wikitext
tt::tokenizer_type==WordPiece
tt::is_phonetic==FALSE